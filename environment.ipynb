{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a876fdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Trading Environment for DDQN Agent\n",
    "Simulates the trading of calendar spreads with realistic constraints\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e023711",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, Tuple, Any, Optional\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad5aa4e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Define action space for position changes\n",
    "class TradingEnvironment:\n",
    "    def __init__(self, data_sequences: np.ndarray, data_targets: np.ndarray, \n",
    "                 data_dates: np.ndarray, initial_capital: float = None):\n",
    "        \"\"\"\n",
    "        Initialize trading environment\n",
    "        \n",
    "        Args:\n",
    "            data_sequences: Feature sequences (n_samples, n_features, lookback)\n",
    "            data_targets: Calendar spread values (n_samples,)\n",
    "            data_dates: Corresponding dates (n_samples,)\n",
    "            initial_capital: Starting capital\n",
    "        \"\"\"\n",
    "        self.data_sequences = data_sequences\n",
    "        self.data_targets = data_targets\n",
    "        self.data_dates = data_dates\n",
    "        self.n_samples = len(data_sequences)\n",
    "        \n",
    "        # Trading parameters\n",
    "        self.initial_capital = initial_capital or config.INITIAL_CAPITAL\n",
    "        self.tick_value = config.TICK_VALUE\n",
    "        self.max_contracts = config.MAX_CONTRACTS\n",
    "        self.transaction_cost = config.TRANSACTION_COST_PER_CONTRACT\n",
    "        self.slippage = config.BID_ASK_SLIPPAGE if config.SLIPPAGE_ENABLED else 0.0\n",
    "        \n",
    "        # Risk management\n",
    "        self.max_daily_loss = config.MAX_DAILY_LOSS\n",
    "        self.stop_loss_enabled = config.STOP_LOSS_ENABLED\n",
    "        self.stop_loss_pct = config.STOP_LOSS_PERCENTAGE\n",
    "        \n",
    "        # State tracking\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self) -> np.ndarray:\n",
    "        \"\"\"Reset environment to initial state\"\"\"\n",
    "        self.current_step = 0\n",
    "        self.position = 0  # Current position in contracts (-8 to +8)\n",
    "        self.cash = self.initial_capital\n",
    "        self.total_pnl = 0.0\n",
    "        self.unrealized_pnl = 0.0\n",
    "        self.daily_pnl = 0.0\n",
    "        \n",
    "        # Performance tracking\n",
    "        self.trade_history = []\n",
    "        self.equity_curve = [self.initial_capital]\n",
    "        self.daily_returns = []\n",
    "        self.max_drawdown = 0.0\n",
    "        self.peak_equity = self.initial_capital\n",
    "        \n",
    "        # Trade statistics\n",
    "        self.total_trades = 0\n",
    "        self.winning_trades = 0\n",
    "        self.losing_trades = 0\n",
    "        self.current_trade_entry_price = None\n",
    "        self.current_trade_entry_step = None\n",
    "        \n",
    "        # Risk tracking\n",
    "        self.daily_loss_tracker = 0.0\n",
    "        self.stop_loss_triggered = False\n",
    "        \n",
    "        return self._get_state()\n",
    "    \n",
    "    def _get_state(self) -> np.ndarray:\n",
    "        \"\"\"Get current state representation\"\"\"\n",
    "        if self.current_step >= self.n_samples:\n",
    "            # Return zeros if we've exceeded data\n",
    "            market_features = np.zeros((self.data_sequences.shape[1], self.data_sequences.shape[2]))\n",
    "        else:\n",
    "            market_features = self.data_sequences[self.current_step]\n",
    "        \n",
    "        # Flatten market features\n",
    "        market_state = market_features.flatten()\n",
    "        \n",
    "        # Add trading state features\n",
    "        trading_state = np.array([\n",
    "            self.position / self.max_contracts,  # Normalized position\n",
    "            self.unrealized_pnl / self.initial_capital,  # Normalized unrealized PnL\n",
    "            self.total_pnl / self.initial_capital,  # Normalized total PnL\n",
    "            self.daily_pnl / self.initial_capital,  # Normalized daily PnL\n",
    "            (self.cash + self.unrealized_pnl) / self.initial_capital,  # Normalized equity\n",
    "            self.max_drawdown,  # Current drawdown\n",
    "            float(self.stop_loss_triggered),  # Stop loss status\n",
    "            self.current_step / self.n_samples,  # Progress through data\n",
    "        ])\n",
    "        \n",
    "        return np.concatenate([market_state, trading_state])\n",
    "    \n",
    "    def step(self, action: int) -> Tuple[np.ndarray, float, bool, Dict]:\n",
    "        \"\"\"\n",
    "        Execute one trading step\n",
    "        \n",
    "        Args:\n",
    "            action: Action index from POSITION_ACTIONS\n",
    "            \n",
    "        Returns:\n",
    "            next_state, reward, done, info\n",
    "        \"\"\"\n",
    "        if self.current_step >= self.n_samples - 1:\n",
    "            return self._get_state(), 0.0, True, {'reason': 'end_of_data'}\n",
    "        \n",
    "     \n",
    "\n",
    "        # Get current and next prices\n",
    "        current_price = self.data_targets[self.current_step]\n",
    "        next_price = self.data_targets[self.current_step + 1]\n",
    "\n",
    "        # Get OI Ratio from data        \n",
    "        oi_ratio_feature_index = 15  # OI ratio is column 15 in FEATURE_COLUMNS\n",
    "        self.current_oi_ratio = self.data_sequences[self.current_step][oi_ratio_feature_index][-1]  # Get latest value\n",
    "        \n",
    "        # Calculate position change from action\n",
    "        position_change_pct = config.POSITION_ACTIONS[action]\n",
    "        position_change = int(position_change_pct * self.max_contracts)\n",
    "        \n",
    "        # Apply position change with constraints\n",
    "        new_position = np.clip(\n",
    "            self.position + position_change, \n",
    "            -self.max_contracts, \n",
    "            self.max_contracts\n",
    "        )\n",
    "        actual_position_change = new_position - self.position\n",
    "        \n",
    "        # Calculate transaction costs and slippage\n",
    "        transaction_cost = abs(actual_position_change) * self.transaction_cost\n",
    "        slippage_cost = abs(actual_position_change) * self.slippage * self.tick_value\n",
    "        total_cost = transaction_cost + slippage_cost\n",
    "        \n",
    "        # Update position and cash\n",
    "        self.position = new_position\n",
    "        self.cash -= total_cost\n",
    "        \n",
    "        # Move to next step\n",
    "        self.current_step += 1\n",
    "        \n",
    "        # Calculate P&L from price movement\n",
    "        if self.position != 0:\n",
    "            price_change = next_price - current_price\n",
    "            position_pnl = self.position * price_change * self.tick_value\n",
    "            self.unrealized_pnl += position_pnl\n",
    "            self.daily_pnl += position_pnl\n",
    "        \n",
    "        # Track trade if position closed\n",
    "        if actual_position_change != 0 and self.current_trade_entry_price is not None:\n",
    "            if self.position == 0:  # Position fully closed\n",
    "                self._record_trade()\n",
    "        \n",
    "        # Track new trade entry\n",
    "        if self.position != 0 and self.current_trade_entry_price is None:\n",
    "            self.current_trade_entry_price = current_price\n",
    "            self.current_trade_entry_step = self.current_step\n",
    "        \n",
    "        # Update equity and performance metrics\n",
    "        current_equity = self.cash + self.unrealized_pnl\n",
    "        self.equity_curve.append(current_equity)\n",
    "        \n",
    "        # Update peak and drawdown\n",
    "        if current_equity > self.peak_equity:\n",
    "            self.peak_equity = current_equity\n",
    "        \n",
    "        current_drawdown = (self.peak_equity - current_equity) / self.peak_equity\n",
    "        self.max_drawdown = max(self.max_drawdown, current_drawdown)\n",
    "        \n",
    "        # Check risk limits\n",
    "        done = False\n",
    "        info = {}\n",
    "        \n",
    "        # Check daily loss limit\n",
    "        if self.daily_pnl < -self.max_daily_loss:\n",
    "            done = True\n",
    "            info['reason'] = 'daily_loss_limit'\n",
    "            self.stop_loss_triggered = True\n",
    "        \n",
    "        # Check stop loss\n",
    "        if self.stop_loss_enabled and current_drawdown > self.stop_loss_pct:\n",
    "            done = True\n",
    "            info['reason'] = 'stop_loss'\n",
    "            self.stop_loss_triggered = True\n",
    "        \n",
    "        # Calculate reward\n",
    "        reward = self._calculate_reward()\n",
    "        \n",
    "        # Reset daily P&L at end of day (simplified - you might want date-based logic)\n",
    "        if self.current_step % 1 == 0:  # Reset every step for now\n",
    "            self.daily_pnl = 0.0\n",
    "        \n",
    "        next_state = self._get_state()\n",
    "        \n",
    "        # Add performance info\n",
    "        info.update({\n",
    "            'position': self.position,\n",
    "            'cash': self.cash,\n",
    "            'unrealized_pnl': self.unrealized_pnl,\n",
    "            'total_pnl': self.total_pnl,\n",
    "            'equity': current_equity,\n",
    "            'drawdown': current_drawdown,\n",
    "            'transaction_cost': total_cost,\n",
    "            'current_price': next_price\n",
    "        })\n",
    "        \n",
    "        return next_state, reward, done, info\n",
    "    \n",
    "    def _calculate_reward(self) -> float:\n",
    "        \"\"\"Calculate reward based on multiple factors\"\"\"\n",
    "        current_equity = self.cash + self.unrealized_pnl\n",
    "        \n",
    "         #Get current OI Ratio from your data\n",
    "        # You'll need to find which index in your features corresponds to 'OI Ratio'\n",
    "        current_state = self._get_state()\n",
    "        # Assuming OI Ratio is in your market features - you'll need to identify the exact index\n",
    "    \n",
    "        # For now, let's add it to the step method where we have access to the data\n",
    "        # We'll pass it as a parameter or store it as an instance variable\n",
    "    \n",
    "        # P&L component (normalized by initial capital)\n",
    "        pnl_reward = self.daily_pnl / self.initial_capital\n",
    "    \n",
    "        # Win rate component\n",
    "        win_rate = self.winning_trades / max(1, self.total_trades)\n",
    "        win_rate_reward = (win_rate - 0.5) * 2\n",
    "    \n",
    "        # OI Ratio constraint penalty\n",
    "        oi_penalty = 0.0\n",
    "        if hasattr(self, 'current_oi_ratio') and self.current_oi_ratio >= 1.0:\n",
    "            if self.position != 0:\n",
    "                oi_penalty = 5.0  # Strong penalty for trading when OI ratio >= 1\n",
    "            else:\n",
    "                oi_penalty = -1  # Small bonus for staying flat when OI ratio >= 1\n",
    "    \n",
    "        # Risk penalty component\n",
    "        risk_penalty = 0.0\n",
    "        if self.max_drawdown > config.DRAWDOWN_PENALTY_THRESHOLD:\n",
    "            risk_penalty += (self.max_drawdown - config.DRAWDOWN_PENALTY_THRESHOLD) * 10\n",
    "    \n",
    "        # Combine components\n",
    "        reward = (\n",
    "            config.REWARD_PNL_WEIGHT * pnl_reward +\n",
    "            config.REWARD_WINRATE_WEIGHT * win_rate_reward -\n",
    "            config.REWARD_RISK_WEIGHT * risk_penalty -\n",
    "            oi_penalty  # Subtract OI penalty\n",
    "        )   \n",
    "    \n",
    "        return reward\n",
    "    \n",
    "    def _record_trade(self):\n",
    "        \"\"\"Record completed trade statistics\"\"\"\n",
    "        if self.current_trade_entry_price is None:\n",
    "            return\n",
    "            \n",
    "        current_price = self.data_targets[self.current_step]\n",
    "        trade_pnl = (current_price - self.current_trade_entry_price) * self.position * self.tick_value\n",
    "        \n",
    "        self.total_trades += 1\n",
    "        \n",
    "        if trade_pnl > 0:\n",
    "            self.winning_trades += 1\n",
    "        else:\n",
    "            self.losing_trades += 1\n",
    "        \n",
    "        trade_info = {\n",
    "            'entry_price': self.current_trade_entry_price,\n",
    "            'exit_price': current_price,\n",
    "            'entry_step': self.current_trade_entry_step,\n",
    "            'exit_step': self.current_step,\n",
    "            'position': self.position,\n",
    "            'pnl': trade_pnl,\n",
    "            'duration': self.current_step - self.current_trade_entry_step\n",
    "        }\n",
    "        \n",
    "        self.trade_history.append(trade_info)\n",
    "        self.total_pnl += trade_pnl\n",
    "        \n",
    "        # Reset trade tracking\n",
    "        self.current_trade_entry_price = None\n",
    "        self.current_trade_entry_step = None\n",
    "    \n",
    "    def get_performance_metrics(self) -> Dict:\n",
    "        \"\"\"Calculate comprehensive performance metrics\"\"\"\n",
    "        if len(self.equity_curve) < 2:\n",
    "            return {}\n",
    "        \n",
    "        returns = np.diff(self.equity_curve) / self.equity_curve[:-1]\n",
    "        total_return = (self.equity_curve[-1] - self.initial_capital) / self.initial_capital\n",
    "        \n",
    "        # Sharpe ratio (assuming daily data, annualized)\n",
    "        if len(returns) > 1 and np.std(returns) > 0:\n",
    "            sharpe_ratio = np.mean(returns) / np.std(returns) * np.sqrt(252)\n",
    "        else:\n",
    "            sharpe_ratio = 0.0\n",
    "        \n",
    "        # Win rate\n",
    "        win_rate = self.winning_trades / max(1, self.total_trades)\n",
    "        \n",
    "        # Profit factor\n",
    "        if self.losing_trades > 0:\n",
    "            winning_pnl = sum([trade['pnl'] for trade in self.trade_history if trade['pnl'] > 0])\n",
    "            losing_pnl = abs(sum([trade['pnl'] for trade in self.trade_history if trade['pnl'] < 0]))\n",
    "            profit_factor = winning_pnl / losing_pnl if losing_pnl > 0 else float('inf')\n",
    "        else:\n",
    "            profit_factor = float('inf') if self.winning_trades > 0 else 0.0\n",
    "        \n",
    "        # Average trade duration\n",
    "        if self.trade_history:\n",
    "            avg_trade_duration = np.mean([trade['duration'] for trade in self.trade_history])\n",
    "        else:\n",
    "            avg_trade_duration = 0.0\n",
    "        \n",
    "        return {\n",
    "            'total_return': total_return,\n",
    "            'sharpe_ratio': sharpe_ratio,\n",
    "            'max_drawdown': self.max_drawdown,\n",
    "            'win_rate': win_rate,\n",
    "            'profit_factor': profit_factor,\n",
    "            'total_trades': self.total_trades,\n",
    "            'winning_trades': self.winning_trades,\n",
    "            'losing_trades': self.losing_trades,\n",
    "            'avg_trade_duration': avg_trade_duration,\n",
    "            'final_equity': self.equity_curve[-1],\n",
    "            'total_pnl': self.total_pnl,\n",
    "            'unrealized_pnl': self.unrealized_pnl\n",
    "        }\n",
    "    \n",
    "    def get_action_space_size(self) -> int:\n",
    "        \"\"\"Get the size of the action space\"\"\"\n",
    "        return len(config.POSITION_ACTIONS)\n",
    "    \n",
    "    def get_state_size(self) -> int:\n",
    "        \"\"\"Get the size of the state space\"\"\"\n",
    "        if self.current_step < self.n_samples:\n",
    "            market_features = self.data_sequences[self.current_step].flatten()\n",
    "        else:\n",
    "            market_features = np.zeros((self.data_sequences.shape[1] * self.data_sequences.shape[2]))\n",
    "        \n",
    "        trading_features = 8  # Number of trading state features\n",
    "        return len(market_features) + trading_features\n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        \"\"\"Render current environment state\"\"\"\n",
    "        if mode == 'human':\n",
    "            print(f\"Step: {self.current_step}\")\n",
    "            print(f\"Position: {self.position} contracts\")\n",
    "            print(f\"Cash: ${self.cash:,.2f}\")\n",
    "            print(f\"Unrealized P&L: ${self.unrealized_pnl:,.2f}\")\n",
    "            print(f\"Total P&L: ${self.total_pnl:,.2f}\")\n",
    "            print(f\"Equity: ${self.cash + self.unrealized_pnl:,.2f}\")\n",
    "            print(f\"Drawdown: {self.max_drawdown:.2%}\")\n",
    "            print(f\"Trades: {self.total_trades} (Win Rate: {self.winning_trades/max(1,self.total_trades):.1%})\")\n",
    "            print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95980ca0",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Example environment wrapper for easier use with different data splits\n",
    "class TradingEnvironmentWrapper:\n",
    "    def __init__(self, data_splits: Dict):\n",
    "        \"\"\"\n",
    "        Wrapper to easily create environments for train/val/test splits\n",
    "        \n",
    "        Args:\n",
    "            data_splits: Dictionary containing train/val/test data\n",
    "        \"\"\"\n",
    "        self.data_splits = data_splits\n",
    "        \n",
    "    def create_env(self, split: str = 'train') -> TradingEnvironment:\n",
    "        \"\"\"Create environment for specified data split\"\"\"\n",
    "        if split not in self.data_splits:\n",
    "            raise ValueError(f\"Split '{split}' not found. Available: {list(self.data_splits.keys())}\")\n",
    "        \n",
    "        data = self.data_splits[split]\n",
    "        return TradingEnvironment(\n",
    "            data_sequences=data['sequences'],\n",
    "            data_targets=data['targets'],\n",
    "            data_dates=data['dates']\n",
    "        )\n",
    "    \n",
    "    def get_env_info(self) -> Dict:\n",
    "        \"\"\"Get information about environments\"\"\"\n",
    "        info = {}\n",
    "        for split in self.data_splits:\n",
    "            env = self.create_env(split)\n",
    "            info[split] = {\n",
    "                'n_samples': env.n_samples,\n",
    "                'state_size': env.get_state_size(),\n",
    "                'action_space_size': env.get_action_space_size(),\n",
    "                'date_range': (env.data_dates[0], env.data_dates[-1])\n",
    "            }\n",
    "        return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff7ad34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing function\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Testing TradingEnvironment...\")\n",
    "    \n",
    "    # Create dummy data for testing\n",
    "    n_samples = 100\n",
    "    n_features = 25\n",
    "    lookback = 30\n",
    "    \n",
    "    dummy_sequences = np.random.randn(n_samples, n_features, lookback)\n",
    "    dummy_targets = np.random.randn(n_samples) * 10  # Calendar spread values\n",
    "    dummy_dates = pd.date_range('2020-01-01', periods=n_samples, freq='D')\n",
    "    \n",
    "    # Test environment\n",
    "    env = TradingEnvironment(dummy_sequences, dummy_targets, dummy_dates)\n",
    "    \n",
    "    print(f\"State size: {env.get_state_size()}\")\n",
    "    print(f\"Action space size: {env.get_action_space_size()}\")\n",
    "    \n",
    "    # Test a few random actions\n",
    "    state = env.reset()\n",
    "    print(f\"Initial state shape: {state.shape}\")\n",
    "    \n",
    "    for i in range(5):\n",
    "        action = np.random.randint(0, env.get_action_space_size())\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        print(f\"Step {i+1}: Action={action}, Reward={reward:.4f}, Done={done}\")\n",
    "        env.render()\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    # Print final performance\n",
    "    metrics = env.get_performance_metrics()\n",
    "    print(\"\\nFinal Performance Metrics:\")\n",
    "    for key, value in metrics.items():\n",
    "        if isinstance(value, float):\n",
    "            print(f\"{key}: {value:.4f}\")\n",
    "        else:\n",
    "            print(f\"{key}: {value}\")\n",
    "    \n",
    "    print(\"Environment testing complete!\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
