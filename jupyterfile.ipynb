{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7469856",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Complete DDQN Trading System for Jupyter Notebook\n",
    "All modules combined into a single file\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ba75082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded successfully!\n",
      "Device: cuda\n",
      "Lookback window: 30 days\n",
      "Max contracts: 8\n",
      "Tick value: $10.0\n",
      "Quick test mode: False\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION PARAMETERS\n",
    "# =============================================================================\n",
    "\n",
    "# Market parameters\n",
    "TICK_VALUE = 10.0\n",
    "MAX_CONTRACTS = 8\n",
    "TRANSACTION_COST_PER_CONTRACT = 1.50\n",
    "BID_ASK_SLIPPAGE = 1\n",
    "SLIPPAGE_ENABLED = True\n",
    "\n",
    "# Risk management\n",
    "MAX_DAILY_LOSS = 1000.0\n",
    "STOP_LOSS_ENABLED = True\n",
    "STOP_LOSS_PERCENTAGE = 0.3\n",
    "\n",
    "# Data parameters\n",
    "LOOKBACK_WINDOW = 30\n",
    "MIN_LOOKBACK_FOR_TRAINING = 30\n",
    "TRAIN_RATIO = 0.6\n",
    "VAL_RATIO = 0.10\n",
    "TEST_RATIO = 0.30\n",
    "\n",
    "FEATURE_COLUMNS = [\n",
    "   'PX_OPEN1', 'PX_HIGH1', 'PX_LOW1', 'PX_LAST1', 'PX_VOLUME1', 'OPEN_INT1',\n",
    "   'PX_OPEN2', 'PX_HIGH2', 'PX_LOW2', 'PX_LAST2', 'PX_VOLUME2', 'OPEN_INT2',\n",
    "   'VOL Change1', 'Vol Change %1', 'OI Change1', 'OI Change %1',\n",
    "   'CALENDAR', 'Vol Ratio', 'Vol Ratio Change', 'OI Ratio', 'OI Ratio Change',\n",
    "   'VOL Change2', 'Vol Change %2', 'OI Change2', 'OI Change %2'\n",
    "]\n",
    "\n",
    "TARGET_COLUMN = 'CALENDAR'\n",
    "DATE_COLUMN = 'Dates'\n",
    "\n",
    "# Goldman roll parameters\n",
    "GOLDMAN_ROLL_START_DAY = 5\n",
    "GOLDMAN_ROLL_END_DAY = 9\n",
    "GOLDMAN_ROLL_WINDOW = 15\n",
    "\n",
    "# LSTM parameters\n",
    "LSTM_HIDDEN_SIZE = 128\n",
    "LSTM_NUM_LAYERS = 2\n",
    "LSTM_DROPOUT = 0.2\n",
    "LSTM_BIDIRECTIONAL = False\n",
    "LSTM_PROCESSING_DIM = 128\n",
    "HIDDEN_DIMS = [256, 64]\n",
    "\n",
    "# DDQN parameters\n",
    "LEARNING_RATE = 0.0001\n",
    "GAMMA = 0.99 #How much future rewards matter\n",
    "TAU = 0.005  #How fast the \"teacher network\" updates\n",
    "BATCH_SIZE = 16 #How many past trades to learn from at once\n",
    "\n",
    "\n",
    "# Action space\n",
    "POSITION_ACTIONS = [-1.0, -0.5, 0.0, 0.5, 1.0]\n",
    "ACTION_SPACE_SIZE = len(POSITION_ACTIONS)\n",
    "\n",
    "# Reward function\n",
    "REWARD_PNL_WEIGHT = 0.7\n",
    "REWARD_WINRATE_WEIGHT = 0.2\n",
    "REWARD_RISK_WEIGHT = 0.1\n",
    "DRAWDOWN_PENALTY_THRESHOLD = 0.1\n",
    "POSITION_SIZE_PENALTY = 0.01\n",
    "\n",
    "# Training parameters\n",
    "EPISODES = 500\n",
    "MEMORY_SIZE = 4000 #How many past trading experiences the agent remembers\n",
    "UPDATE_FREQUENCY = 2 #How often the neural network actually trains\n",
    "TARGET_UPDATE_FREQUENCY = 200 #How often the \"teacher network\" gets updated\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.01\n",
    "EPSILON_DECAY = 0.996 #Reduce randomness by 0.1% each episode\n",
    "PATIENCE = 100 #Early stopping threshold\n",
    "MIN_IMPROVEMENT = 0.001\n",
    "\n",
    "# Device\n",
    "DEVICE = torch.device(\"cuda\")# if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "# Logging\n",
    "LOG_FREQUENCY = 10\n",
    "SAVE_FREQUENCY = 250\n",
    "MODEL_SAVE_PATH = \"models/\"\n",
    "LOG_SAVE_PATH = \"logs/\"\n",
    "RESULTS_SAVE_PATH = \"results/\"\n",
    "\n",
    "# Backtesting\n",
    "INITIAL_CAPITAL = 10000.0\n",
    "TRACK_METRICS = [\n",
    "   'total_return', 'sharpe_ratio', 'max_drawdown', 'win_rate',\n",
    "   'profit_factor', 'total_trades', 'avg_trade_duration'\n",
    "]\n",
    "\n",
    "# Quick test\n",
    "QUICK_TEST_MODE = False\n",
    "QUICK_TEST_YEARS = 3\n",
    "QUICK_TEST_EPISODES = 100\n",
    "\n",
    "# Plotting\n",
    "PLOT_TRAINING_CURVES = True\n",
    "PLOT_TRADING_RESULTS = True\n",
    "SAVE_PLOTS = True\n",
    "\n",
    "print(f\"Configuration loaded successfully!\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Lookback window: {LOOKBACK_WINDOW} days\")\n",
    "print(f\"Max contracts: {MAX_CONTRACTS}\")\n",
    "print(f\"Tick value: ${TICK_VALUE}\")\n",
    "print(f\"Quick test mode: {QUICK_TEST_MODE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49ee67c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATA HANDLER\n",
    "# =============================================================================\n",
    "\n",
    "class TradingDataHandler:\n",
    "   def __init__(self, csv_path: str):\n",
    "       self.csv_path = csv_path\n",
    "       self.raw_data = None\n",
    "       self.processed_data = None\n",
    "       self.scaler = StandardScaler()\n",
    "       self.feature_columns = FEATURE_COLUMNS\n",
    "       self.date_column = DATE_COLUMN\n",
    "       self.target_column = TARGET_COLUMN\n",
    "       \n",
    "   def load_data(self) -> pd.DataFrame:\n",
    "       print(f\"Loading data from {self.csv_path}...\")\n",
    "       \n",
    "       column_names = [\n",
    "           'PX_OPEN1', 'PX_HIGH1', 'PX_LOW1', 'PX_LAST1', 'PX_VOLUME1', 'OPEN_INT1',\n",
    "           'PX_OPEN2', 'PX_HIGH2', 'PX_LOW2', 'PX_LAST2', 'PX_VOLUME2', 'OPEN_INT2',\n",
    "           'Dates', 'VOL Change1', 'Vol Change %1', 'OI Change1', 'OI Change %1',\n",
    "           'CALENDAR', 'Vol Ratio', 'Vol Ratio Change', 'OI Ratio', 'OI Ratio Change',\n",
    "           'VOL Change2', 'Vol Change %2', 'OI Change2', 'OI Change %2'\n",
    "           ]\n",
    "       \n",
    "       self.raw_data = pd.read_csv(self.csv_path, names=column_names, header=None, skiprows=3)\n",
    "       \n",
    "       self.raw_data['Dates'] = pd.to_datetime(self.raw_data['Dates'], dayfirst=True)\n",
    "       self.raw_data = self.raw_data.sort_values('Dates').reset_index(drop=True)\n",
    "       self.raw_data['date'] = self.raw_data['Dates']\n",
    "\n",
    "       print(f\"Loaded {len(self.raw_data)} rows of data\")\n",
    "       print(f\"Date range: {self.raw_data['Dates'].min()} to {self.raw_data['Dates'].max()}\")\n",
    "       print(f\"CALENDAR spread stats:\")\n",
    "       print(f\"  Min: {self.raw_data['CALENDAR'].min()}\")\n",
    "       print(f\"  Max: {self.raw_data['CALENDAR'].max()}\")\n",
    "       print(f\"  Mean: {self.raw_data['CALENDAR'].mean()}\")\n",
    "       print(f\"  Std: {self.raw_data['CALENDAR'].std()}\")\n",
    "       \n",
    "       return self.raw_data\n",
    "   \n",
    "   def calculate_business_day_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "       df = df.copy()\n",
    "       \n",
    "       df['business_day_of_month'] = df['Dates'].apply(self._get_business_day_of_month)\n",
    "       \n",
    "       df['days_to_roll_start'] = df['business_day_of_month'].apply(\n",
    "           lambda x: max(0, GOLDMAN_ROLL_START_DAY - x) if x < GOLDMAN_ROLL_START_DAY \n",
    "           else 0\n",
    "       )\n",
    "       \n",
    "       df['days_since_roll_end'] = df['business_day_of_month'].apply(\n",
    "           lambda x: max(0, x - GOLDMAN_ROLL_END_DAY) if x > GOLDMAN_ROLL_END_DAY \n",
    "           else 0\n",
    "       )\n",
    "       \n",
    "       df['in_goldman_roll'] = (\n",
    "           (df['business_day_of_month'] >= GOLDMAN_ROLL_START_DAY) & \n",
    "           (df['business_day_of_month'] <= GOLDMAN_ROLL_END_DAY)\n",
    "       ).astype(int)\n",
    "       \n",
    "       df['in_extended_roll_window'] = (\n",
    "           (df['business_day_of_month'] >= GOLDMAN_ROLL_START_DAY - 5) & \n",
    "           (df['business_day_of_month'] <= GOLDMAN_ROLL_END_DAY + 5)\n",
    "       ).astype(int)\n",
    "       \n",
    "       return df\n",
    "   \n",
    "   def _get_business_day_of_month(self, date: datetime) -> int:\n",
    "       first_day = date.replace(day=1)\n",
    "       business_days = 0\n",
    "       current_date = first_day\n",
    "       \n",
    "       while current_date <= date:\n",
    "           if current_date.weekday() < 5:\n",
    "               business_days += 1\n",
    "           current_date += timedelta(days=1)\n",
    "           \n",
    "       return business_days\n",
    "   \n",
    "   def engineer_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "       df = df.copy()\n",
    "   \n",
    "       df['front_hl_ratio'] = df['PX_HIGH1'] / df['PX_LOW1']\n",
    "       df['second_hl_ratio'] = df['PX_HIGH2'] / df['PX_LOW2']\n",
    "       df['front_close_position'] = (df['PX_LAST1'] - df['PX_LOW1']) / (df['PX_HIGH1'] - df['PX_LOW1'])\n",
    "       df['second_close_position'] = (df['PX_LAST2'] - df['PX_LOW2']) / (df['PX_HIGH2'] - df['PX_LOW2'])\n",
    "\n",
    "       df['volume_momentum'] = df['Vol Ratio'].rolling(5).mean()\n",
    "       df['oi_momentum'] = df['OI Ratio'].rolling(5).mean()\n",
    "\n",
    "       df['spread_momentum_5'] = df['CALENDAR'].rolling(5).mean()\n",
    "       df['spread_momentum_10'] = df['CALENDAR'].rolling(10).mean()\n",
    "       df['spread_volatility'] = df['CALENDAR'].rolling(10).std()\n",
    "\n",
    "       df['front_vs_second_strength'] = df['PX_LAST1'] / df['PX_LAST2']\n",
    "   \n",
    "       return df\n",
    "   \n",
    "   def preprocess_data(self) -> pd.DataFrame:\n",
    "       if self.raw_data is None:\n",
    "           self.load_data()\n",
    "           \n",
    "       df = self.raw_data.copy()\n",
    "       \n",
    "       df = df.fillna(method='ffill').fillna(method='bfill')\n",
    "       df = self.calculate_business_day_features(df)\n",
    "       df = self.engineer_features(df)\n",
    "       df = df.dropna()\n",
    "       \n",
    "       self.processed_data = df\n",
    "       print(f\"Preprocessing complete. Final dataset: {len(df)} rows\")\n",
    "       \n",
    "       return df\n",
    "   \n",
    "   def create_sequences(self, df: pd.DataFrame, lookback: int = None) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "       if lookback is None:\n",
    "           lookback = LOOKBACK_WINDOW\n",
    "       \n",
    "       feature_cols = [col for col in df.columns if col not in ['date', 'Dates']]\n",
    "       feature_cols = [col for col in feature_cols if not col.startswith('business_day')]\n",
    "       feature_cols = [col for col in feature_cols if not col.startswith('days_')]\n",
    "       feature_cols = [col for col in feature_cols if not col.startswith('in_goldman')]\n",
    "       feature_cols = [col for col in feature_cols if not col.startswith('in_extended')]\n",
    "   \n",
    "       print(f\"Using {len(feature_cols)} features for training\")\n",
    "       print(f\"Features: {feature_cols}...\")\n",
    "   \n",
    "       feature_data = self.scaler.fit_transform(df[feature_cols])\n",
    "   \n",
    "       sequences = []\n",
    "       targets = []\n",
    "       dates = []\n",
    "   \n",
    "       for i in range(lookback, len(df)):\n",
    "           sequence = feature_data[i-lookback:i].T\n",
    "           sequences.append(sequence)\n",
    "           targets.append(df.iloc[i][self.target_column])\n",
    "           dates.append(df.iloc[i]['Dates'])\n",
    "   \n",
    "       return np.array(sequences), np.array(targets), np.array(dates)\n",
    "   \n",
    "   def split_data(self, sequences: np.ndarray, targets: np.ndarray, dates: np.ndarray, \n",
    "                  quick_test: bool = False) -> Dict:\n",
    "       \n",
    "       if quick_test:\n",
    "           n_years = QUICK_TEST_YEARS\n",
    "           cutoff_date = dates[-1] - pd.DateOffset(years=n_years)\n",
    "           mask = pd.to_datetime(dates) >= cutoff_date\n",
    "           \n",
    "           sequences = sequences[mask]\n",
    "           targets = targets[mask]\n",
    "           dates = dates[mask]\n",
    "           \n",
    "       n_samples = len(sequences)\n",
    "       \n",
    "       train_end = int(n_samples * TRAIN_RATIO)\n",
    "       val_end = int(n_samples * (TRAIN_RATIO + VAL_RATIO))\n",
    "       \n",
    "       data_splits = {\n",
    "           'train': {\n",
    "               'sequences': sequences[:train_end],\n",
    "               'targets': targets[:train_end],\n",
    "               'dates': dates[:train_end]\n",
    "           },\n",
    "           'val': {\n",
    "               'sequences': sequences[train_end:val_end],\n",
    "               'targets': targets[train_end:val_end],\n",
    "               'dates': dates[train_end:val_end]\n",
    "           },\n",
    "           'test': {\n",
    "               'sequences': sequences[val_end:],\n",
    "               'targets': targets[val_end:],\n",
    "               'dates': dates[val_end:]\n",
    "           }\n",
    "       }\n",
    "       \n",
    "       print(f\"Data splits:\")\n",
    "       print(f\"  Train: {len(data_splits['train']['sequences'])} samples\")\n",
    "       print(f\"  Validation: {len(data_splits['val']['sequences'])} samples\") \n",
    "       print(f\"  Test: {len(data_splits['test']['sequences'])} samples\")\n",
    "       \n",
    "       return data_splits\n",
    "   \n",
    "   def get_feature_info(self) -> Dict:\n",
    "       if self.processed_data is None:\n",
    "           self.preprocess_data()\n",
    "           \n",
    "       feature_cols = [col for col in self.processed_data.columns if col not in ['date']]\n",
    "       \n",
    "       return {\n",
    "           'n_features': len(feature_cols),\n",
    "           'feature_names': feature_cols,\n",
    "           'n_samples': len(self.processed_data),\n",
    "           'date_range': (self.processed_data['date'].min(), self.processed_data['date'].max())\n",
    "       }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf54f118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# NEURAL NETWORKS\n",
    "# =============================================================================\n",
    "\n",
    "class LSTMFeatureExtractor(nn.Module):\n",
    "   def __init__(self, n_features: int, sequence_length: int):\n",
    "       super(LSTMFeatureExtractor, self).__init__()\n",
    "       \n",
    "       self.n_features = n_features\n",
    "       self.sequence_length = sequence_length\n",
    "       \n",
    "       self.hidden_size = LSTM_HIDDEN_SIZE\n",
    "       self.num_layers = LSTM_NUM_LAYERS\n",
    "       self.dropout = LSTM_DROPOUT\n",
    "       \n",
    "       self.lstm = nn.LSTM(\n",
    "           input_size=n_features,\n",
    "           hidden_size=self.hidden_size,\n",
    "           num_layers=self.num_layers,\n",
    "           dropout=self.dropout if self.num_layers > 1 else 0,\n",
    "           batch_first=True,\n",
    "           bidirectional=LSTM_BIDIRECTIONAL\n",
    "       )\n",
    "       \n",
    "       self.lstm_output_size = self.hidden_size * (2 if LSTM_BIDIRECTIONAL else 1)\n",
    "       \n",
    "       self.feature_processor = nn.Sequential(\n",
    "           nn.Linear(self.lstm_output_size, LSTM_PROCESSING_DIM),\n",
    "           nn.ReLU(),\n",
    "           nn.Dropout(LSTM_DROPOUT),\n",
    "           nn.Linear(LSTM_PROCESSING_DIM, LSTM_PROCESSING_DIM // 2),\n",
    "           nn.ReLU(),\n",
    "           nn.Dropout(LSTM_DROPOUT)\n",
    "       )\n",
    "       \n",
    "       self.output_dim = LSTM_PROCESSING_DIM // 2\n",
    "       \n",
    "   def forward(self, x):\n",
    "       batch_size = x.size(0)\n",
    "       h0 = torch.zeros(\n",
    "           self.num_layers * (2 if LSTM_BIDIRECTIONAL else 1),\n",
    "           batch_size,\n",
    "           self.hidden_size,\n",
    "           device=x.device\n",
    "       )\n",
    "       c0 = torch.zeros(\n",
    "           self.num_layers * (2 if LSTM_BIDIRECTIONAL else 1),\n",
    "           batch_size,\n",
    "           self.hidden_size,\n",
    "           device=x.device\n",
    "       )\n",
    "       \n",
    "       lstm_out, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "       final_output = lstm_out[:, -1, :]\n",
    "       features = self.feature_processor(final_output)\n",
    "       \n",
    "       return features\n",
    "\n",
    "class DQNNetwork(nn.Module):\n",
    "   def __init__(self, market_feature_dim: int, trading_state_dim: int, \n",
    "                action_space_size: int, sequence_length: int):\n",
    "       super(DQNNetwork, self).__init__()\n",
    "       \n",
    "       self.market_feature_dim = market_feature_dim\n",
    "       self.trading_state_dim = trading_state_dim\n",
    "       self.action_space_size = action_space_size\n",
    "       \n",
    "       self.lstm = LSTMFeatureExtractor(market_feature_dim, sequence_length)\n",
    "       \n",
    "       total_input_dim = self.lstm.output_dim + trading_state_dim\n",
    "       \n",
    "       self.fc_layers = nn.ModuleList()\n",
    "       \n",
    "       self.fc_layers.append(nn.Linear(total_input_dim, HIDDEN_DIMS[0]))\n",
    "       \n",
    "       for i in range(1, len(HIDDEN_DIMS)):\n",
    "           self.fc_layers.append(\n",
    "               nn.Linear(HIDDEN_DIMS[i-1], HIDDEN_DIMS[i])\n",
    "           )\n",
    "       \n",
    "       self.q_values = nn.Linear(HIDDEN_DIMS[-1], action_space_size)\n",
    "       \n",
    "       self.dropout = nn.Dropout(LSTM_DROPOUT)\n",
    "       \n",
    "       self._initialize_weights()\n",
    "   \n",
    "   def _initialize_weights(self):\n",
    "       for module in self.modules():\n",
    "           if isinstance(module, nn.Linear):\n",
    "               nn.init.xavier_uniform_(module.weight)\n",
    "               if module.bias is not None:\n",
    "                   nn.init.constant_(module.bias, 0)\n",
    "           elif isinstance(module, nn.LSTM):\n",
    "               for name, param in module.named_parameters():\n",
    "                   if 'weight_ih' in name:\n",
    "                       nn.init.xavier_uniform_(param.data)\n",
    "                   elif 'weight_hh' in name:\n",
    "                       nn.init.orthogonal_(param.data)\n",
    "                   elif 'bias' in name:\n",
    "                       param.data.fill_(0)\n",
    "   \n",
    "   def forward(self, market_features, trading_state):\n",
    "       lstm_features = self.lstm(market_features)\n",
    "       \n",
    "       combined_features = torch.cat([lstm_features, trading_state], dim=1)\n",
    "       \n",
    "       x = combined_features\n",
    "       for fc_layer in self.fc_layers:\n",
    "           x = fc_layer(x)\n",
    "           x = F.relu(x)\n",
    "           x = self.dropout(x)\n",
    "       \n",
    "       q_values = self.q_values(x)\n",
    "       \n",
    "       return q_values\n",
    "\n",
    "class DoubleDQN(nn.Module):\n",
    "   def __init__(self, market_feature_dim: int, trading_state_dim: int,\n",
    "                action_space_size: int, sequence_length: int):\n",
    "       super(DoubleDQN, self).__init__()\n",
    "       \n",
    "       self.online_net = DQNNetwork(\n",
    "           market_feature_dim, trading_state_dim, action_space_size, sequence_length\n",
    "       )\n",
    "       \n",
    "       self.target_net = DQNNetwork(\n",
    "           market_feature_dim, trading_state_dim, action_space_size, sequence_length\n",
    "       )\n",
    "       \n",
    "       self.update_target_network()\n",
    "       \n",
    "       for param in self.target_net.parameters():\n",
    "           param.requires_grad = False\n",
    "   \n",
    "   def forward(self, market_features, trading_state, use_target=False):\n",
    "       if use_target:\n",
    "           return self.target_net(market_features, trading_state)\n",
    "       else:\n",
    "           return self.online_net(market_features, trading_state)\n",
    "   \n",
    "   def update_target_network(self, tau: float = None):\n",
    "       if tau is None:\n",
    "           self.target_net.load_state_dict(self.online_net.state_dict())\n",
    "       else:\n",
    "           for target_param, online_param in zip(\n",
    "               self.target_net.parameters(), self.online_net.parameters()\n",
    "           ):\n",
    "               target_param.data.copy_(\n",
    "                   tau * online_param.data + (1.0 - tau) * target_param.data\n",
    "               )\n",
    "   \n",
    "   def get_action(self, market_features, trading_state, epsilon=0.0):\n",
    "       if np.random.random() < epsilon:\n",
    "           return np.random.randint(0, self.online_net.action_space_size)\n",
    "       else:\n",
    "           with torch.no_grad():\n",
    "               q_values = self.online_net(market_features, trading_state)\n",
    "               return q_values.argmax().item()\n",
    "\n",
    "class ReplayBuffer:\n",
    "   def __init__(self, capacity: int):\n",
    "       self.capacity = capacity\n",
    "       self.buffer = []\n",
    "       self.position = 0\n",
    "   \n",
    "   def push(self, market_features, trading_state, action, reward, \n",
    "            next_market_features, next_trading_state, done):\n",
    "       if len(self.buffer) < self.capacity:\n",
    "           self.buffer.append(None)\n",
    "       \n",
    "       self.buffer[self.position] = (\n",
    "           market_features, trading_state, action, reward,\n",
    "           next_market_features, next_trading_state, done\n",
    "       )\n",
    "       self.position = (self.position + 1) % self.capacity\n",
    "   \n",
    "   def sample(self, batch_size: int):\n",
    "       batch = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "       \n",
    "       market_features = []\n",
    "       trading_states = []\n",
    "       actions = []\n",
    "       rewards = []\n",
    "       next_market_features = []\n",
    "       next_trading_states = []\n",
    "       dones = []\n",
    "       \n",
    "       for idx in batch:\n",
    "           mf, ts, a, r, nmf, nts, d = self.buffer[idx]\n",
    "           market_features.append(mf)\n",
    "           trading_states.append(ts)\n",
    "           actions.append(a)\n",
    "           rewards.append(r)\n",
    "           next_market_features.append(nmf)\n",
    "           next_trading_states.append(nts)\n",
    "           dones.append(d)\n",
    "       \n",
    "       return (\n",
    "           torch.FloatTensor(market_features),\n",
    "           torch.FloatTensor(trading_states),\n",
    "           torch.LongTensor(actions),\n",
    "           torch.FloatTensor(rewards),\n",
    "           torch.FloatTensor(next_market_features),\n",
    "           torch.FloatTensor(next_trading_states),\n",
    "           torch.BoolTensor(dones)\n",
    "       )\n",
    "   \n",
    "   def __len__(self):\n",
    "       return len(self.buffer)\n",
    "\n",
    "def save_model(model: DoubleDQN, filepath: str, optimizer_state: dict = None, \n",
    "              metadata: dict = None):\n",
    "   checkpoint = {\n",
    "       'online_net_state_dict': model.online_net.state_dict(),\n",
    "       'target_net_state_dict': model.target_net.state_dict(),\n",
    "   }\n",
    "   \n",
    "   if optimizer_state:\n",
    "       checkpoint['optimizer_state_dict'] = optimizer_state\n",
    "   \n",
    "   if metadata:\n",
    "       checkpoint['metadata'] = metadata\n",
    "   \n",
    "   torch.save(checkpoint, filepath)\n",
    "   print(f\"Model saved to {filepath}\")\n",
    "\n",
    "def load_model(model: DoubleDQN, filepath: str, load_optimizer: bool = False):\n",
    "   checkpoint = torch.load(filepath, map_location=DEVICE)\n",
    "   \n",
    "   model.online_net.load_state_dict(checkpoint['online_net_state_dict'])\n",
    "   model.target_net.load_state_dict(checkpoint['target_net_state_dict'])\n",
    "   \n",
    "   optimizer_state = None\n",
    "   if load_optimizer and 'optimizer_state_dict' in checkpoint:\n",
    "       optimizer_state = checkpoint['optimizer_state_dict']\n",
    "   \n",
    "   metadata = checkpoint.get('metadata', {})\n",
    "   \n",
    "   print(f\"Model loaded from {filepath}\")\n",
    "   return optimizer_state, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "025e1363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TRADING ENVIRONMENT\n",
    "# =============================================================================\n",
    "\n",
    "class TradingEnvironment:\n",
    "   def __init__(self, data_sequences: np.ndarray, data_targets: np.ndarray, \n",
    "                data_dates: np.ndarray, initial_capital: float = None):\n",
    "       self.data_sequences = data_sequences\n",
    "       self.data_targets = data_targets\n",
    "       self.data_dates = data_dates\n",
    "       self.n_samples = len(data_sequences)\n",
    "       \n",
    "       self.initial_capital = initial_capital or INITIAL_CAPITAL\n",
    "       self.tick_value = TICK_VALUE\n",
    "       self.max_contracts = MAX_CONTRACTS\n",
    "       self.transaction_cost = TRANSACTION_COST_PER_CONTRACT\n",
    "       self.slippage = BID_ASK_SLIPPAGE if SLIPPAGE_ENABLED else 0.0\n",
    "       \n",
    "       self.max_daily_loss = MAX_DAILY_LOSS\n",
    "       self.stop_loss_enabled = STOP_LOSS_ENABLED\n",
    "       self.stop_loss_pct = STOP_LOSS_PERCENTAGE\n",
    "       \n",
    "       self.reset()\n",
    "       \n",
    "   def reset(self) -> np.ndarray:\n",
    "       self.current_step = 0\n",
    "       self.position = 0\n",
    "       self.cash = self.initial_capital\n",
    "       self.total_pnl = 0.0\n",
    "       self.unrealized_pnl = 0.0\n",
    "       self.daily_pnl = 0.0\n",
    "       \n",
    "       self.trade_history = []\n",
    "       self.equity_curve = [self.initial_capital]\n",
    "       self.daily_returns = []\n",
    "       self.max_drawdown = 0.0\n",
    "       self.peak_equity = self.initial_capital\n",
    "       \n",
    "       self.total_trades = 0\n",
    "       self.winning_trades = 0\n",
    "       self.losing_trades = 0\n",
    "       self.current_trade_entry_price = None\n",
    "       self.current_trade_entry_step = None\n",
    "       \n",
    "       self.daily_loss_tracker = 0.0\n",
    "       self.stop_loss_triggered = False\n",
    "       \n",
    "       return self._get_state()\n",
    "   \n",
    "   def _get_state(self) -> np.ndarray:\n",
    "       if self.current_step >= self.n_samples:\n",
    "           market_features = np.zeros((self.data_sequences.shape[1], self.data_sequences.shape[2]))\n",
    "       else:\n",
    "           market_features = self.data_sequences[self.current_step]\n",
    "       \n",
    "       market_state = market_features.flatten()\n",
    "       \n",
    "       trading_state = np.array([\n",
    "           self.position / self.max_contracts,\n",
    "           self.unrealized_pnl / self.initial_capital,\n",
    "           self.total_pnl / self.initial_capital,\n",
    "           self.daily_pnl / self.initial_capital,\n",
    "           (self.cash + self.unrealized_pnl) / self.initial_capital,\n",
    "           self.max_drawdown,\n",
    "           float(self.stop_loss_triggered),\n",
    "           self.current_step / self.n_samples,\n",
    "       ])\n",
    "       \n",
    "       return np.concatenate([market_state, trading_state])\n",
    "   \n",
    "   def step(self, action: int) -> Tuple[np.ndarray, float, bool, Dict]:\n",
    "       if self.current_step >= self.n_samples - 1:\n",
    "           return self._get_state(), 0.0, True, {'reason': 'end_of_data'}\n",
    "       \n",
    "       current_price = self.data_targets[self.current_step]\n",
    "       next_price = self.data_targets[self.current_step + 1]\n",
    "\n",
    "       oi_ratio_feature_index = 15\n",
    "       self.current_oi_ratio = self.data_sequences[self.current_step][oi_ratio_feature_index][-1]\n",
    "       \n",
    "       position_change_pct = POSITION_ACTIONS[action]\n",
    "       position_change = int(position_change_pct * self.max_contracts)\n",
    "       \n",
    "       new_position = np.clip(\n",
    "           self.position + position_change, \n",
    "           -self.max_contracts, \n",
    "           self.max_contracts\n",
    "       )\n",
    "       actual_position_change = new_position - self.position\n",
    "       \n",
    "       transaction_cost = abs(actual_position_change) * self.transaction_cost\n",
    "       slippage_cost = abs(actual_position_change) * self.slippage * self.tick_value\n",
    "       total_cost = transaction_cost + slippage_cost\n",
    "       \n",
    "       self.position = new_position\n",
    "       self.cash -= total_cost\n",
    "       \n",
    "       self.current_step += 1\n",
    "       \n",
    "       if self.position != 0:\n",
    "           price_change = next_price - current_price\n",
    "           position_pnl = self.position * price_change * self.tick_value\n",
    "           self.unrealized_pnl += position_pnl\n",
    "           self.daily_pnl += position_pnl\n",
    "       \n",
    "       if actual_position_change != 0 and self.current_trade_entry_price is not None:\n",
    "           if self.position == 0:\n",
    "               self._record_trade()\n",
    "       \n",
    "       if self.position != 0 and self.current_trade_entry_price is None:\n",
    "           self.current_trade_entry_price = current_price\n",
    "           self.current_trade_entry_step = self.current_step\n",
    "       \n",
    "       current_equity = self.cash + self.unrealized_pnl\n",
    "       self.equity_curve.append(current_equity)\n",
    "       \n",
    "       if current_equity > self.peak_equity:\n",
    "           self.peak_equity = current_equity\n",
    "       \n",
    "       current_drawdown = (self.peak_equity - current_equity) / self.peak_equity\n",
    "       self.max_drawdown = max(self.max_drawdown, current_drawdown)\n",
    "       \n",
    "       done = False\n",
    "       info = {}\n",
    "       \n",
    "       if self.daily_pnl < -self.max_daily_loss:\n",
    "           done = True\n",
    "           info['reason'] = 'daily_loss_limit'\n",
    "           self.stop_loss_triggered = True\n",
    "       \n",
    "       if self.stop_loss_enabled and current_drawdown > self.stop_loss_pct:\n",
    "           done = True\n",
    "           info['reason'] = 'stop_loss'\n",
    "           self.stop_loss_triggered = True\n",
    "       \n",
    "       reward = self._calculate_reward()\n",
    "       \n",
    "       if self.current_step % 1 == 0:\n",
    "           self.daily_pnl = 0.0\n",
    "       \n",
    "       next_state = self._get_state()\n",
    "       \n",
    "       info.update({\n",
    "           'position': self.position,\n",
    "           'cash': self.cash,\n",
    "           'unrealized_pnl': self.unrealized_pnl,\n",
    "           'total_pnl': self.total_pnl,\n",
    "           'equity': current_equity,\n",
    "           'drawdown': current_drawdown,\n",
    "           'transaction_cost': total_cost,\n",
    "           'current_price': next_price\n",
    "       })\n",
    "       \n",
    "       return next_state, reward, done, info\n",
    "   \n",
    "   def _calculate_reward(self) -> float:\n",
    "    \"\"\"Calculate reward based on multiple factors\"\"\"\n",
    "    current_equity = self.cash + self.unrealized_pnl\n",
    "    \n",
    "    # P&L component (normalized by initial capital)\n",
    "    pnl_reward = self.daily_pnl / self.initial_capital\n",
    "\n",
    "    # Win rate component\n",
    "    win_rate = self.winning_trades / max(1, self.total_trades)\n",
    "    win_rate_reward = (win_rate - 0.5) * 2\n",
    "\n",
    "    # OI Ratio constraint penalty - INITIALIZE FIRST\n",
    "    oi_penalty = 0.0\n",
    "    if hasattr(self, 'current_oi_ratio') and self.current_oi_ratio >= 1.5:\n",
    "        if self.position != 0:\n",
    "            oi_penalty = 5.0  # Strong penalty for trading when OI ratio >= 1.5\n",
    "        else:\n",
    "            oi_penalty = -1  # Small bonus for staying flat when OI ratio >= 1.5\n",
    "    if hasattr(self, 'current_oi_ratio') and self.current_oi_ratio < 0.9:\n",
    "        if self.position != 0:\n",
    "            oi_penalty = -2.0  # Bonus for trading when OI ratio < 0.8 (good time)\n",
    "\n",
    "    # Risk penalty component - INITIALIZE FIRST\n",
    "    risk_penalty = 0.0\n",
    "    if self.max_drawdown > DRAWDOWN_PENALTY_THRESHOLD:\n",
    "        risk_penalty += (self.max_drawdown - DRAWDOWN_PENALTY_THRESHOLD) * 10\n",
    "\n",
    "    # Combine components\n",
    "    reward = (\n",
    "        REWARD_PNL_WEIGHT * pnl_reward +\n",
    "        REWARD_WINRATE_WEIGHT * win_rate_reward -\n",
    "        REWARD_RISK_WEIGHT * risk_penalty -\n",
    "        oi_penalty  # Subtract OI penalty\n",
    "    )   \n",
    "\n",
    "    return reward\n",
    "   \n",
    "   def _record_trade(self):\n",
    "       if self.current_trade_entry_price is None:\n",
    "           return\n",
    "           \n",
    "       current_price = self.data_targets[self.current_step]\n",
    "       trade_pnl = (current_price - self.current_trade_entry_price) * self.position * self.tick_value\n",
    "       \n",
    "       self.total_trades += 1\n",
    "       \n",
    "       if trade_pnl > 0:\n",
    "           self.winning_trades += 1\n",
    "       else:\n",
    "           self.losing_trades += 1\n",
    "       \n",
    "       trade_info = {\n",
    "           'entry_price': self.current_trade_entry_price,\n",
    "           'exit_price': current_price,\n",
    "           'entry_step': self.current_trade_entry_step,\n",
    "           'exit_step': self.current_step,\n",
    "           'position': self.position,\n",
    "           'pnl': trade_pnl,\n",
    "           'duration': self.current_step - self.current_trade_entry_step\n",
    "       }\n",
    "       \n",
    "       self.trade_history.append(trade_info)\n",
    "       self.total_pnl += trade_pnl\n",
    "       \n",
    "       self.current_trade_entry_price = None\n",
    "       self.current_trade_entry_step = None\n",
    "   \n",
    "   def get_performance_metrics(self) -> Dict:\n",
    "       if len(self.equity_curve) < 2:\n",
    "           return {}\n",
    "       \n",
    "       returns = np.diff(self.equity_curve) / self.equity_curve[:-1]\n",
    "       total_return = (self.equity_curve[-1] - self.initial_capital) / self.initial_capital\n",
    "       \n",
    "       if len(returns) > 1 and np.std(returns) > 0:\n",
    "           sharpe_ratio = np.mean(returns) / np.std(returns) * np.sqrt(252)\n",
    "       else:\n",
    "           sharpe_ratio = 0.0\n",
    "       \n",
    "       win_rate = self.winning_trades / max(1, self.total_trades)\n",
    "       \n",
    "       if self.losing_trades > 0:\n",
    "           winning_pnl = sum([trade['pnl'] for trade in self.trade_history if trade['pnl'] > 0])\n",
    "           losing_pnl = abs(sum([trade['pnl'] for trade in self.trade_history if trade['pnl'] < 0]))\n",
    "           profit_factor = winning_pnl / losing_pnl if losing_pnl > 0 else float('inf')\n",
    "       else:\n",
    "           profit_factor = float('inf') if self.winning_trades > 0 else 0.0\n",
    "       \n",
    "       if self.trade_history:\n",
    "           avg_trade_duration = np.mean([trade['duration'] for trade in self.trade_history])\n",
    "       else:\n",
    "           avg_trade_duration = 0.0\n",
    "       \n",
    "       return {\n",
    "           'total_return': total_return,\n",
    "           'sharpe_ratio': sharpe_ratio,\n",
    "           'max_drawdown': self.max_drawdown,\n",
    "           'win_rate': win_rate,\n",
    "           'profit_factor': profit_factor,\n",
    "           'total_trades': self.total_trades,\n",
    "           'winning_trades': self.winning_trades,\n",
    "           'losing_trades': self.losing_trades,\n",
    "           'avg_trade_duration': avg_trade_duration,\n",
    "           'final_equity': self.equity_curve[-1],\n",
    "           'total_pnl': self.total_pnl,\n",
    "           'unrealized_pnl': self.unrealized_pnl\n",
    "       }\n",
    "   \n",
    "   def get_action_space_size(self) -> int:\n",
    "       return len(POSITION_ACTIONS)\n",
    "   \n",
    "   def get_state_size(self) -> int:\n",
    "       if self.current_step < self.n_samples:\n",
    "           market_features = self.data_sequences[self.current_step].flatten()\n",
    "       else:\n",
    "           market_features = np.zeros((self.data_sequences.shape[1] * self.data_sequences.shape[2]))\n",
    "       \n",
    "       trading_features = 8\n",
    "       return len(market_features) + trading_features\n",
    "   \n",
    "   def render(self, mode='human'):\n",
    "       if mode == 'human':\n",
    "           print(f\"Step: {self.current_step}\")\n",
    "           print(f\"Position: {self.position} contracts\")\n",
    "           print(f\"Cash: ${self.cash:,.2f}\")\n",
    "           print(f\"Unrealized P&L: ${self.unrealized_pnl:,.2f}\")\n",
    "           print(f\"Total P&L: ${self.total_pnl:,.2f}\")\n",
    "           print(f\"Equity: ${self.cash + self.unrealized_pnl:,.2f}\")\n",
    "           print(f\"Drawdown: {self.max_drawdown:.2%}\")\n",
    "           print(f\"Trades: {self.total_trades} (Win Rate: {self.winning_trades/max(1,self.total_trades):.1%})\")\n",
    "           print(\"-\" * 50)\n",
    "\n",
    "class TradingEnvironmentWrapper:\n",
    "   def __init__(self, data_splits: Dict):\n",
    "       self.data_splits = data_splits\n",
    "       \n",
    "   def create_env(self, split: str = 'train') -> TradingEnvironment:\n",
    "       if split not in self.data_splits:\n",
    "           raise ValueError(f\"Split '{split}' not found. Available: {list(self.data_splits.keys())}\")\n",
    "       \n",
    "       data = self.data_splits[split]\n",
    "       return TradingEnvironment(\n",
    "           data_sequences=data['sequences'],\n",
    "           data_targets=data['targets'],\n",
    "           data_dates=data['dates']\n",
    "       )\n",
    "   \n",
    "   def get_env_info(self) -> Dict:\n",
    "       info = {}\n",
    "       for split in self.data_splits:\n",
    "           env = self.create_env(split)\n",
    "           info[split] = {\n",
    "               'n_samples': env.n_samples,\n",
    "               'state_size': env.get_state_size(),\n",
    "               'action_space_size': env.get_action_space_size(),\n",
    "               'date_range': (env.data_dates[0], env.data_dates[-1])\n",
    "           }\n",
    "       return info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b4784e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DDQN AGENT\n",
    "# =============================================================================\n",
    "\n",
    "class DDQNTradingAgent:\n",
    "   def __init__(self, market_feature_dim: int, trading_state_dim: int, \n",
    "                action_space_size: int, lookback_window: int):\n",
    "       self.market_feature_dim = market_feature_dim\n",
    "       self.trading_state_dim = trading_state_dim\n",
    "       self.action_space_size = action_space_size\n",
    "       self.lookback_window = lookback_window\n",
    "       \n",
    "       self.ddqn = DoubleDQN(\n",
    "           market_feature_dim, trading_state_dim, action_space_size, lookback_window\n",
    "       ).to(DEVICE)\n",
    "       \n",
    "       self.optimizer = optim.Adam(\n",
    "           self.ddqn.online_net.parameters(), \n",
    "           lr=LEARNING_RATE\n",
    "       )\n",
    "       \n",
    "       self.criterion = nn.MSELoss()\n",
    "       \n",
    "       self.replay_buffer = ReplayBuffer(MEMORY_SIZE)\n",
    "       \n",
    "       self.epsilon = EPSILON_START\n",
    "       self.epsilon_min = EPSILON_END\n",
    "       self.epsilon_decay = EPSILON_DECAY\n",
    "       self.gamma = GAMMA\n",
    "       self.tau = TAU\n",
    "       self.batch_size = BATCH_SIZE\n",
    "       self.update_frequency = UPDATE_FREQUENCY\n",
    "       self.target_update_frequency = TARGET_UPDATE_FREQUENCY\n",
    "       \n",
    "       self.training_step = 0\n",
    "       self.episode_rewards = []\n",
    "       self.episode_losses = []\n",
    "       self.episode_epsilons = []\n",
    "       self.training_metrics = {\n",
    "           'rewards': [],\n",
    "           'losses': [],\n",
    "           'epsilons': [],\n",
    "           'q_values': [],\n",
    "           'actions_taken': []\n",
    "       }\n",
    "       \n",
    "       print(f\"DDQN Agent initialized with {self._count_parameters()} parameters\")\n",
    "       print(f\"Device: {DEVICE}\")\n",
    "   \n",
    "   def _count_parameters(self) -> int:\n",
    "       return sum(p.numel() for p in self.ddqn.online_net.parameters() if p.requires_grad)\n",
    "   \n",
    "   def _state_to_tensors(self, state: np.ndarray) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "       market_size = self.market_feature_dim * self.lookback_window\n",
    "       market_features = state[:market_size].reshape(self.market_feature_dim, self.lookback_window)\n",
    "       trading_state = state[market_size:]\n",
    "       \n",
    "       market_tensor = torch.FloatTensor(market_features).permute(1, 0).unsqueeze(0).to(DEVICE)\n",
    "       trading_tensor = torch.FloatTensor(trading_state).unsqueeze(0).to(DEVICE)\n",
    "       \n",
    "       return market_tensor, trading_tensor\n",
    "   \n",
    "   def get_action(self, state: np.ndarray, training: bool = True) -> int:\n",
    "       if training and random.random() < self.epsilon:\n",
    "           action = random.randint(0, self.action_space_size - 1)\n",
    "           self.training_metrics['actions_taken'].append(('random', action))\n",
    "       else:\n",
    "           market_features, trading_state = self._state_to_tensors(state)\n",
    "           \n",
    "           with torch.no_grad():\n",
    "               q_values = self.ddqn(market_features, trading_state, use_target=False)\n",
    "               action = q_values.argmax().item()\n",
    "               \n",
    "               if training:\n",
    "                   self.training_metrics['q_values'].append(q_values.cpu().numpy()[0])\n",
    "                   self.training_metrics['actions_taken'].append(('greedy', action))\n",
    "       \n",
    "       return action\n",
    "   \n",
    "   def store_transition(self, state: np.ndarray, action: int, reward: float,\n",
    "                       next_state: np.ndarray, done: bool):\n",
    "       market_features, trading_state = self._state_to_tensors(state)\n",
    "       next_market_features, next_trading_state = self._state_to_tensors(next_state)\n",
    "       \n",
    "       self.replay_buffer.push(\n",
    "           market_features.squeeze(0).cpu().numpy(),\n",
    "           trading_state.squeeze(0).cpu().numpy(),\n",
    "           action,\n",
    "           reward,\n",
    "           next_market_features.squeeze(0).cpu().numpy(),\n",
    "           next_trading_state.squeeze(0).cpu().numpy(),\n",
    "           done\n",
    "       )\n",
    "   \n",
    "   def train_step(self) -> float:\n",
    "       if len(self.replay_buffer) < self.batch_size:\n",
    "           return 0.0\n",
    "       \n",
    "       batch = self.replay_buffer.sample(self.batch_size)\n",
    "       market_features, trading_states, actions, rewards, next_market_features, next_trading_states, dones = batch\n",
    "       \n",
    "       market_features = market_features.to(DEVICE)\n",
    "       trading_states = trading_states.to(DEVICE)\n",
    "       actions = actions.to(DEVICE)\n",
    "       rewards = rewards.to(DEVICE)\n",
    "       next_market_features = next_market_features.to(DEVICE)\n",
    "       next_trading_states = next_trading_states.to(DEVICE)\n",
    "       dones = dones.to(DEVICE)\n",
    "       \n",
    "       current_q_values = self.ddqn(market_features, trading_states, use_target=False)\n",
    "       current_q_values = current_q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "       \n",
    "       with torch.no_grad():\n",
    "           next_q_values_online = self.ddqn(next_market_features, next_trading_states, use_target=False)\n",
    "           next_actions = next_q_values_online.argmax(1)\n",
    "           \n",
    "           next_q_values_target = self.ddqn(next_market_features, next_trading_states, use_target=True)\n",
    "           next_q_values = next_q_values_target.gather(1, next_actions.unsqueeze(1)).squeeze(1)\n",
    "           \n",
    "           target_q_values = rewards + (self.gamma * next_q_values * ~dones)\n",
    "       \n",
    "       loss = self.criterion(current_q_values, target_q_values)\n",
    "       \n",
    "       self.optimizer.zero_grad()\n",
    "       loss.backward()\n",
    "       \n",
    "       torch.nn.utils.clip_grad_norm_(self.ddqn.online_net.parameters(), 1.0)\n",
    "       \n",
    "       self.optimizer.step()\n",
    "       \n",
    "       if self.training_step % self.target_update_frequency == 0:\n",
    "           self.ddqn.update_target_network(self.tau)\n",
    "       \n",
    "       if self.epsilon > self.epsilon_min:\n",
    "           self.epsilon *= self.epsilon_decay\n",
    "       \n",
    "       self.training_step += 1\n",
    "       \n",
    "       return loss.item()\n",
    "   \n",
    "   def train_episode(self, env, episode: int) -> Dict:\n",
    "       state = env.reset()\n",
    "       total_reward = 0.0\n",
    "       total_loss = 0.0\n",
    "       steps = 0\n",
    "       episode_actions = []\n",
    "       \n",
    "       while True:\n",
    "           action = self.get_action(state, training=True)\n",
    "           episode_actions.append(action)\n",
    "           \n",
    "           next_state, reward, done, info = env.step(action)\n",
    "           \n",
    "           self.store_transition(state, action, reward, next_state, done)\n",
    "           \n",
    "           if len(self.replay_buffer) >= self.batch_size and steps % self.update_frequency == 0:\n",
    "               loss = self.train_step()\n",
    "               total_loss += loss\n",
    "           \n",
    "           total_reward += reward\n",
    "           steps += 1\n",
    "           state = next_state\n",
    "           \n",
    "           if done:\n",
    "               break\n",
    "       \n",
    "       episode_metrics = {\n",
    "           'episode': episode,\n",
    "           'total_reward': total_reward,\n",
    "           'average_loss': total_loss / max(1, steps // self.update_frequency),\n",
    "           'steps': steps,\n",
    "           'epsilon': self.epsilon,\n",
    "           'actions': episode_actions,\n",
    "           'final_performance': env.get_performance_metrics()\n",
    "       }\n",
    "       \n",
    "       self.episode_rewards.append(total_reward)\n",
    "       self.episode_losses.append(total_loss / max(1, steps // self.update_frequency))\n",
    "       self.episode_epsilons.append(self.epsilon)\n",
    "       \n",
    "       return episode_metrics\n",
    "   \n",
    "   def evaluate_episode(self, env) -> Dict:\n",
    "       state = env.reset()\n",
    "       total_reward = 0.0\n",
    "       steps = 0\n",
    "       episode_actions = []\n",
    "       \n",
    "       while True:\n",
    "           action = self.get_action(state, training=False)\n",
    "           episode_actions.append(action)\n",
    "           \n",
    "           next_state, reward, done, info = env.step(action)\n",
    "           \n",
    "           total_reward += reward\n",
    "           steps += 1\n",
    "           state = next_state\n",
    "           \n",
    "           if done:\n",
    "               break\n",
    "       \n",
    "       performance_metrics = env.get_performance_metrics()\n",
    "       \n",
    "       eval_metrics = {\n",
    "           'total_reward': total_reward,\n",
    "           'steps': steps,\n",
    "           'actions': episode_actions,\n",
    "           'performance': performance_metrics\n",
    "       }\n",
    "       \n",
    "       return eval_metrics\n",
    "   \n",
    "   def train(self, train_env, val_env, num_episodes: int = None) -> Dict:\n",
    "       if num_episodes is None:\n",
    "           num_episodes = EPISODES\n",
    "       \n",
    "       training_history = {\n",
    "           'train_rewards': [],\n",
    "           'val_rewards': [],\n",
    "           'train_performance': [],\n",
    "           'val_performance': [],\n",
    "           'losses': [],\n",
    "           'epsilons': []\n",
    "       }\n",
    "       \n",
    "       best_val_reward = float('-inf')\n",
    "       patience_counter = 0\n",
    "       \n",
    "       print(f\"Starting training for {num_episodes} episodes...\")\n",
    "       print(f\"Replay buffer size: {MEMORY_SIZE}\")\n",
    "       print(f\"Batch size: {self.batch_size}\")\n",
    "       print(f\"Update frequency: {self.update_frequency}\")\n",
    "       \n",
    "       for episode in range(num_episodes):\n",
    "           train_metrics = self.train_episode(train_env, episode)\n",
    "           \n",
    "           if episode % LOG_FREQUENCY == 0:\n",
    "               val_metrics = self.evaluate_episode(val_env)\n",
    "               \n",
    "               training_history['train_rewards'].append(train_metrics['total_reward'])\n",
    "               training_history['val_rewards'].append(val_metrics['total_reward'])\n",
    "               training_history['train_performance'].append(train_metrics['final_performance'])\n",
    "               training_history['val_performance'].append(val_metrics['performance'])\n",
    "               training_history['losses'].append(train_metrics['average_loss'])\n",
    "               training_history['epsilons'].append(train_metrics['epsilon'])\n",
    "               \n",
    "               print(f\"Episode {episode}/{num_episodes}\")\n",
    "               print(f\"  Train Reward: {train_metrics['total_reward']:.4f}\")\n",
    "               print(f\"  Val Reward: {val_metrics['total_reward']:.4f}\")\n",
    "               print(f\"  Loss: {train_metrics['average_loss']:.6f}\")\n",
    "               print(f\"  Epsilon: {train_metrics['epsilon']:.4f}\")\n",
    "               print(f\"  Val Performance: {val_metrics['performance'].get('total_return', 0):.2%}\")\n",
    "               print(\"-\" * 50)\n",
    "               \n",
    "               if val_metrics['total_reward'] > best_val_reward + MIN_IMPROVEMENT:\n",
    "                   best_val_reward = val_metrics['total_reward']\n",
    "                   patience_counter = 0\n",
    "               else:\n",
    "                   patience_counter += 1\n",
    "               \n",
    "               if patience_counter >= PATIENCE:\n",
    "                   print(f\"Early stopping at episode {episode}\")\n",
    "                   break\n",
    "       \n",
    "       print(\"Training completed!\")\n",
    "       return training_history\n",
    "   \n",
    "   def backtest(self, test_env) -> Dict:\n",
    "       print(\"Starting backtesting...\")\n",
    "       \n",
    "       self.ddqn.online_net.eval()\n",
    "       \n",
    "       results = self.evaluate_episode(test_env)\n",
    "       \n",
    "       performance = results['performance']\n",
    "       actions = results['actions']\n",
    "       \n",
    "       action_counts = {}\n",
    "       for action in actions:\n",
    "           action_counts[action] = action_counts.get(action, 0) + 1\n",
    "       \n",
    "       backtest_results = {\n",
    "           'performance_metrics': performance,\n",
    "           'total_reward': results['total_reward'],\n",
    "           'total_steps': results['steps'],\n",
    "           'action_distribution': action_counts,\n",
    "           'action_sequence': actions\n",
    "       }\n",
    "       \n",
    "       print(\"\\n\" + \"=\"*60)\n",
    "       print(\"BACKTESTING RESULTS\")\n",
    "       print(\"=\"*60)\n",
    "       \n",
    "       print(f\"Total Return: {performance.get('total_return', 0):.2%}\")\n",
    "       print(f\"Sharpe Ratio: {performance.get('sharpe_ratio', 0):.2f}\")\n",
    "       print(f\"Max Drawdown: {performance.get('max_drawdown', 0):.2%}\")\n",
    "       print(f\"Win Rate: {performance.get('win_rate', 0):.1%}\")\n",
    "       print(f\"Profit Factor: {performance.get('profit_factor', 0):.2f}\")\n",
    "       print(f\"Total Trades: {performance.get('total_trades', 0)}\")\n",
    "       print(f\"Final Equity: ${performance.get('final_equity', 0):,.2f}\")\n",
    "       \n",
    "       print(f\"\\nAction Distribution:\")\n",
    "       for action, count in action_counts.items():\n",
    "           action_pct = POSITION_ACTIONS[action]\n",
    "           print(f\"  Action {action} ({action_pct:+.0%}): {count} times ({count/len(actions):.1%})\")\n",
    "       \n",
    "       return backtest_results\n",
    "   \n",
    "   def plot_training_curves(self, training_history: Dict, save_path: str = None):\n",
    "       fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "       \n",
    "       axes[0, 0].plot(training_history['train_rewards'], label='Train', alpha=0.7)\n",
    "       axes[0, 0].plot(training_history['val_rewards'], label='Validation', alpha=0.7)\n",
    "       axes[0, 0].set_title('Episode Rewards')\n",
    "       axes[0, 0].set_xlabel('Episode')\n",
    "       axes[0, 0].set_ylabel('Total Reward')\n",
    "       axes[0, 0].legend()\n",
    "       axes[0, 0].grid(True)\n",
    "       \n",
    "       axes[0, 1].plot(training_history['losses'], color='red', alpha=0.7)\n",
    "       axes[0, 1].set_title('Training Loss')\n",
    "       axes[0, 1].set_xlabel('Episode')\n",
    "       axes[0, 1].set_ylabel('Average Loss')\n",
    "       axes[0, 1].grid(True)\n",
    "       \n",
    "       axes[1, 0].plot(training_history['epsilons'], color='green', alpha=0.7)\n",
    "       axes[1, 0].set_title('Epsilon Decay')\n",
    "       axes[1, 0].set_xlabel('Episode')\n",
    "       axes[1, 0].set_ylabel('Epsilon')\n",
    "       axes[1, 0].grid(True)\n",
    "       \n",
    "       if training_history['val_performance']:\n",
    "           returns = [p.get('total_return', 0) for p in training_history['val_performance']]\n",
    "           sharpes = [p.get('sharpe_ratio', 0) for p in training_history['val_performance']]\n",
    "           \n",
    "           ax2 = axes[1, 1]\n",
    "           ax3 = ax2.twinx()\n",
    "           \n",
    "           line1 = ax2.plot(returns, 'b-', label='Total Return', alpha=0.7)\n",
    "           line2 = ax3.plot(sharpes, 'r-', label='Sharpe Ratio', alpha=0.7)\n",
    "           \n",
    "           ax2.set_xlabel('Episode')\n",
    "           ax2.set_ylabel('Total Return', color='b')\n",
    "           ax3.set_ylabel('Sharpe Ratio', color='r')\n",
    "           ax2.set_title('Validation Performance')\n",
    "           \n",
    "           lines = line1 + line2\n",
    "           labels = [l.get_label() for l in lines]\n",
    "           ax2.legend(lines, labels, loc='upper left')\n",
    "           ax2.grid(True)\n",
    "       \n",
    "       plt.tight_layout()\n",
    "       \n",
    "       if save_path:\n",
    "           plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "       \n",
    "       if PLOT_TRAINING_CURVES:\n",
    "           plt.show()\n",
    "\n",
    "def create_agent_from_env(env) -> DDQNTradingAgent:\n",
    "   state = env.reset()\n",
    "   state_size = len(state)\n",
    "   \n",
    "   market_size = env.data_sequences.shape[1] * env.data_sequences.shape[2]\n",
    "   trading_state_size = state_size - market_size\n",
    "   \n",
    "   agent = DDQNTradingAgent(\n",
    "       market_feature_dim=env.data_sequences.shape[1],\n",
    "       trading_state_dim=trading_state_size,\n",
    "       action_space_size=env.get_action_space_size(),\n",
    "       lookback_window=env.data_sequences.shape[2]\n",
    "   )\n",
    "   \n",
    "   return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ca7fd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MAIN EXECUTION FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def create_directories():\n",
    "   directories = [MODEL_SAVE_PATH, LOG_SAVE_PATH, RESULTS_SAVE_PATH]\n",
    "   for directory in directories:\n",
    "       os.makedirs(directory, exist_ok=True)\n",
    "   print(\"Directories created successfully\")\n",
    "\n",
    "def load_and_prepare_data(csv_path: str):\n",
    "   print(\"Loading and preparing data...\")\n",
    "   \n",
    "   data_handler = TradingDataHandler(csv_path)\n",
    "   \n",
    "   raw_data = data_handler.load_data()\n",
    "   processed_data = data_handler.preprocess_data()\n",
    "   \n",
    "   sequences, targets, dates = data_handler.create_sequences(\n",
    "       processed_data, \n",
    "       lookback=LOOKBACK_WINDOW\n",
    "   )\n",
    "   \n",
    "   data_splits = data_handler.split_data(\n",
    "       sequences, targets, dates, \n",
    "       quick_test=QUICK_TEST_MODE\n",
    "   )\n",
    "   \n",
    "   feature_info = data_handler.get_feature_info()\n",
    "   print(f\"\\nData Information:\")\n",
    "   print(f\"  Features: {feature_info['n_features']}\")\n",
    "   print(f\"  Samples: {feature_info['n_samples']}\")\n",
    "   print(f\"  Date range: {feature_info['date_range'][0]} to {feature_info['date_range'][1]}\")\n",
    "   print(f\"  Lookback window: {LOOKBACK_WINDOW} days\")\n",
    "   \n",
    "   return data_splits, data_handler\n",
    "\n",
    "def create_environments(data_splits):\n",
    "   print(\"Creating trading environments...\")\n",
    "   \n",
    "   env_wrapper = TradingEnvironmentWrapper(data_splits)\n",
    "   \n",
    "   train_env = env_wrapper.create_env('train')\n",
    "   val_env = env_wrapper.create_env('val')\n",
    "   test_env = env_wrapper.create_env('test')\n",
    "   \n",
    "   env_info = env_wrapper.get_env_info()\n",
    "   for split, info in env_info.items():\n",
    "       print(f\"\\n{split.upper()} Environment:\")\n",
    "       print(f\"  Samples: {info['n_samples']}\")\n",
    "       print(f\"  State size: {info['state_size']}\")\n",
    "       print(f\"  Action space: {info['action_space_size']}\")\n",
    "       print(f\"  Date range: {info['date_range'][0]} to {info['date_range'][1]}\")\n",
    "   \n",
    "   return env_wrapper, train_env, val_env, test_env\n",
    "\n",
    "def train_agent(train_env, val_env):\n",
    "   print(\"\\nInitializing DDQN agent...\")\n",
    "   \n",
    "   agent = create_agent_from_env(train_env)\n",
    "   \n",
    "   num_episodes = QUICK_TEST_EPISODES if QUICK_TEST_MODE else EPISODES\n",
    "   \n",
    "   print(f\"Starting training for {num_episodes} episodes...\")\n",
    "   print(f\"Quick test mode: {QUICK_TEST_MODE}\")\n",
    "   \n",
    "   training_history = agent.train(train_env, val_env, num_episodes)\n",
    "   \n",
    "   if PLOT_TRAINING_CURVES:\n",
    "       save_path = f\"{RESULTS_SAVE_PATH}training_curves.png\" if SAVE_PLOTS else None\n",
    "       agent.plot_training_curves(training_history, save_path)\n",
    "   \n",
    "   return agent, training_history\n",
    "\n",
    "def backtest_agent(agent, test_env):\n",
    "   print(\"\\nStarting backtesting...\")\n",
    "   \n",
    "   backtest_results = agent.backtest(test_env)\n",
    "   \n",
    "   return backtest_results\n",
    "\n",
    "def plot_backtest_results(test_env, backtest_results):\n",
    "   if not PLOT_TRADING_RESULTS:\n",
    "       return\n",
    "   \n",
    "   fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "   \n",
    "   equity_curve = test_env.equity_curve\n",
    "   dates = test_env.data_dates[:len(equity_curve)]\n",
    "   \n",
    "   axes[0, 0].plot(dates, equity_curve, linewidth=2, alpha=0.8)\n",
    "   axes[0, 0].set_title('Equity Curve', fontsize=14, fontweight='bold')\n",
    "   axes[0, 0].set_xlabel('Date')\n",
    "   axes[0, 0].set_ylabel('Equity ($)')\n",
    "   axes[0, 0].grid(True, alpha=0.3)\n",
    "   axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "   \n",
    "   peak = np.maximum.accumulate(equity_curve)\n",
    "   drawdown = (peak - equity_curve) / peak\n",
    "   \n",
    "   axes[0, 1].fill_between(dates, drawdown, 0, alpha=0.7, color='red')\n",
    "   axes[0, 1].set_title('Drawdown', fontsize=14, fontweight='bold')\n",
    "   axes[0, 1].set_xlabel('Date')\n",
    "   axes[0, 1].set_ylabel('Drawdown (%)')\n",
    "   axes[0, 1].yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: '{:.1%}'.format(y)))\n",
    "   axes[0, 1].grid(True, alpha=0.3)\n",
    "   axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "   \n",
    "   action_dist = backtest_results['action_distribution']\n",
    "   actions = list(action_dist.keys())\n",
    "   counts = list(action_dist.values())\n",
    "   action_labels = [f\"Action {a}\\n({POSITION_ACTIONS[a]:+.0%})\" for a in actions]\n",
    "   \n",
    "   axes[1, 0].bar(action_labels, counts, alpha=0.8)\n",
    "   axes[1, 0].set_title('Action Distribution', fontsize=14, fontweight='bold')\n",
    "   axes[1, 0].set_xlabel('Action')\n",
    "   axes[1, 0].set_ylabel('Frequency')\n",
    "   axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "   axes[1, 0].grid(True, alpha=0.3)\n",
    "   \n",
    "   performance = backtest_results['performance_metrics']\n",
    "   metrics_names = ['Total Return', 'Sharpe Ratio', 'Max Drawdown', 'Win Rate', 'Profit Factor']\n",
    "   metrics_values = [\n",
    "       performance.get('total_return', 0),\n",
    "       performance.get('sharpe_ratio', 0),\n",
    "       performance.get('max_drawdown', 0),\n",
    "       performance.get('win_rate', 0),\n",
    "       performance.get('profit_factor', 0)\n",
    "   ]\n",
    "   \n",
    "   formatted_values = [\n",
    "       f\"{metrics_values[0]:.1%}\",\n",
    "       f\"{metrics_values[1]:.2f}\",\n",
    "       f\"{metrics_values[2]:.1%}\",\n",
    "       f\"{metrics_values[3]:.1%}\",\n",
    "       f\"{metrics_values[4]:.2f}\"\n",
    "   ]\n",
    "   \n",
    "   bars = axes[1, 1].bar(metrics_names, metrics_values, alpha=0.8)\n",
    "   axes[1, 1].set_title('Performance Metrics', fontsize=14, fontweight='bold')\n",
    "   axes[1, 1].set_ylabel('Value')\n",
    "   axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "   axes[1, 1].grid(True, alpha=0.3)\n",
    "   \n",
    "   for bar, formatted_val in zip(bars, formatted_values):\n",
    "       height = bar.get_height()\n",
    "       axes[1, 1].text(bar.get_x() + bar.get_width()/2., height + max(metrics_values)*0.01,\n",
    "                      formatted_val, ha='center', va='bottom', fontweight='bold')\n",
    "   \n",
    "   plt.tight_layout()\n",
    "   \n",
    "   if SAVE_PLOTS:\n",
    "       plt.savefig(f\"{RESULTS_SAVE_PATH}backtest_analysis.png\", \n",
    "                  dpi=300, bbox_inches='tight')\n",
    "   \n",
    "   plt.show()\n",
    "\n",
    "def main(csv_path: str):\n",
    "    print(\"=\"*60)\n",
    "    print(\"DDQN TRADING SYSTEM - GOLDMAN ROLL STRATEGY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    create_directories()\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(42)\n",
    "    \n",
    "    try:\n",
    "        data_splits, data_handler = load_and_prepare_data(csv_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        return None, None, None  # Return None values instead of just return\n",
    "    \n",
    "    env_wrapper, train_env, val_env, test_env = create_environments(data_splits)\n",
    "    \n",
    "    try:\n",
    "        agent, training_history = train_agent(train_env, val_env)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during training: {e}\")\n",
    "        return None, None, None  # Return None values instead of just return\n",
    "    \n",
    "    try:\n",
    "        backtest_results = backtest_agent(agent, test_env)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during backtesting: {e}\")\n",
    "        return None, None, None  # Return None values instead of just return\n",
    "    \n",
    "    try:\n",
    "        plot_backtest_results(test_env, backtest_results)\n",
    "    except Exception as e:\n",
    "        print(f\"Error plotting results: {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"EXECUTION COMPLETED SUCCESSFULLY!\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return agent, backtest_results, training_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf80860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found CSV file: /notebooks/CL12 - Sheet1.csv\n",
      "============================================================\n",
      "DDQN TRADING SYSTEM - GOLDMAN ROLL STRATEGY\n",
      "============================================================\n",
      "Directories created successfully\n",
      "Loading and preparing data...\n",
      "Loading data from /notebooks/CL12 - Sheet1.csv...\n",
      "Loaded 4018 rows of data\n",
      "Date range: 2010-01-06 00:00:00 to 2025-05-30 00:00:00\n",
      "CALENDAR spread stats:\n",
      "  Min: -58.06\n",
      "  Max: 4.05\n",
      "  Mean: -0.11837730214036837\n",
      "  Std: 1.275053125556842\n",
      "Preprocessing complete. Final dataset: 4009 rows\n",
      "Using 35 features for training\n",
      "Features: ['PX_OPEN1', 'PX_HIGH1', 'PX_LOW1', 'PX_LAST1', 'PX_VOLUME1', 'OPEN_INT1', 'PX_OPEN2', 'PX_HIGH2', 'PX_LOW2', 'PX_LAST2', 'PX_VOLUME2', 'OPEN_INT2', 'VOL Change1', 'Vol Change %1', 'OI Change1', 'OI Change %1', 'CALENDAR', 'Vol Ratio', 'Vol Ratio Change', 'OI Ratio', 'OI Ratio Change', 'VOL Change2', 'Vol Change %2', 'OI Change2', 'OI Change %2', 'front_hl_ratio', 'second_hl_ratio', 'front_close_position', 'second_close_position', 'volume_momentum', 'oi_momentum', 'spread_momentum_5', 'spread_momentum_10', 'spread_volatility', 'front_vs_second_strength']...\n",
      "Data splits:\n",
      "  Train: 2387 samples\n",
      "  Validation: 398 samples\n",
      "  Test: 1194 samples\n",
      "\n",
      "Data Information:\n",
      "  Features: 41\n",
      "  Samples: 4009\n",
      "  Date range: 2010-01-19 00:00:00 to 2025-05-30 00:00:00\n",
      "  Lookback window: 30 days\n",
      "Creating trading environments...\n",
      "\n",
      "TRAIN Environment:\n",
      "  Samples: 2387\n",
      "  State size: 1058\n",
      "  Action space: 5\n",
      "  Date range: 2010-03-02 00:00:00 to 2019-04-24 00:00:00\n",
      "\n",
      "VAL Environment:\n",
      "  Samples: 398\n",
      "  State size: 1058\n",
      "  Action space: 5\n",
      "  Date range: 2019-04-25 00:00:00 to 2020-11-02 00:00:00\n",
      "\n",
      "TEST Environment:\n",
      "  Samples: 1194\n",
      "  State size: 1058\n",
      "  Action space: 5\n",
      "  Date range: 2020-11-03 00:00:00 to 2025-05-30 00:00:00\n",
      "\n",
      "Initializing DDQN agent...\n",
      "DDQN Agent initialized with 276805 parameters\n",
      "Device: cuda\n",
      "Starting training for 500 episodes...\n",
      "Quick test mode: False\n",
      "Starting training for 500 episodes...\n",
      "Replay buffer size: 4000\n",
      "Batch size: 16\n",
      "Update frequency: 2\n",
      "Episode 0/500\n",
      "  Train Reward: 139.3402\n",
      "  Val Reward: 378.0885\n",
      "  Loss: 2.456500\n",
      "  Epsilon: 0.8587\n",
      "  Val Performance: -69.23%\n",
      "--------------------------------------------------\n",
      "Episode 10/500\n",
      "  Train Reward: 550.2434\n",
      "  Val Reward: 450.8348\n",
      "  Loss: 0.411202\n",
      "  Epsilon: 0.0507\n",
      "  Val Performance: -51.22%\n",
      "--------------------------------------------------\n",
      "Episode 20/500\n",
      "  Train Reward: 654.9430\n",
      "  Val Reward: 449.8705\n",
      "  Loss: 0.099324\n",
      "  Epsilon: 0.0100\n",
      "  Val Performance: -58.82%\n",
      "--------------------------------------------------\n",
      "Episode 30/500\n",
      "  Train Reward: 562.5125\n",
      "  Val Reward: 430.4640\n",
      "  Loss: 0.087284\n",
      "  Epsilon: 0.0100\n",
      "  Val Performance: -28.99%\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# AUTO-EXECUTION FOR JUPYTER \"RUN ALL\"\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # CHANGE THIS PATH TO YOUR ACTUAL CSV FILE\n",
    "    csv_file_path = \"/notebooks/CL12 - Sheet1.csv\"  # UPDATE THIS PATH!\n",
    "    \n",
    "    # Check if file exists\n",
    "    if os.path.exists(csv_file_path):\n",
    "        print(f\"Found CSV file: {csv_file_path}\")\n",
    "        result = main(csv_file_path)\n",
    "        if result[0] is not None:  # Check if training was successful\n",
    "            agent, backtest_results, training_history = result\n",
    "            print(\"Training completed successfully!\")\n",
    "        else:\n",
    "            print(\"Training failed - check error messages above\")\n",
    "    #else:\n",
    "     #   print(f\"ERROR: CSV file not found at: {csv_file_path}\")\n",
    "      #  print(\"Please update the csv_file_path variable with the correct path to your data file\")\n",
    "       # print(\"Example: csv_file_path = '/path/to/your/data.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
