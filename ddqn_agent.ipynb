{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c1f649",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DDQN Agent for Trading Calendar Spreads\n",
    "Main agent class that implements the Double Deep Q-Network algorithm\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4e36da",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import config\n",
    "from networks import DoubleDQN, ReplayBuffer, save_model, load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dee547",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class DDQNTradingAgent:\n",
    "    \"\"\"\n",
    "    Double Deep Q-Network agent for trading calendar spreads\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, market_feature_dim: int, trading_state_dim: int, \n",
    "                 action_space_size: int, lookback_window: int):\n",
    "        \"\"\"\n",
    "        Initialize DDQN agent\n",
    "        \n",
    "        Args:\n",
    "            market_feature_dim: Number of market features\n",
    "            trading_state_dim: Number of trading state features\n",
    "            action_space_size: Number of possible actions\n",
    "            lookback_window: Number of time steps to look back\n",
    "        \"\"\"\n",
    "        self.market_feature_dim = market_feature_dim\n",
    "        self.trading_state_dim = trading_state_dim\n",
    "        self.action_space_size = action_space_size\n",
    "        self.lookback_window = lookback_window\n",
    "        \n",
    "        # Initialize networks\n",
    "        self.ddqn = DoubleDQN(\n",
    "            market_feature_dim, trading_state_dim, action_space_size, lookback_window\n",
    "        ).to(config.DEVICE)\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = optim.Adam(\n",
    "            self.ddqn.online_net.parameters(), \n",
    "            lr=config.LEARNING_RATE\n",
    "        )\n",
    "        \n",
    "        # Loss function\n",
    "        self.criterion = nn.MSELoss()\n",
    "        \n",
    "        # Replay buffer\n",
    "        self.replay_buffer = ReplayBuffer(config.MEMORY_SIZE)\n",
    "        \n",
    "        # Training parameters\n",
    "        self.epsilon = config.EPSILON_START\n",
    "        self.epsilon_min = config.EPSILON_END\n",
    "        self.epsilon_decay = config.EPSILON_DECAY\n",
    "        self.gamma = config.GAMMA\n",
    "        self.tau = config.TAU\n",
    "        self.batch_size = config.BATCH_SIZE\n",
    "        self.update_frequency = config.UPDATE_FREQUENCY\n",
    "        self.target_update_frequency = config.TARGET_UPDATE_FREQUENCY\n",
    "        \n",
    "        # Training tracking\n",
    "        self.training_step = 0\n",
    "        self.episode_rewards = []\n",
    "        self.episode_losses = []\n",
    "        self.episode_epsilons = []\n",
    "        self.training_metrics = {\n",
    "            'rewards': [],\n",
    "            'losses': [],\n",
    "            'epsilons': [],\n",
    "            'q_values': [],\n",
    "            'actions_taken': []\n",
    "        }\n",
    "        \n",
    "        print(f\"DDQN Agent initialized with {self._count_parameters()} parameters\")\n",
    "        print(f\"Device: {config.DEVICE}\")\n",
    "    \n",
    "    def _count_parameters(self) -> int:\n",
    "        \"\"\"Count total trainable parameters\"\"\"\n",
    "        return sum(p.numel() for p in self.ddqn.online_net.parameters() if p.requires_grad)\n",
    "    \n",
    "    def _state_to_tensors(self, state: np.ndarray) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Convert environment state to network input tensors\n",
    "        \n",
    "        Args:\n",
    "            state: Combined state from environment\n",
    "            \n",
    "        Returns:\n",
    "            market_features, trading_state tensors\n",
    "        \"\"\"\n",
    "        # Split state into market features and trading state\n",
    "        market_size = self.market_feature_dim * self.lookback_window\n",
    "        market_features = state[:market_size].reshape(self.market_feature_dim, self.lookback_window)\n",
    "        trading_state = state[market_size:]\n",
    "        \n",
    "        # Convert to tensors and add batch dimension\n",
    "        market_tensor = torch.FloatTensor(market_features).permute(1, 0).unsqueeze(0).to(config.DEVICE)\n",
    "        trading_tensor = torch.FloatTensor(trading_state).unsqueeze(0).to(config.DEVICE)\n",
    "        \n",
    "        return market_tensor, trading_tensor\n",
    "    \n",
    "    def get_action(self, state: np.ndarray, training: bool = True) -> int:\n",
    "        \"\"\"\n",
    "        Select action using epsilon-greedy policy\n",
    "        \n",
    "        Args:\n",
    "            state: Current environment state\n",
    "            training: Whether in training mode (affects epsilon)\n",
    "            \n",
    "        Returns:\n",
    "            Selected action index\n",
    "        \"\"\"\n",
    "        if training and random.random() < self.epsilon:\n",
    "            # Random action (exploration)\n",
    "            action = random.randint(0, self.action_space_size - 1)\n",
    "            self.training_metrics['actions_taken'].append(('random', action))\n",
    "        else:\n",
    "            # Greedy action (exploitation)\n",
    "            market_features, trading_state = self._state_to_tensors(state)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                q_values = self.ddqn(market_features, trading_state, use_target=False)\n",
    "                action = q_values.argmax().item()\n",
    "                \n",
    "                # Store Q-values for analysis\n",
    "                if training:\n",
    "                    self.training_metrics['q_values'].append(q_values.cpu().numpy()[0])\n",
    "                    self.training_metrics['actions_taken'].append(('greedy', action))\n",
    "        \n",
    "\n",
    "        #print(f\"Action taken: {action} ({config.POSITION_ACTIONS[action]:+.0%})\")\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def store_transition(self, state: np.ndarray, action: int, reward: float,\n",
    "                        next_state: np.ndarray, done: bool):\n",
    "        \"\"\"\n",
    "        Store transition in replay buffer\n",
    "        \n",
    "        Args:\n",
    "            state: Current state\n",
    "            action: Action taken\n",
    "            reward: Reward received\n",
    "            next_state: Next state\n",
    "            done: Whether episode ended\n",
    "        \"\"\"\n",
    "        # Convert states to market features and trading states\n",
    "        market_features, trading_state = self._state_to_tensors(state)\n",
    "        next_market_features, next_trading_state = self._state_to_tensors(next_state)\n",
    "        \n",
    "        # Remove batch dimension for storage\n",
    "        self.replay_buffer.push(\n",
    "            market_features.squeeze(0).cpu().numpy(),\n",
    "            trading_state.squeeze(0).cpu().numpy(),\n",
    "            action,\n",
    "            reward,\n",
    "            next_market_features.squeeze(0).cpu().numpy(),\n",
    "            next_trading_state.squeeze(0).cpu().numpy(),\n",
    "            done\n",
    "        )\n",
    "    \n",
    "    def train_step(self) -> float:\n",
    "        \"\"\"\n",
    "        Perform one training step\n",
    "        \n",
    "        Returns:\n",
    "            Loss value\n",
    "        \"\"\"\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return 0.0\n",
    "        \n",
    "        # Sample batch from replay buffer\n",
    "        batch = self.replay_buffer.sample(self.batch_size)\n",
    "        market_features, trading_states, actions, rewards, next_market_features, next_trading_states, dones = batch\n",
    "        \n",
    "        # Move to device\n",
    "        market_features = market_features.to(config.DEVICE)\n",
    "        trading_states = trading_states.to(config.DEVICE)\n",
    "        actions = actions.to(config.DEVICE)\n",
    "        rewards = rewards.to(config.DEVICE)\n",
    "        next_market_features = next_market_features.to(config.DEVICE)\n",
    "        next_trading_states = next_trading_states.to(config.DEVICE)\n",
    "        dones = dones.to(config.DEVICE)\n",
    "        \n",
    "        # Current Q-values\n",
    "        current_q_values = self.ddqn(market_features, trading_states, use_target=False)\n",
    "        current_q_values = current_q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        # Double DQN: use online network to select actions, target network to evaluate\n",
    "        with torch.no_grad():\n",
    "            next_q_values_online = self.ddqn(next_market_features, next_trading_states, use_target=False)\n",
    "            next_actions = next_q_values_online.argmax(1)\n",
    "            \n",
    "            next_q_values_target = self.ddqn(next_market_features, next_trading_states, use_target=True)\n",
    "            next_q_values = next_q_values_target.gather(1, next_actions.unsqueeze(1)).squeeze(1)\n",
    "            \n",
    "            # Calculate target Q-values\n",
    "            target_q_values = rewards + (self.gamma * next_q_values * ~dones)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = self.criterion(current_q_values, target_q_values)\n",
    "        \n",
    "        # Optimize\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(self.ddqn.online_net.parameters(), 1.0)\n",
    "        \n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Update target network\n",
    "        if self.training_step % self.target_update_frequency == 0:\n",
    "            self.ddqn.update_target_network(self.tau)\n",
    "        \n",
    "        # Update epsilon\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "        \n",
    "        self.training_step += 1\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def train_episode(self, env, episode: int) -> Dict:\n",
    "        \"\"\"\n",
    "        Train for one episode\n",
    "        \n",
    "        Args:\n",
    "            env: Trading environment\n",
    "            episode: Episode number\n",
    "            \n",
    "        Returns:\n",
    "            Episode metrics\n",
    "        \"\"\"\n",
    "        state = env.reset()\n",
    "        total_reward = 0.0\n",
    "        total_loss = 0.0\n",
    "        steps = 0\n",
    "        episode_actions = []\n",
    "        \n",
    "        while True:\n",
    "            # Get action\n",
    "            action = self.get_action(state, training=True)\n",
    "            episode_actions.append(action)\n",
    "            \n",
    "            # Take step in environment\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            # Store transition\n",
    "            self.store_transition(state, action, reward, next_state, done)\n",
    "            \n",
    "            # Train if enough samples available\n",
    "            if len(self.replay_buffer) >= self.batch_size and steps % self.update_frequency == 0:\n",
    "                loss = self.train_step()\n",
    "                total_loss += loss\n",
    "            \n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Store episode metrics\n",
    "        episode_metrics = {\n",
    "            'episode': episode,\n",
    "            'total_reward': total_reward,\n",
    "            'average_loss': total_loss / max(1, steps // self.update_frequency),\n",
    "            'steps': steps,\n",
    "            'epsilon': self.epsilon,\n",
    "            'actions': episode_actions,\n",
    "            'final_performance': env.get_performance_metrics()\n",
    "        }\n",
    "        \n",
    "        self.episode_rewards.append(total_reward)\n",
    "        self.episode_losses.append(total_loss / max(1, steps // self.update_frequency))\n",
    "        self.episode_epsilons.append(self.epsilon)\n",
    "        \n",
    "        return episode_metrics\n",
    "    \n",
    "    def evaluate_episode(self, env) -> Dict:\n",
    "        \"\"\"\n",
    "        Evaluate agent performance for one episode (no training)\n",
    "        \n",
    "        Args:\n",
    "            env: Trading environment\n",
    "            \n",
    "        Returns:\n",
    "            Evaluation metrics\n",
    "        \"\"\"\n",
    "        state = env.reset()\n",
    "        total_reward = 0.0\n",
    "        steps = 0\n",
    "        episode_actions = []\n",
    "        \n",
    "        while True:\n",
    "            # Get action (no exploration)\n",
    "            action = self.get_action(state, training=False)\n",
    "            episode_actions.append(action)\n",
    "            \n",
    "            # Take step in environment\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Get comprehensive performance metrics\n",
    "        performance_metrics = env.get_performance_metrics()\n",
    "        \n",
    "        eval_metrics = {\n",
    "            'total_reward': total_reward,\n",
    "            'steps': steps,\n",
    "            'actions': episode_actions,\n",
    "            'performance': performance_metrics\n",
    "        }\n",
    "        \n",
    "        return eval_metrics\n",
    "    \n",
    "    def train(self, train_env, val_env, num_episodes: int = None) -> Dict:\n",
    "        \"\"\"\n",
    "        Main training loop\n",
    "        \n",
    "        Args:\n",
    "            train_env: Training environment\n",
    "            val_env: Validation environment\n",
    "            num_episodes: Number of episodes to train\n",
    "            \n",
    "        Returns:\n",
    "            Training history\n",
    "        \"\"\"\n",
    "        if num_episodes is None:\n",
    "            num_episodes = config.EPISODES\n",
    "        \n",
    "        training_history = {\n",
    "            'train_rewards': [],\n",
    "            'val_rewards': [],\n",
    "            'train_performance': [],\n",
    "            'val_performance': [],\n",
    "            'losses': [],\n",
    "            'epsilons': []\n",
    "        }\n",
    "        \n",
    "        best_val_reward = float('-inf')\n",
    "        patience_counter = 0\n",
    "        \n",
    "        print(f\"Starting training for {num_episodes} episodes...\")\n",
    "        print(f\"Replay buffer size: {config.MEMORY_SIZE}\")\n",
    "        print(f\"Batch size: {self.batch_size}\")\n",
    "        print(f\"Update frequency: {self.update_frequency}\")\n",
    "        \n",
    "        for episode in range(num_episodes):\n",
    "            # Training episode\n",
    "            train_metrics = self.train_episode(train_env, episode)\n",
    "            \n",
    "            # Validation episode (every N episodes)\n",
    "            if episode % config.LOG_FREQUENCY == 0:\n",
    "                val_metrics = self.evaluate_episode(val_env)\n",
    "                \n",
    "                # Store metrics\n",
    "                training_history['train_rewards'].append(train_metrics['total_reward'])\n",
    "                training_history['val_rewards'].append(val_metrics['total_reward'])\n",
    "                training_history['train_performance'].append(train_metrics['final_performance'])\n",
    "                training_history['val_performance'].append(val_metrics['performance'])\n",
    "                training_history['losses'].append(train_metrics['average_loss'])\n",
    "                training_history['epsilons'].append(train_metrics['epsilon'])\n",
    "                \n",
    "                # Print progress\n",
    "                print(f\"Episode {episode}/{num_episodes}\")\n",
    "                print(f\"  Train Reward: {train_metrics['total_reward']:.4f}\")\n",
    "                print(f\"  Val Reward: {val_metrics['total_reward']:.4f}\")\n",
    "                print(f\"  Loss: {train_metrics['average_loss']:.6f}\")\n",
    "                print(f\"  Epsilon: {train_metrics['epsilon']:.4f}\")\n",
    "                print(f\"  Val Performance: {val_metrics['performance'].get('total_return', 0):.2%}\")\n",
    "                print(\"-\" * 50)\n",
    "                \n",
    "                # Early stopping check\n",
    "                if val_metrics['total_reward'] > best_val_reward + config.MIN_IMPROVEMENT:\n",
    "                    best_val_reward = val_metrics['total_reward']\n",
    "                    patience_counter = 0\n",
    "                    \n",
    "                    # Save best model\n",
    "                    save_model(\n",
    "                        self.ddqn, \n",
    "                        f\"{config.MODEL_SAVE_PATH}best_model.pth\",\n",
    "                        self.optimizer.state_dict(),\n",
    "                        {'episode': episode, 'val_reward': best_val_reward}\n",
    "                    )\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                \n",
    "                if patience_counter >= config.PATIENCE:\n",
    "                    print(f\"Early stopping at episode {episode}\")\n",
    "                    break\n",
    "            \n",
    "            # Save checkpoint periodically\n",
    "            if episode % config.SAVE_FREQUENCY == 0 and episode > 0:\n",
    "                save_model(\n",
    "                    self.ddqn,\n",
    "                    f\"{config.MODEL_SAVE_PATH}checkpoint_episode_{episode}.pth\",\n",
    "                    self.optimizer.state_dict(),\n",
    "                    {'episode': episode, 'training_step': self.training_step}\n",
    "                )\n",
    "        \n",
    "        print(\"Training completed!\")\n",
    "        return training_history\n",
    "    \n",
    "    def backtest(self, test_env) -> Dict:\n",
    "        \"\"\"\n",
    "        Perform backtesting on test environment\n",
    "        \n",
    "        Args:\n",
    "            test_env: Test environment\n",
    "            \n",
    "        Returns:\n",
    "            Backtesting results\n",
    "        \"\"\"\n",
    "        print(\"Starting backtesting...\")\n",
    "        \n",
    "        # Set to evaluation mode\n",
    "        self.ddqn.online_net.eval()\n",
    "        \n",
    "        # Run evaluation\n",
    "        results = self.evaluate_episode(test_env)\n",
    "        \n",
    "        # Add detailed analysis\n",
    "        performance = results['performance']\n",
    "        actions = results['actions']\n",
    "        \n",
    "        # Action analysis\n",
    "        action_counts = {}\n",
    "        for action in actions:\n",
    "            action_counts[action] = action_counts.get(action, 0) + 1\n",
    "        \n",
    "        backtest_results = {\n",
    "            'performance_metrics': performance,\n",
    "            'total_reward': results['total_reward'],\n",
    "            'total_steps': results['steps'],\n",
    "            'action_distribution': action_counts,\n",
    "            'action_sequence': actions\n",
    "        }\n",
    "        \n",
    "        # Print results\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"BACKTESTING RESULTS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        print(f\"Total Return: {performance.get('total_return', 0):.2%}\")\n",
    "        print(f\"Sharpe Ratio: {performance.get('sharpe_ratio', 0):.2f}\")\n",
    "        print(f\"Max Drawdown: {performance.get('max_drawdown', 0):.2%}\")\n",
    "        print(f\"Win Rate: {performance.get('win_rate', 0):.1%}\")\n",
    "        print(f\"Profit Factor: {performance.get('profit_factor', 0):.2f}\")\n",
    "        print(f\"Total Trades: {performance.get('total_trades', 0)}\")\n",
    "        print(f\"Final Equity: ${performance.get('final_equity', 0):,.2f}\")\n",
    "        \n",
    "        print(f\"\\nAction Distribution:\")\n",
    "        for action, count in action_counts.items():\n",
    "            action_pct = config.POSITION_ACTIONS[action]\n",
    "            print(f\"  Action {action} ({action_pct:+.0%}): {count} times ({count/len(actions):.1%})\")\n",
    "        \n",
    "        return backtest_results\n",
    "    \n",
    "    def plot_training_curves(self, training_history: Dict, save_path: str = None):\n",
    "        \"\"\"Plot training curves\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # Rewards\n",
    "        axes[0, 0].plot(training_history['train_rewards'], label='Train', alpha=0.7)\n",
    "        axes[0, 0].plot(training_history['val_rewards'], label='Validation', alpha=0.7)\n",
    "        axes[0, 0].set_title('Episode Rewards')\n",
    "        axes[0, 0].set_xlabel('Episode')\n",
    "        axes[0, 0].set_ylabel('Total Reward')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True)\n",
    "        \n",
    "        # Losses\n",
    "        axes[0, 1].plot(training_history['losses'], color='red', alpha=0.7)\n",
    "        axes[0, 1].set_title('Training Loss')\n",
    "        axes[0, 1].set_xlabel('Episode')\n",
    "        axes[0, 1].set_ylabel('Average Loss')\n",
    "        axes[0, 1].grid(True)\n",
    "        \n",
    "        # Epsilon decay\n",
    "        axes[1, 0].plot(training_history['epsilons'], color='green', alpha=0.7)\n",
    "        axes[1, 0].set_title('Epsilon Decay')\n",
    "        axes[1, 0].set_xlabel('Episode')\n",
    "        axes[1, 0].set_ylabel('Epsilon')\n",
    "        axes[1, 0].grid(True)\n",
    "        \n",
    "        # Performance metrics\n",
    "        if training_history['val_performance']:\n",
    "            returns = [p.get('total_return', 0) for p in training_history['val_performance']]\n",
    "            sharpes = [p.get('sharpe_ratio', 0) for p in training_history['val_performance']]\n",
    "            \n",
    "            ax2 = axes[1, 1]\n",
    "            ax3 = ax2.twinx()\n",
    "            \n",
    "            line1 = ax2.plot(returns, 'b-', label='Total Return', alpha=0.7)\n",
    "            line2 = ax3.plot(sharpes, 'r-', label='Sharpe Ratio', alpha=0.7)\n",
    "            \n",
    "            ax2.set_xlabel('Episode')\n",
    "            ax2.set_ylabel('Total Return', color='b')\n",
    "            ax3.set_ylabel('Sharpe Ratio', color='r')\n",
    "            ax2.set_title('Validation Performance')\n",
    "            \n",
    "            lines = line1 + line2\n",
    "            labels = [l.get_label() for l in lines]\n",
    "            ax2.legend(lines, labels, loc='upper left')\n",
    "            ax2.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        \n",
    "        if config.PLOT_TRAINING_CURVES:\n",
    "            plt.show()\n",
    "    \n",
    "    def save_agent(self, filepath: str, metadata: Dict = None):\n",
    "        \"\"\"Save complete agent state\"\"\"\n",
    "        if metadata is None:\n",
    "            metadata = {}\n",
    "        \n",
    "        metadata.update({\n",
    "            'market_feature_dim': self.market_feature_dim,\n",
    "            'trading_state_dim': self.trading_state_dim,\n",
    "            'action_space_size': self.action_space_size,\n",
    "            'lookback_window': self.lookback_window,\n",
    "            'training_step': self.training_step,\n",
    "            'epsilon': self.epsilon\n",
    "        })\n",
    "        \n",
    "        save_model(self.ddqn, filepath, self.optimizer.state_dict(), metadata)\n",
    "    \n",
    "    def load_agent(self, filepath: str):\n",
    "        \"\"\"Load complete agent state\"\"\"\n",
    "        optimizer_state, metadata = load_model(self.ddqn, filepath, load_optimizer=True)\n",
    "        \n",
    "        if optimizer_state:\n",
    "            self.optimizer.load_state_dict(optimizer_state)\n",
    "        \n",
    "        if metadata:\n",
    "            self.training_step = metadata.get('training_step', 0)\n",
    "            self.epsilon = metadata.get('epsilon', self.epsilon)\n",
    "        \n",
    "        print(f\"Agent loaded from {filepath}\")\n",
    "        print(f\"Training step: {self.training_step}\")\n",
    "        print(f\"Current epsilon: {self.epsilon}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ea682a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Utility functions for agent management\n",
    "def create_agent_from_env(env) -> DDQNTradingAgent:\n",
    "    \"\"\"Create agent with correct dimensions from environment\"\"\"\n",
    "    state = env.reset()\n",
    "    state_size = len(state)\n",
    "    \n",
    "    # Calculate dimensions\n",
    "    market_size = env.data_sequences.shape[1] * env.data_sequences.shape[2]\n",
    "    trading_state_size = state_size - market_size\n",
    "    \n",
    "    agent = DDQNTradingAgent(\n",
    "        market_feature_dim=env.data_sequences.shape[1],\n",
    "        trading_state_dim=trading_state_size,\n",
    "        action_space_size=env.get_action_space_size(),\n",
    "        lookback_window=env.data_sequences.shape[2]\n",
    "    )\n",
    "    \n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22240fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing function\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Testing DDQN Agent...\")\n",
    "    \n",
    "    # This would be used with real environment\n",
    "    # env = TradingEnvironment(...)\n",
    "    # agent = create_agent_from_env(env)\n",
    "    # training_history = agent.train(train_env, val_env, num_episodes=10)\n",
    "    # backtest_results = agent.backtest(test_env)\n",
    "    \n",
    "    print(\"DDQN Agent ready for use!\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
