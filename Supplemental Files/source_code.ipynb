{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7469856",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ba75082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION PARAMETERS\n",
    "# =============================================================================\n",
    "\n",
    "# Market parameters\n",
    "TICK_VALUE = 10.0\n",
    "MAX_CONTRACTS = 8\n",
    "TRANSACTION_COST_PER_CONTRACT = 0.75\n",
    "BID_ASK_SLIPPAGE = 0.5 # in ticks\n",
    "SLIPPAGE_ENABLED = True\n",
    "\n",
    "# Risk management\n",
    "MAX_DAILY_LOSS = 500.0\n",
    "STOP_LOSS_ENABLED = True\n",
    "STOP_LOSS_PERCENTAGE = 0.3\n",
    "\n",
    "# Data parameters\n",
    "LOOKBACK_WINDOW = 20\n",
    "MIN_LOOKBACK_FOR_TRAINING = 20\n",
    "TRAIN_RATIO = 0.7\n",
    "VAL_RATIO = 0.10\n",
    "TEST_RATIO = 0.20\n",
    "\n",
    "FEATURE_COLUMNS = [\n",
    "   'PX_OPEN1', 'PX_HIGH1', 'PX_LOW1', 'PX_LAST1', 'PX_VOLUME1', 'OPEN_INT1',\n",
    "   'PX_OPEN2', 'PX_HIGH2', 'PX_LOW2', 'PX_LAST2', 'PX_VOLUME2', 'OPEN_INT2',\n",
    "   'VOL Change1', 'Vol Change %1', 'OI Change1', 'OI Change %1',\n",
    "   'CALENDAR', 'Vol Ratio', 'Vol Ratio Change', 'OI Ratio', 'OI Ratio Change',\n",
    "   'VOL Change2', 'Vol Change %2', 'OI Change2', 'OI Change %2'\n",
    "]\n",
    "\n",
    "TARGET_COLUMN = 'CALENDAR'\n",
    "DATE_COLUMN = 'Dates'\n",
    "\n",
    "# Goldman roll parameters\n",
    "GOLDMAN_ROLL_START_DAY = 5\n",
    "GOLDMAN_ROLL_END_DAY = 9\n",
    "GOLDMAN_ROLL_WINDOW = 15\n",
    "\n",
    "# LSTM parameters\n",
    "LSTM_HIDDEN_SIZE = 32\n",
    "LSTM_NUM_LAYERS = 2\n",
    "LSTM_DROPOUT = 0.05\n",
    "LSTM_BIDIRECTIONAL = False\n",
    "LSTM_PROCESSING_DIM = 32\n",
    "HIDDEN_DIMS = [32, 16]\n",
    "\n",
    "# DDQN parameters\n",
    "LEARNING_RATE = 0.0001\n",
    "GAMMA = 0.99 #How much future rewards matter\n",
    "TAU = 0.005  #How fast the \"teacher network\" updates\n",
    "BATCH_SIZE = 16 #How many past trades to learn from at once\n",
    "\n",
    "\n",
    "# Action space\n",
    "POSITION_ACTIONS = [-0.5, -0.25, 0.0, 0.25, 0.5]\n",
    "ACTION_SPACE_SIZE = len(POSITION_ACTIONS)\n",
    "\n",
    "# Reward function\n",
    "REWARD_PNL_WEIGHT = 0.7\n",
    "REWARD_WINRATE_WEIGHT = 0.2\n",
    "REWARD_RISK_WEIGHT = 0.1\n",
    "DRAWDOWN_PENALTY_THRESHOLD = 0.1\n",
    "POSITION_SIZE_PENALTY = 0.01\n",
    "\n",
    "# Training parameters\n",
    "EPISODES = 2000\n",
    "MEMORY_SIZE = 500 #How many past trading experiences the agent remembers\n",
    "UPDATE_FREQUENCY = 2 #How often the neural network actually trains\n",
    "TARGET_UPDATE_FREQUENCY = 200 #How often the \"teacher network\" gets updated\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.2\n",
    "EPSILON_DECAY = 0.995 #Reduce randomness by x% each episode\n",
    "PATIENCE = 50 #Early stopping threshold\n",
    "MIN_IMPROVEMENT = 0.001\n",
    "\n",
    "# Device\n",
    "DEVICE = torch.device(\"cuda\")# if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Logging\n",
    "LOG_FREQUENCY = 10\n",
    "SAVE_FREQUENCY = 250\n",
    "MODEL_SAVE_PATH = \"models/\"\n",
    "LOG_SAVE_PATH = \"logs/\"\n",
    "RESULTS_SAVE_PATH = \"results/\"\n",
    "\n",
    "# Backtesting\n",
    "INITIAL_CAPITAL = 10000.0\n",
    "TRACK_METRICS = [\n",
    "   'total_return', 'sharpe_ratio', 'max_drawdown', 'win_rate',\n",
    "   'profit_factor', 'total_trades', 'avg_trade_duration'\n",
    "]\n",
    "\n",
    "# Quick test\n",
    "QUICK_TEST_MODE = False\n",
    "QUICK_TEST_YEARS = 3\n",
    "QUICK_TEST_EPISODES = 100\n",
    "\n",
    "# Plotting\n",
    "PLOT_TRAINING_CURVES = True\n",
    "PLOT_TRADING_RESULTS = True\n",
    "SAVE_PLOTS = True\n",
    "\n",
    "print(f\"Configuration loaded successfully!\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Lookback window: {LOOKBACK_WINDOW} days\")\n",
    "print(f\"Max contracts: {MAX_CONTRACTS}\")\n",
    "print(f\"Tick value: ${TICK_VALUE}\")\n",
    "print(f\"Quick test mode: {QUICK_TEST_MODE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49ee67c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATA HANDLER\n",
    "# =============================================================================\n",
    "\n",
    "class TradingDataHandler:\n",
    "   def __init__(self, csv_path: str):\n",
    "       self.csv_path = csv_path\n",
    "       self.raw_data = None\n",
    "       self.processed_data = None\n",
    "       self.scaler = StandardScaler()\n",
    "       self.feature_columns = FEATURE_COLUMNS\n",
    "       self.date_column = DATE_COLUMN\n",
    "       self.target_column = TARGET_COLUMN\n",
    "       \n",
    "   def load_data(self) -> pd.DataFrame:\n",
    "       print(f\"Loading data from {self.csv_path}...\")\n",
    "       \n",
    "       column_names = [\n",
    "           'PX_OPEN1', 'PX_HIGH1', 'PX_LOW1', 'PX_LAST1', 'PX_VOLUME1', 'OPEN_INT1',\n",
    "           'PX_OPEN2', 'PX_HIGH2', 'PX_LOW2', 'PX_LAST2', 'PX_VOLUME2', 'OPEN_INT2',\n",
    "           'Dates', 'VOL Change1', 'Vol Change %1', 'OI Change1', 'OI Change %1',\n",
    "           'CALENDAR', 'Vol Ratio', 'Vol Ratio Change', 'OI Ratio', 'OI Ratio Change',\n",
    "           'VOL Change2', 'Vol Change %2', 'OI Change2', 'OI Change %2'\n",
    "           ]\n",
    "       \n",
    "       self.raw_data = pd.read_csv(self.csv_path, names=column_names, header=None, skiprows=3)\n",
    "       \n",
    "       self.raw_data['Dates'] = pd.to_datetime(self.raw_data['Dates'], dayfirst=True)\n",
    "       self.raw_data = self.raw_data.sort_values('Dates').reset_index(drop=True)\n",
    "       self.raw_data['date'] = self.raw_data['Dates']\n",
    "\n",
    "        # to check if the data is loaded correctly\n",
    "       print(f\"Loaded {len(self.raw_data)} rows of data\")\n",
    "       print(f\"Date range: {self.raw_data['Dates'].min()} to {self.raw_data['Dates'].max()}\")\n",
    "       print(f\"CALENDAR spread stats:\")\n",
    "       print(f\"  Min: {self.raw_data['CALENDAR'].min()}\")\n",
    "       print(f\"  Max: {self.raw_data['CALENDAR'].max()}\")\n",
    "       print(f\"  Mean: {self.raw_data['CALENDAR'].mean()}\")\n",
    "       print(f\"  Std: {self.raw_data['CALENDAR'].std()}\")\n",
    "       \n",
    "       return self.raw_data\n",
    "   \n",
    "   def calculate_business_day_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "       df = df.copy()\n",
    "       \n",
    "       df['business_day_of_month'] = df['Dates'].apply(self._get_business_day_of_month)\n",
    "       \n",
    "       df['days_to_roll_start'] = df['business_day_of_month'].apply(\n",
    "           lambda x: max(0, GOLDMAN_ROLL_START_DAY - x) if x < GOLDMAN_ROLL_START_DAY \n",
    "           else 0\n",
    "       )\n",
    "       \n",
    "       df['days_since_roll_end'] = df['business_day_of_month'].apply(\n",
    "           lambda x: max(0, x - GOLDMAN_ROLL_END_DAY) if x > GOLDMAN_ROLL_END_DAY \n",
    "           else 0\n",
    "       )\n",
    "       \n",
    "       df['in_goldman_roll'] = (\n",
    "           (df['business_day_of_month'] >= GOLDMAN_ROLL_START_DAY) & \n",
    "           (df['business_day_of_month'] <= GOLDMAN_ROLL_END_DAY)\n",
    "       ).astype(int)\n",
    "       \n",
    "       df['in_extended_roll_window'] = (\n",
    "           (df['business_day_of_month'] >= GOLDMAN_ROLL_START_DAY - 5) & \n",
    "           (df['business_day_of_month'] <= GOLDMAN_ROLL_END_DAY + 5)\n",
    "       ).astype(int)\n",
    "       \n",
    "       return df\n",
    "   \n",
    "   def _get_business_day_of_month(self, date: datetime) -> int:\n",
    "       first_day = date.replace(day=1)\n",
    "       business_days = 0\n",
    "       current_date = first_day\n",
    "       \n",
    "       while current_date <= date:\n",
    "           if current_date.weekday() < 5:\n",
    "               business_days += 1\n",
    "           current_date += timedelta(days=1)\n",
    "           \n",
    "       return business_days\n",
    "   \n",
    "   def preprocess_data(self) -> pd.DataFrame:\n",
    "       if self.raw_data is None:\n",
    "           self.load_data()\n",
    "           \n",
    "       df = self.raw_data.copy()\n",
    "       \n",
    "       df = df.fillna(method='ffill').fillna(method='bfill')\n",
    "       df = self.calculate_business_day_features(df)\n",
    "       df = df.dropna()\n",
    "       \n",
    "       self.processed_data = df\n",
    "       print(f\"Preprocess complete final dataset: {len(df)} rows\")\n",
    "       \n",
    "       return df\n",
    "   \n",
    "   def create_sequences(self, df: pd.DataFrame, lookback: int = None) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "       if lookback is None:\n",
    "           lookback = LOOKBACK_WINDOW\n",
    "       \n",
    "       feature_cols = [col for col in df.columns if col not in ['date', 'Dates']]\n",
    "       feature_cols = [col for col in feature_cols if not col.startswith('business_day')]\n",
    "       feature_cols = [col for col in feature_cols if not col.startswith('days_')]\n",
    "       feature_cols = [col for col in feature_cols if not col.startswith('in_goldman')]\n",
    "       feature_cols = [col for col in feature_cols if not col.startswith('in_extended')]\n",
    "   \n",
    "    \n",
    "   \n",
    "       feature_data = self.scaler.fit_transform(df[feature_cols])\n",
    "   \n",
    "       sequences = []\n",
    "       targets = []\n",
    "       dates = []\n",
    "   \n",
    "       for i in range(lookback, len(df)):\n",
    "           sequence = feature_data[i-lookback:i].T\n",
    "           sequences.append(sequence)\n",
    "           targets.append(df.iloc[i][self.target_column])\n",
    "           dates.append(df.iloc[i]['Dates'])\n",
    "   \n",
    "       return np.array(sequences), np.array(targets), np.array(dates)\n",
    "   \n",
    "   def split_data(self, sequences: np.ndarray, targets: np.ndarray, dates: np.ndarray, \n",
    "                  quick_test: bool = False) -> Dict:\n",
    "       \n",
    "       if quick_test:\n",
    "           n_years = QUICK_TEST_YEARS\n",
    "           cutoff_date = dates[-1] - pd.DateOffset(years=n_years)\n",
    "           mask = pd.to_datetime(dates) >= cutoff_date\n",
    "           \n",
    "           sequences = sequences[mask]\n",
    "           targets = targets[mask]\n",
    "           dates = dates[mask]\n",
    "           \n",
    "       n_samples = len(sequences)\n",
    "       \n",
    "       train_end = int(n_samples * TRAIN_RATIO)\n",
    "       val_end = int(n_samples * (TRAIN_RATIO + VAL_RATIO))\n",
    "       \n",
    "       data_splits = {\n",
    "           'train': {\n",
    "               'sequences': sequences[:train_end],\n",
    "               'targets': targets[:train_end],\n",
    "               'dates': dates[:train_end]\n",
    "           },\n",
    "           'val': {\n",
    "               'sequences': sequences[train_end:val_end],\n",
    "               'targets': targets[train_end:val_end],\n",
    "               'dates': dates[train_end:val_end]\n",
    "           },\n",
    "           'test': {\n",
    "               'sequences': sequences[val_end:],\n",
    "               'targets': targets[val_end:],\n",
    "               'dates': dates[val_end:]\n",
    "           }\n",
    "       }\n",
    "       \n",
    "       # to check if the data splits are correct\n",
    "       print(f\"Data splits:\")\n",
    "       print(f\"  Train: {len(data_splits['train']['sequences'])} samples\")\n",
    "       print(f\"  Validation: {len(data_splits['val']['sequences'])} samples\") \n",
    "       print(f\"  Test: {len(data_splits['test']['sequences'])} samples\")\n",
    "       \n",
    "       return data_splits\n",
    "   \n",
    "   def get_feature_info(self) -> Dict:\n",
    "       if self.processed_data is None:\n",
    "           self.preprocess_data()\n",
    "           \n",
    "       feature_cols = [col for col in self.processed_data.columns if col not in ['date']]\n",
    "       \n",
    "       return {\n",
    "           'n_features': len(feature_cols),\n",
    "           'feature_names': feature_cols,\n",
    "           'n_samples': len(self.processed_data),\n",
    "           'date_range': (self.processed_data['date'].min(), self.processed_data['date'].max())\n",
    "       }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf54f118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# NEURAL NETWORKS\n",
    "# =============================================================================\n",
    "\n",
    "class LSTMFeatureExtractor(nn.Module):\n",
    "   def __init__(self, n_features: int, sequence_length: int):\n",
    "       super(LSTMFeatureExtractor, self).__init__()\n",
    "       \n",
    "       self.n_features = n_features\n",
    "       self.sequence_length = sequence_length\n",
    "       \n",
    "       self.hidden_size = LSTM_HIDDEN_SIZE\n",
    "       self.num_layers = LSTM_NUM_LAYERS\n",
    "       self.dropout = LSTM_DROPOUT\n",
    "       \n",
    "       self.lstm = nn.LSTM(\n",
    "           input_size=n_features,\n",
    "           hidden_size=self.hidden_size,\n",
    "           num_layers=self.num_layers,\n",
    "           dropout=self.dropout if self.num_layers > 1 else 0,\n",
    "           batch_first=True,\n",
    "           bidirectional=LSTM_BIDIRECTIONAL\n",
    "       )\n",
    "       \n",
    "       self.lstm_output_size = self.hidden_size * (2 if LSTM_BIDIRECTIONAL else 1)\n",
    "       \n",
    "       self.feature_processor = nn.Sequential(\n",
    "           nn.Linear(self.lstm_output_size, LSTM_PROCESSING_DIM),\n",
    "           nn.ReLU(),\n",
    "           nn.Dropout(LSTM_DROPOUT),\n",
    "           nn.Linear(LSTM_PROCESSING_DIM, LSTM_PROCESSING_DIM // 2),\n",
    "           nn.ReLU(),\n",
    "           nn.Dropout(LSTM_DROPOUT)\n",
    "       )\n",
    "       \n",
    "       self.output_dim = LSTM_PROCESSING_DIM // 2\n",
    "       \n",
    "   def forward(self, x):\n",
    "       batch_size = x.size(0)\n",
    "       h0 = torch.zeros(\n",
    "           self.num_layers * (2 if LSTM_BIDIRECTIONAL else 1),\n",
    "           batch_size,\n",
    "           self.hidden_size,\n",
    "           device=x.device\n",
    "       )\n",
    "       c0 = torch.zeros(\n",
    "           self.num_layers * (2 if LSTM_BIDIRECTIONAL else 1),\n",
    "           batch_size,\n",
    "           self.hidden_size,\n",
    "           device=x.device\n",
    "       )\n",
    "       \n",
    "       lstm_out, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "       final_output = lstm_out[:, -1, :]\n",
    "       features = self.feature_processor(final_output)\n",
    "       \n",
    "       return features\n",
    "\n",
    "class DQNNetwork(nn.Module):\n",
    "   def __init__(self, market_feature_dim: int, trading_state_dim: int, \n",
    "                action_space_size: int, sequence_length: int):\n",
    "       super(DQNNetwork, self).__init__()\n",
    "       \n",
    "       self.market_feature_dim = market_feature_dim\n",
    "       self.trading_state_dim = trading_state_dim\n",
    "       self.action_space_size = action_space_size\n",
    "       \n",
    "       self.lstm = LSTMFeatureExtractor(market_feature_dim, sequence_length)\n",
    "       \n",
    "       total_input_dim = self.lstm.output_dim + trading_state_dim\n",
    "       \n",
    "       self.fc_layers = nn.ModuleList()\n",
    "       \n",
    "       self.fc_layers.append(nn.Linear(total_input_dim, HIDDEN_DIMS[0]))\n",
    "       \n",
    "       for i in range(1, len(HIDDEN_DIMS)):\n",
    "           self.fc_layers.append(\n",
    "               nn.Linear(HIDDEN_DIMS[i-1], HIDDEN_DIMS[i])\n",
    "           )\n",
    "       \n",
    "       self.q_values = nn.Linear(HIDDEN_DIMS[-1], action_space_size)\n",
    "       \n",
    "       self.dropout = nn.Dropout(LSTM_DROPOUT)\n",
    "       \n",
    "       self._initialize_weights()\n",
    "   \n",
    "   def _initialize_weights(self):\n",
    "       for module in self.modules():\n",
    "           if isinstance(module, nn.Linear):\n",
    "               nn.init.xavier_uniform_(module.weight)\n",
    "               if module.bias is not None:\n",
    "                   nn.init.constant_(module.bias, 0)\n",
    "           elif isinstance(module, nn.LSTM):\n",
    "               for name, param in module.named_parameters():\n",
    "                   if 'weight_ih' in name:\n",
    "                       nn.init.xavier_uniform_(param.data)\n",
    "                   elif 'weight_hh' in name:\n",
    "                       nn.init.orthogonal_(param.data)\n",
    "                   elif 'bias' in name:\n",
    "                       param.data.fill_(0)\n",
    "   \n",
    "   def forward(self, market_features, trading_state):\n",
    "       lstm_features = self.lstm(market_features)\n",
    "       \n",
    "       combined_features = torch.cat([lstm_features, trading_state], dim=1)\n",
    "       \n",
    "       x = combined_features\n",
    "       for fc_layer in self.fc_layers:\n",
    "           x = fc_layer(x)\n",
    "           x = F.relu(x)\n",
    "           x = self.dropout(x)\n",
    "       \n",
    "       q_values = self.q_values(x)\n",
    "       \n",
    "       return q_values\n",
    "\n",
    "class DoubleDQN(nn.Module):\n",
    "   def __init__(self, market_feature_dim: int, trading_state_dim: int,\n",
    "                action_space_size: int, sequence_length: int):\n",
    "       super(DoubleDQN, self).__init__()\n",
    "       \n",
    "       self.online_net = DQNNetwork(\n",
    "           market_feature_dim, trading_state_dim, action_space_size, sequence_length\n",
    "       )\n",
    "       \n",
    "       self.target_net = DQNNetwork(\n",
    "           market_feature_dim, trading_state_dim, action_space_size, sequence_length\n",
    "       )\n",
    "       \n",
    "       self.update_target_network()\n",
    "       \n",
    "       for param in self.target_net.parameters():\n",
    "           param.requires_grad = False\n",
    "   \n",
    "   def forward(self, market_features, trading_state, use_target=False):\n",
    "       if use_target:\n",
    "           return self.target_net(market_features, trading_state)\n",
    "       else:\n",
    "           return self.online_net(market_features, trading_state)\n",
    "   \n",
    "   def update_target_network(self, tau: float = None):\n",
    "       if tau is None:\n",
    "           self.target_net.load_state_dict(self.online_net.state_dict())\n",
    "       else:\n",
    "           for target_param, online_param in zip(\n",
    "               self.target_net.parameters(), self.online_net.parameters()\n",
    "           ):\n",
    "               target_param.data.copy_(\n",
    "                   tau * online_param.data + (1.0 - tau) * target_param.data\n",
    "               )\n",
    "   \n",
    "   def get_action(self, market_features, trading_state, epsilon=0.0):\n",
    "       if np.random.random() < epsilon:\n",
    "           return np.random.randint(0, self.online_net.action_space_size)\n",
    "       else:\n",
    "           with torch.no_grad():\n",
    "               q_values = self.online_net(market_features, trading_state)\n",
    "               return q_values.argmax().item()\n",
    "\n",
    "class ReplayBuffer:\n",
    "   def __init__(self, capacity: int):\n",
    "       self.capacity = capacity\n",
    "       self.buffer = []\n",
    "       self.position = 0\n",
    "   \n",
    "   def push(self, market_features, trading_state, action, reward, \n",
    "            next_market_features, next_trading_state, done):\n",
    "       if len(self.buffer) < self.capacity:\n",
    "           self.buffer.append(None)\n",
    "       \n",
    "       self.buffer[self.position] = (\n",
    "           market_features, trading_state, action, reward,\n",
    "           next_market_features, next_trading_state, done\n",
    "       )\n",
    "       self.position = (self.position + 1) % self.capacity\n",
    "   \n",
    "   def sample(self, batch_size: int):\n",
    "       batch = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "       \n",
    "       market_features = []\n",
    "       trading_states = []\n",
    "       actions = []\n",
    "       rewards = []\n",
    "       next_market_features = []\n",
    "       next_trading_states = []\n",
    "       dones = []\n",
    "       \n",
    "       for idx in batch:\n",
    "           mf, ts, a, r, nmf, nts, d = self.buffer[idx]\n",
    "           market_features.append(mf)\n",
    "           trading_states.append(ts)\n",
    "           actions.append(a)\n",
    "           rewards.append(r)\n",
    "           next_market_features.append(nmf)\n",
    "           next_trading_states.append(nts)\n",
    "           dones.append(d)\n",
    "       \n",
    "       return (\n",
    "           torch.FloatTensor(market_features),\n",
    "           torch.FloatTensor(trading_states),\n",
    "           torch.LongTensor(actions),\n",
    "           torch.FloatTensor(rewards),\n",
    "           torch.FloatTensor(next_market_features),\n",
    "           torch.FloatTensor(next_trading_states),\n",
    "           torch.BoolTensor(dones)\n",
    "       )\n",
    "   \n",
    "   def __len__(self):\n",
    "       return len(self.buffer)\n",
    "\n",
    "def save_model(model: DoubleDQN, filepath: str, optimizer_state: dict = None, \n",
    "              metadata: dict = None):\n",
    "   checkpoint = {\n",
    "       'online_net_state_dict': model.online_net.state_dict(),\n",
    "       'target_net_state_dict': model.target_net.state_dict(),\n",
    "   }\n",
    "   \n",
    "   if optimizer_state:\n",
    "       checkpoint['optimizer_state_dict'] = optimizer_state\n",
    "   \n",
    "   if metadata:\n",
    "       checkpoint['metadata'] = metadata\n",
    "   \n",
    "   torch.save(checkpoint, filepath)\n",
    "   print(f\"Model saved to {filepath}\")\n",
    "\n",
    "def load_model(model: DoubleDQN, filepath: str, load_optimizer: bool = False):\n",
    "   checkpoint = torch.load(filepath, map_location=DEVICE)\n",
    "   \n",
    "   model.online_net.load_state_dict(checkpoint['online_net_state_dict'])\n",
    "   model.target_net.load_state_dict(checkpoint['target_net_state_dict'])\n",
    "   \n",
    "   optimizer_state = None\n",
    "   if load_optimizer and 'optimizer_state_dict' in checkpoint:\n",
    "       optimizer_state = checkpoint['optimizer_state_dict']\n",
    "   \n",
    "   metadata = checkpoint.get('metadata', {})\n",
    "   \n",
    "   print(f\"Model loaded from {filepath}\")\n",
    "   return optimizer_state, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "025e1363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TRADING ENVIRONMENT\n",
    "# =============================================================================\n",
    "\n",
    "class TradingEnvironment:\n",
    "    def __init__(self, data_sequences: np.ndarray, data_targets: np.ndarray, \n",
    "                 data_dates: np.ndarray, initial_capital: float = None):\n",
    "        self.data_sequences = data_sequences\n",
    "        self.data_targets = data_targets\n",
    "        self.data_dates = data_dates\n",
    "        self.n_samples = len(data_sequences)\n",
    "        \n",
    "        self.initial_capital = initial_capital or INITIAL_CAPITAL\n",
    "        self.tick_value = TICK_VALUE\n",
    "        self.max_contracts = MAX_CONTRACTS\n",
    "        self.transaction_cost = TRANSACTION_COST_PER_CONTRACT\n",
    "        self.slippage = BID_ASK_SLIPPAGE if SLIPPAGE_ENABLED else 0.0\n",
    "        \n",
    "        self.max_daily_loss = MAX_DAILY_LOSS\n",
    "        self.stop_loss_enabled = STOP_LOSS_ENABLED\n",
    "        self.stop_loss_pct = STOP_LOSS_PERCENTAGE\n",
    "        \n",
    "        self.reset()\n",
    "\n",
    "    def debug_pnl_calculation(self):\n",
    "\n",
    "        trade_pnl_sum = sum(trade['pnl'] for trade in self.trade_history)\n",
    "        expected_equity = self.initial_capital + trade_pnl_sum\n",
    "        actual_equity = self.cash + self.unrealized_pnl\n",
    "    \n",
    "        print(f\"Trade P&L Sum: ${trade_pnl_sum:,.2f}\")\n",
    "        print(f\"Expected Equity: ${expected_equity:,.2f}\")\n",
    "        print(f\"Actual Equity: ${actual_equity:,.2f}\")\n",
    "        print(f\"Missing: ${actual_equity - expected_equity:,.2f}\")\n",
    "        print(f\"Transaction Costs: ${getattr(self, 'total_transaction_costs', 0):,.2f}\")\n",
    "    \n",
    "    def _get_business_day_of_month(self, date) -> int:\n",
    "        if isinstance(date, str):\n",
    "             date = pd.to_datetime(date)\n",
    " \n",
    "        first_day = date.replace(day=1)\n",
    "        business_days = 0\n",
    "        current_date = first_day\n",
    " \n",
    "        while current_date <= date:\n",
    "             if current_date.weekday() < 5:  # Monday = 0, Friday = 4\n",
    "                 business_days += 1\n",
    "             current_date += pd.Timedelta(days=1)\n",
    "         \n",
    "        return business_days\n",
    " \n",
    "        \n",
    "    def reset(self) -> np.ndarray:\n",
    "        self.current_step = 0\n",
    "        self.position = 0\n",
    "        self.cash = self.initial_capital\n",
    "        self.total_pnl = 0.0\n",
    "        self.unrealized_pnl = 0.0\n",
    "        self.daily_pnl = 0.0\n",
    "        \n",
    "        # trade tracking\n",
    "        self.winning_trades = 0\n",
    "        self.losing_trades = 0\n",
    "        self.total_trades = 0\n",
    "        self.all_trade_pnls = []  \n",
    "        \n",
    "        # Position tracking\n",
    "        self.position_entry_price = None\n",
    "        self.position_entry_step = None\n",
    "        self.last_position = 0\n",
    "        \n",
    "        self.trade_history = []\n",
    "        self.equity_curve = [self.initial_capital]\n",
    "        self.daily_returns = []\n",
    "        self.max_drawdown = 0.0\n",
    "        self.peak_equity = self.initial_capital\n",
    "        \n",
    "        self.daily_loss_tracker = 0.0\n",
    "        self.stop_loss_triggered = False\n",
    "        \n",
    "        \n",
    "        return self._get_state()\n",
    "    \n",
    "    def _get_state(self) -> np.ndarray:\n",
    "        if self.current_step >= self.n_samples:\n",
    "            market_features = np.zeros((self.data_sequences.shape[1], self.data_sequences.shape[2]))\n",
    "        else:\n",
    "            market_features = self.data_sequences[self.current_step]\n",
    "        \n",
    "        market_state = market_features.flatten()\n",
    "        \n",
    "        trading_state = np.array([\n",
    "            self.position / self.max_contracts,\n",
    "            self.unrealized_pnl / self.initial_capital,\n",
    "            self.total_pnl / self.initial_capital,\n",
    "            self.daily_pnl / self.initial_capital,\n",
    "            (self.cash + self.unrealized_pnl) / self.initial_capital,\n",
    "            self.max_drawdown,\n",
    "            float(self.stop_loss_triggered),\n",
    "            self.current_step / self.n_samples,\n",
    "        ])\n",
    "        \n",
    "        return np.concatenate([market_state, trading_state])\n",
    "    \n",
    "\n",
    "\n",
    "    def step(self, action: int) -> Tuple[np.ndarray, float, bool, Dict]:\n",
    "    \n",
    "        if self.current_step >= self.n_samples - 1:\n",
    "            if self.position != 0:\n",
    "                self.close_final_position()\n",
    "            return self._get_state(), 0.0, True, {'reason': 'end_of_data'}\n",
    "    \n",
    "        current_price = self.data_targets[self.current_step]\n",
    "        next_price = self.data_targets[self.current_step + 1]\n",
    "\n",
    "        oi_ratio_feature_index = 15\n",
    "        self.current_oi_ratio = self.data_sequences[self.current_step][oi_ratio_feature_index][-1]\n",
    "\n",
    "        position_change_pct = POSITION_ACTIONS[action]\n",
    "        position_change = int(position_change_pct * self.max_contracts)\n",
    "\n",
    "        new_position = np.clip(\n",
    "            self.position + position_change, \n",
    "            -self.max_contracts, \n",
    "            self.max_contracts\n",
    "        )\n",
    "\n",
    "        # Position hold timer logic\n",
    "        current_date = self.data_dates[self.current_step]\n",
    "        current_bday = self._get_business_day_of_month(current_date)\n",
    "\n",
    "        if not hasattr(self, 'position_hold_timer'):\n",
    "            self.position_hold_timer = 0\n",
    "            self.forced_closes = 0\n",
    "\n",
    "        if self.position != 0:\n",
    "            self.position_hold_timer += 1\n",
    "            if self.position_hold_timer >= 10:\n",
    "                new_position = 0\n",
    "                self.forced_closes += 1\n",
    "                self.position_hold_timer = 0\n",
    "        else:\n",
    "            self.position_hold_timer = 0\n",
    "    \n",
    "        actual_position_change = new_position - self.position\n",
    "    \n",
    "        # Calculate transaction costs\n",
    "        transaction_cost = abs(actual_position_change) * self.transaction_cost\n",
    "        slippage_cost = abs(actual_position_change) * self.slippage * self.tick_value\n",
    "        total_cost = transaction_cost + slippage_cost\n",
    "\n",
    "        #if total_cost > 0:\n",
    "         #   print(f\"Action {action}: {abs(actual_position_change)} contracts Ã— cost = ${total_cost:.2f}\")\n",
    "    \n",
    "        # Track total transaction costs\n",
    "        if not hasattr(self, 'total_transaction_costs'):\n",
    "            self.total_transaction_costs = 0\n",
    "        self.total_transaction_costs += total_cost\n",
    "    \n",
    "        old_position = self.position\n",
    "    \n",
    "        if self.position != 0 and self.position_entry_price is not None:\n",
    "            # Calculate current unrealized P&L\n",
    "            price_change_since_entry = current_price - self.position_entry_price\n",
    "            current_unrealized = self.position * price_change_since_entry * self.tick_value\n",
    "        \n",
    "            # If position is changing, to account for the P&L\n",
    "            if actual_position_change != 0:\n",
    "                pass  # Let handle_position_change deal with this\n",
    "    \n",
    "        # Record trade if position changes\n",
    "        if actual_position_change != 0:\n",
    "            self.handle_position_change(old_position, new_position, current_price)\n",
    "    \n",
    "        # Update position and cash\n",
    "        self.position = new_position\n",
    "        self.cash -= total_cost\n",
    "    \n",
    "        self.current_step += 1\n",
    "    \n",
    "        if self.position != 0 and self.position_entry_price is not None:\n",
    "            # Calculate unrealized P&L based on current position\n",
    "            price_change_since_entry = next_price - self.position_entry_price\n",
    "            self.unrealized_pnl = self.position * price_change_since_entry * self.tick_value\n",
    "        else:\n",
    "            # No position = no unrealized P&L\n",
    "            self.unrealized_pnl = 0.0\n",
    "    \n",
    "        # Daily P&L is just the change in total equity\n",
    "        current_equity = self.cash + self.unrealized_pnl\n",
    "        previous_equity = self.equity_curve[-1] if self.equity_curve else self.initial_capital\n",
    "        self.daily_pnl = current_equity - previous_equity\n",
    "    \n",
    "        self.equity_curve.append(current_equity)\n",
    "    \n",
    "        # Update peak and drawdown\n",
    "        if current_equity > self.peak_equity:\n",
    "            self.peak_equity = current_equity\n",
    "    \n",
    "        current_drawdown = (self.peak_equity - current_equity) / self.peak_equity\n",
    "        self.max_drawdown = max(self.max_drawdown, current_drawdown)\n",
    "    \n",
    "        done = False\n",
    "        info = {'transaction_cost': total_cost}\n",
    "    \n",
    "        # Risk checks\n",
    "        if self.daily_pnl < -self.max_daily_loss:\n",
    "            done = True\n",
    "            info['reason'] = 'daily_loss_limit'\n",
    "            self.stop_loss_triggered = True\n",
    "    \n",
    "        if self.stop_loss_enabled and current_drawdown > self.stop_loss_pct:\n",
    "            done = True\n",
    "            info['reason'] = 'stop_loss'\n",
    "            self.stop_loss_triggered = True\n",
    "    \n",
    "        reward = self.calculate_reward()\n",
    "    \n",
    "        # Reset daily P&L periodically\n",
    "        if self.current_step % 1 == 0:\n",
    "            self.daily_pnl = 0.0\n",
    "    \n",
    "        next_state = self._get_state()\n",
    "    \n",
    "        # Enhanced info\n",
    "        current_win_rate = self.winning_trades / max(1, self.total_trades)\n",
    "    \n",
    "        info.update({\n",
    "            'position': self.position,\n",
    "            'cash': self.cash,\n",
    "            'unrealized_pnl': self.unrealized_pnl,\n",
    "            'total_pnl': self.total_pnl,\n",
    "            'equity': current_equity,\n",
    "            'drawdown': current_drawdown,\n",
    "            'transaction_cost': total_cost,\n",
    "            'current_price': next_price,\n",
    "            'total_trades': self.total_trades,\n",
    "            'winning_trades': self.winning_trades,\n",
    "            'losing_trades': self.losing_trades,\n",
    "            'win_rate': current_win_rate,\n",
    "            'total_transaction_costs': getattr(self, 'total_transaction_costs', 0)\n",
    "        })\n",
    "    \n",
    "        return next_state, reward, done, info\n",
    "\n",
    "    def handle_position_change(self, old_position: int, new_position: int, current_price: float):\n",
    "        \n",
    "        self.last_position = old_position  # Make sure this line exists\n",
    "\n",
    "\n",
    "        # Case 1: Going from zero to non-zero (opening position)\n",
    "        if old_position == 0 and new_position != 0:\n",
    "            self.position_entry_price = current_price\n",
    "            self.position_entry_step = self.current_step\n",
    "            #print(f\"Step {self.current_step}: Opening position {new_position} at ${current_price:.2f}\")\n",
    "            \n",
    "        # Case 2: Going from non-zero to zero (closing position)\n",
    "        elif old_position != 0 and new_position == 0:\n",
    "            if self.position_entry_price is not None:\n",
    "                trade_pnl = (current_price - self.position_entry_price) * old_position * self.tick_value\n",
    "                self.record_simple_trade(trade_pnl, current_price)\n",
    "                #print(f\"Step {self.current_step}: Closing position {old_position} at ${current_price:.2f}, P&L: ${trade_pnl:.2f}\")\n",
    "            \n",
    "        # Case 3: Flipping from long to short or vice versa\n",
    "        elif (old_position > 0 and new_position < 0) or (old_position < 0 and new_position > 0):\n",
    "            if self.position_entry_price is not None:\n",
    "                # Close the old position\n",
    "                trade_pnl = (current_price - self.position_entry_price) * old_position * self.tick_value\n",
    "                self.record_simple_trade(trade_pnl, current_price)\n",
    "                #print(f\"Step {self.current_step}: Flipping position {old_position} to {new_position}, P&L: ${trade_pnl:.2f}\")\n",
    "            \n",
    "            # Open new position in opposite direction\n",
    "            self.position_entry_price = current_price\n",
    "            self.position_entry_step = self.current_step\n",
    "            \n",
    "        # Case 4: Just changing size in same direction (partial close/add)\n",
    "        elif abs(new_position) < abs(old_position):\n",
    "            # Reducing position size - record partial trade\n",
    "            if self.position_entry_price is not None:\n",
    "                size_reduction = abs(old_position) - abs(new_position)\n",
    "                trade_pnl = (current_price - self.position_entry_price) * size_reduction * (1 if old_position > 0 else -1) * self.tick_value\n",
    "                self.record_simple_trade(trade_pnl, current_price)\n",
    "                #print(f\"Step {self.current_step}: Reducing position by {size_reduction}, P&L: ${trade_pnl:.2f}\")\n",
    "    \n",
    "    def record_simple_trade(self, trade_pnl: float, exit_price: float):\n",
    "    \n",
    "            self.total_trades += 1\n",
    "            self.all_trade_pnls.append(trade_pnl)\n",
    "    \n",
    "            # Simple classification\n",
    "            if trade_pnl > 0.01:\n",
    "                self.winning_trades += 1\n",
    "                result = \"WIN\"\n",
    "            elif trade_pnl < -0.01:\n",
    "                self.losing_trades += 1\n",
    "                result = \"LOSS\"\n",
    "            else:\n",
    "                result = \"BREAK-EVEN\"\n",
    "    \n",
    "            # Update total realized P&L\n",
    "            self.total_pnl += trade_pnl\n",
    "    \n",
    "            \n",
    "            self.unrealized_pnl = 0.0\n",
    "    \n",
    "            # Store trade info\n",
    "            trade_info = {\n",
    "            'entry_price': self.position_entry_price,\n",
    "            'exit_price': exit_price,\n",
    "            'entry_step': self.position_entry_step,\n",
    "            'exit_step': self.current_step,\n",
    "            'pnl': trade_pnl,\n",
    "            'position_size': getattr(self, 'last_position', 0),\n",
    "            'result': result,\n",
    "            'duration': self.current_step - (self.position_entry_step or self.current_step)\n",
    "            }\n",
    "            self.trade_history.append(trade_info)\n",
    "\n",
    "            \n",
    "    def close_final_position(self):\n",
    "        #Close any remaining position at episode end\n",
    "        if self.position != 0 and self.position_entry_price is not None:\n",
    "            final_price = self.data_targets[self.current_step]\n",
    "            trade_pnl = (final_price - self.position_entry_price) * self.position * self.tick_value\n",
    "            self.record_simple_trade(trade_pnl, final_price)\n",
    "            #print(f\"Final position close: P&L ${trade_pnl:.2f}\")\n",
    "            self.position = 0\n",
    "    \n",
    "    \n",
    "    #============== REWARD FORMULA ==============\n",
    "    \n",
    "    \n",
    "    def calculate_reward(self) -> float:\n",
    "        current_equity = self.cash + self.unrealized_pnl\n",
    "    \n",
    "        # P&L component (normalised by initial capital)\n",
    "        pnl_reward = self.daily_pnl / self.initial_capital\n",
    "\n",
    "        # OI Ratio constraint penalty\n",
    "        oi_penalty = 0.0\n",
    "        if hasattr(self, 'current_oi_ratio') and self.current_oi_ratio >= 1.5:\n",
    "            if self.position != 0:\n",
    "                oi_penalty = 5.0  # Strong penalty for trading when OI ratio >= 1.5\n",
    "            else:\n",
    "                oi_penalty = -1  # Small bonus for staying flat when OI ratio >= 1.5\n",
    "        if hasattr(self, 'current_oi_ratio') and self.current_oi_ratio < 0.9:\n",
    "            if self.position != 0:\n",
    "                oi_penalty = -2.0  # Bonus for trading when OI ratio < 0.8 (good time)\n",
    "\n",
    "        # Risk penalty component\n",
    "        risk_penalty = 0.0\n",
    "        if self.max_drawdown > DRAWDOWN_PENALTY_THRESHOLD:\n",
    "            risk_penalty += (self.max_drawdown - DRAWDOWN_PENALTY_THRESHOLD) * 10\n",
    "\n",
    "        # Combine components (removed win rate)\n",
    "        reward = (\n",
    "            REWARD_PNL_WEIGHT * pnl_reward -\n",
    "            REWARD_RISK_WEIGHT * risk_penalty -\n",
    "            oi_penalty\n",
    "    )   \n",
    "\n",
    "        return reward\n",
    "    \n",
    "#=================================================================\n",
    "\n",
    "    def get_performance_metrics(self) -> Dict:\n",
    "        if len(self.equity_curve) < 2:\n",
    "            return {}\n",
    "        \n",
    "        # Close any remaining position for final calculations\n",
    "        self.close_final_position()\n",
    "        \n",
    "        returns = np.diff(self.equity_curve) / self.equity_curve[:-1]\n",
    "        total_return = (self.equity_curve[-1] - self.initial_capital) / self.initial_capital\n",
    "        \n",
    "        if len(returns) > 1 and np.std(returns) > 0:\n",
    "            sharpe_ratio = np.mean(returns) / np.std(returns) * np.sqrt(252)\n",
    "        else:\n",
    "            sharpe_ratio = 0.0\n",
    "        \n",
    "        #  win rate calculation\n",
    "        win_rate = self.winning_trades / max(1, self.total_trades)\n",
    "        \n",
    "        # Calculate profit factor\n",
    "        winning_pnl = sum([pnl for pnl in self.all_trade_pnls if pnl > 0])\n",
    "        losing_pnl = abs(sum([pnl for pnl in self.all_trade_pnls if pnl < 0]))\n",
    "        \n",
    "        if losing_pnl > 0:\n",
    "            profit_factor = winning_pnl / losing_pnl\n",
    "        elif winning_pnl > 0:\n",
    "            profit_factor = float('inf')\n",
    "        else:\n",
    "            profit_factor = 0.0\n",
    "        \n",
    "        if self.trade_history:\n",
    "            avg_trade_duration = np.mean([trade['duration'] for trade in self.trade_history])\n",
    "        else:\n",
    "            avg_trade_duration = 0.0\n",
    "        \n",
    "        # Debug output\n",
    "        #print(f\"\\n=== FINAL PERFORMANCE METRICS ===\")\n",
    "        #print(f\"Total trades recorded: {self.total_trades}\")\n",
    "        #print(f\"Winning trades: {self.winning_trades}\")\n",
    "        #print(f\"Losing trades: {self.losing_trades}\")\n",
    "        #print(f\"Win rate calculation: {self.winning_trades} / {self.total_trades} = {win_rate:.1%}\")\n",
    "        #print(f\"All trade P&Ls: {self.all_trade_pnls}\")\n",
    "        #print(f\"Trade history entries: {len(self.trade_history)}\")\n",
    "        \n",
    "        return {\n",
    "            'total_return': total_return,\n",
    "            'sharpe_ratio': sharpe_ratio,\n",
    "            'max_drawdown': self.max_drawdown,\n",
    "            'win_rate': win_rate,\n",
    "            'profit_factor': profit_factor,\n",
    "            'total_trades': self.total_trades,\n",
    "            'winning_trades': self.winning_trades,\n",
    "            'losing_trades': self.losing_trades,\n",
    "            'avg_trade_duration': avg_trade_duration,\n",
    "            'final_equity': self.equity_curve[-1],\n",
    "            'total_pnl': self.total_pnl,\n",
    "            'unrealized_pnl': self.unrealized_pnl\n",
    "        }\n",
    "    \n",
    "    def get_action_space_size(self) -> int:\n",
    "        return len(POSITION_ACTIONS)\n",
    "    \n",
    "    def get_state_size(self) -> int:\n",
    "        if self.current_step < self.n_samples:\n",
    "            market_features = self.data_sequences[self.current_step].flatten()\n",
    "        else:\n",
    "            market_features = np.zeros((self.data_sequences.shape[1] * self.data_sequences.shape[2]))\n",
    "        \n",
    "        trading_features = 8\n",
    "        return len(market_features) + trading_features\n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        if mode == 'human':\n",
    "            print(f\"Step: {self.current_step}\")\n",
    "            print(f\"Position: {self.position} contracts\")\n",
    "            print(f\"Cash: ${self.cash:,.2f}\")\n",
    "            print(f\"Unrealized P&L: ${self.unrealized_pnl:,.2f}\")\n",
    "            print(f\"Total P&L: ${self.total_pnl:,.2f}\")\n",
    "            print(f\"Equity: ${self.cash + self.unrealized_pnl:,.2f}\")\n",
    "            print(f\"Drawdown: {self.max_drawdown:.2%}\")\n",
    "            print(f\"Trades: {self.total_trades} (Wins: {self.winning_trades}, Losses: {self.losing_trades})\")\n",
    "            print(f\"Win Rate: {self.winning_trades/max(1,self.total_trades):.1%}\")\n",
    "            print(\"-\" * 50)\n",
    "    \n",
    "    def _calculate_episode_end_bonus(self) -> float:\n",
    "    \n",
    "        # Win rate bonus\n",
    "        win_rate_bonus = 0.0\n",
    "        if self.total_trades >= 5:  # Only if we have sufficient trades\n",
    "            win_rate = self.winning_trades / self.total_trades\n",
    "            # Scale the bonus: +0.5 for 100% win rate, 0 for 50% win rate, -0.5 for 0% win rate\n",
    "            win_rate_bonus = REWARD_WINRATE_WEIGHT * (win_rate - 0.5) * 2\n",
    "    \n",
    "        # Optional: Add other episode-end performance bonuses\n",
    "        total_return_bonus = 0.0\n",
    "        if len(self.equity_curve) > 1:\n",
    "            total_return = (self.equity_curve[-1] - self.initial_capital) / self.initial_capital\n",
    "            # Small bonus for positive returns\n",
    "            total_return_bonus = max(0, total_return * 0.1)\n",
    "    \n",
    "        # Combine episode-end bonuses\n",
    "        total_bonus = win_rate_bonus + total_return_bonus\n",
    "    \n",
    "        return total_bonus\n",
    "\n",
    "\n",
    "class TradingEnvironmentWrapper:\n",
    "    def __init__(self, data_splits: Dict):\n",
    "        self.data_splits = data_splits\n",
    "        \n",
    "    def create_env(self, split: str = 'train') -> TradingEnvironment:\n",
    "        if split not in self.data_splits:\n",
    "            raise ValueError(f\"Split '{split}' not found. Available: {list(self.data_splits.keys())}\")\n",
    "        \n",
    "        data = self.data_splits[split]\n",
    "        return TradingEnvironment(\n",
    "            data_sequences=data['sequences'],\n",
    "            data_targets=data['targets'],\n",
    "            data_dates=data['dates']\n",
    "        )\n",
    "    \n",
    "    def get_env_info(self) -> Dict:\n",
    "        info = {}\n",
    "        for split in self.data_splits:\n",
    "            env = self.create_env(split)\n",
    "            info[split] = {\n",
    "                'n_samples': env.n_samples,\n",
    "                'state_size': env.get_state_size(),\n",
    "                'action_space_size': env.get_action_space_size(),\n",
    "                'date_range': (env.data_dates[0], env.data_dates[-1])\n",
    "            }\n",
    "        return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b4784e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DDQN AGENT\n",
    "# =============================================================================\n",
    "\n",
    "class DDQNTradingAgent:\n",
    "   def __init__(self, market_feature_dim: int, trading_state_dim: int, \n",
    "                action_space_size: int, lookback_window: int):\n",
    "       self.market_feature_dim = market_feature_dim\n",
    "       self.trading_state_dim = trading_state_dim\n",
    "       self.action_space_size = action_space_size\n",
    "       self.lookback_window = lookback_window\n",
    "       \n",
    "       self.ddqn = DoubleDQN(\n",
    "           market_feature_dim, trading_state_dim, action_space_size, lookback_window\n",
    "       ).to(DEVICE)\n",
    "       \n",
    "       self.optimizer = optim.Adam(\n",
    "           self.ddqn.online_net.parameters(), \n",
    "           lr=LEARNING_RATE\n",
    "       )\n",
    "       \n",
    "       self.criterion = nn.MSELoss()\n",
    "       \n",
    "       self.replay_buffer = ReplayBuffer(MEMORY_SIZE)\n",
    "       \n",
    "       self.epsilon = EPSILON_START\n",
    "       self.epsilon_min = EPSILON_END\n",
    "       self.epsilon_decay = EPSILON_DECAY\n",
    "       self.gamma = GAMMA\n",
    "       self.tau = TAU\n",
    "       self.batch_size = BATCH_SIZE\n",
    "       self.update_frequency = UPDATE_FREQUENCY\n",
    "       self.target_update_frequency = TARGET_UPDATE_FREQUENCY\n",
    "       \n",
    "       self.training_step = 0\n",
    "       self.episode_rewards = []\n",
    "       self.episode_losses = []\n",
    "       self.episode_epsilons = []\n",
    "       self.training_metrics = {\n",
    "           'rewards': [],\n",
    "           'losses': [],\n",
    "           'epsilons': [],\n",
    "           'q_values': [],\n",
    "           'actions_taken': []\n",
    "       }\n",
    "       \n",
    "       print(f\"DDQN Agent initialised {self._count_parameters()} parameters\")\n",
    "       print(f\"Device: {DEVICE}\")\n",
    "   \n",
    "   def _count_parameters(self) -> int:\n",
    "       return sum(p.numel() for p in self.ddqn.online_net.parameters() if p.requires_grad)\n",
    "   \n",
    "   def _state_to_tensors(self, state: np.ndarray) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "       market_size = self.market_feature_dim * self.lookback_window\n",
    "       market_features = state[:market_size].reshape(self.market_feature_dim, self.lookback_window)\n",
    "       trading_state = state[market_size:]\n",
    "       \n",
    "       market_tensor = torch.FloatTensor(market_features).permute(1, 0).unsqueeze(0).to(DEVICE)\n",
    "       trading_tensor = torch.FloatTensor(trading_state).unsqueeze(0).to(DEVICE)\n",
    "       \n",
    "       return market_tensor, trading_tensor\n",
    "   \n",
    "   def get_action(self, state: np.ndarray, training: bool = True) -> int:\n",
    "       if training and random.random() < self.epsilon:\n",
    "           action = random.randint(0, self.action_space_size - 1)\n",
    "           self.training_metrics['actions_taken'].append(('random', action))\n",
    "       else:\n",
    "           market_features, trading_state = self._state_to_tensors(state)\n",
    "           \n",
    "           with torch.no_grad():\n",
    "               q_values = self.ddqn(market_features, trading_state, use_target=False)\n",
    "               action = q_values.argmax().item()\n",
    "               \n",
    "               if training:\n",
    "                   self.training_metrics['q_values'].append(q_values.cpu().numpy()[0])\n",
    "                   self.training_metrics['actions_taken'].append(('greedy', action))\n",
    "       \n",
    "       return action\n",
    "   \n",
    "   def store_transition(self, state: np.ndarray, action: int, reward: float,\n",
    "                       next_state: np.ndarray, done: bool):\n",
    "       market_features, trading_state = self._state_to_tensors(state)\n",
    "       next_market_features, next_trading_state = self._state_to_tensors(next_state)\n",
    "       \n",
    "       self.replay_buffer.push(\n",
    "           market_features.squeeze(0).cpu().numpy(),\n",
    "           trading_state.squeeze(0).cpu().numpy(),\n",
    "           action,\n",
    "           reward,\n",
    "           next_market_features.squeeze(0).cpu().numpy(),\n",
    "           next_trading_state.squeeze(0).cpu().numpy(),\n",
    "           done\n",
    "       )\n",
    "   \n",
    "   def train_step(self) -> float:\n",
    "       if len(self.replay_buffer) < self.batch_size:\n",
    "           return 0.0\n",
    "       \n",
    "       batch = self.replay_buffer.sample(self.batch_size)\n",
    "       market_features, trading_states, actions, rewards, next_market_features, next_trading_states, dones = batch\n",
    "       \n",
    "       market_features = market_features.to(DEVICE)\n",
    "       trading_states = trading_states.to(DEVICE)\n",
    "       actions = actions.to(DEVICE)\n",
    "       rewards = rewards.to(DEVICE)\n",
    "       next_market_features = next_market_features.to(DEVICE)\n",
    "       next_trading_states = next_trading_states.to(DEVICE)\n",
    "       dones = dones.to(DEVICE)\n",
    "       \n",
    "       current_q_values = self.ddqn(market_features, trading_states, use_target=False)\n",
    "       current_q_values = current_q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "       \n",
    "       with torch.no_grad():\n",
    "           next_q_values_online = self.ddqn(next_market_features, next_trading_states, use_target=False)\n",
    "           next_actions = next_q_values_online.argmax(1)\n",
    "           \n",
    "           next_q_values_target = self.ddqn(next_market_features, next_trading_states, use_target=True)\n",
    "           next_q_values = next_q_values_target.gather(1, next_actions.unsqueeze(1)).squeeze(1)\n",
    "           \n",
    "           target_q_values = rewards + (self.gamma * next_q_values * ~dones)\n",
    "       \n",
    "       loss = self.criterion(current_q_values, target_q_values)\n",
    "       \n",
    "       self.optimizer.zero_grad()\n",
    "       loss.backward()\n",
    "       \n",
    "       torch.nn.utils.clip_grad_norm_(self.ddqn.online_net.parameters(), 1.0)\n",
    "       \n",
    "       self.optimizer.step()\n",
    "       \n",
    "       if self.training_step % self.target_update_frequency == 0:\n",
    "           self.ddqn.update_target_network(self.tau)\n",
    "       \n",
    "       if self.epsilon > self.epsilon_min:\n",
    "           self.epsilon *= self.epsilon_decay\n",
    "       \n",
    "       self.training_step += 1\n",
    "       \n",
    "       return loss.item()\n",
    "   \n",
    "   def train_episode(self, env, episode: int) -> Dict:\n",
    "       state = env.reset()\n",
    "       total_reward = 0.0\n",
    "       total_loss = 0.0\n",
    "       steps = 0\n",
    "       episode_actions = []\n",
    "       \n",
    "       while True:\n",
    "           action = self.get_action(state, training=True)\n",
    "           episode_actions.append(action)\n",
    "           \n",
    "           next_state, reward, done, info = env.step(action)\n",
    "           \n",
    "           self.store_transition(state, action, reward, next_state, done)\n",
    "           \n",
    "           if len(self.replay_buffer) >= self.batch_size and steps % self.update_frequency == 0:\n",
    "               loss = self.train_step()\n",
    "               total_loss += loss\n",
    "           \n",
    "           total_reward += reward\n",
    "           steps += 1\n",
    "           state = next_state\n",
    "           \n",
    "           if done:\n",
    "               break\n",
    "       \n",
    "       episode_metrics = {\n",
    "           'episode': episode,\n",
    "           'total_reward': total_reward,\n",
    "           'average_loss': total_loss / max(1, steps // self.update_frequency),\n",
    "           'steps': steps,\n",
    "           'epsilon': self.epsilon,\n",
    "           'actions': episode_actions,\n",
    "           'final_performance': env.get_performance_metrics()\n",
    "       }\n",
    "       \n",
    "       self.episode_rewards.append(total_reward)\n",
    "       self.episode_losses.append(total_loss / max(1, steps // self.update_frequency))\n",
    "       self.episode_epsilons.append(self.epsilon)\n",
    "       \n",
    "       return episode_metrics\n",
    "   \n",
    "   def evaluate_episode(self, env) -> Dict:\n",
    "       state = env.reset()\n",
    "       total_reward = 0.0\n",
    "       steps = 0\n",
    "       episode_actions = []\n",
    "       \n",
    "       while True:\n",
    "           action = self.get_action(state, training=False)\n",
    "           episode_actions.append(action)\n",
    "           \n",
    "           next_state, reward, done, info = env.step(action)\n",
    "           \n",
    "           total_reward += reward\n",
    "           steps += 1\n",
    "           state = next_state\n",
    "           \n",
    "           if done:\n",
    "               break\n",
    "       \n",
    "       performance_metrics = env.get_performance_metrics()\n",
    "       \n",
    "       eval_metrics = {\n",
    "           'total_reward': total_reward,\n",
    "           'steps': steps,\n",
    "           'actions': episode_actions,\n",
    "           'performance': performance_metrics\n",
    "       }\n",
    "       \n",
    "       return eval_metrics\n",
    "   \n",
    "   def train(self, train_env, val_env, num_episodes: int = None) -> Dict:\n",
    "       if num_episodes is None:\n",
    "           num_episodes = EPISODES\n",
    "       \n",
    "       training_history = {\n",
    "           'train_rewards': [],\n",
    "           'val_rewards': [],\n",
    "           'train_performance': [],\n",
    "           'val_performance': [],\n",
    "           'losses': [],\n",
    "           'epsilons': []\n",
    "       }\n",
    "       \n",
    "       best_val_reward = float('-inf')\n",
    "       patience_counter = 0\n",
    "       \n",
    "       print(f\"training for {num_episodes} episodes...\")\n",
    "       print(f\"replay buffer: {MEMORY_SIZE}\")\n",
    "       print(f\"batch size: {self.batch_size}\")\n",
    "       print(f\"update frequency: {self.update_frequency}\")\n",
    "       \n",
    "       for episode in range(num_episodes):\n",
    "           train_metrics = self.train_episode(train_env, episode)\n",
    "           \n",
    "           if episode % LOG_FREQUENCY == 0:\n",
    "               val_metrics = self.evaluate_episode(val_env)\n",
    "               \n",
    "               training_history['train_rewards'].append(train_metrics['total_reward'])\n",
    "               training_history['val_rewards'].append(val_metrics['total_reward'])\n",
    "               training_history['train_performance'].append(train_metrics['final_performance'])\n",
    "               training_history['val_performance'].append(val_metrics['performance'])\n",
    "               training_history['losses'].append(train_metrics['average_loss'])\n",
    "               training_history['epsilons'].append(train_metrics['epsilon'])\n",
    "               \n",
    "               print(f\"Episode {episode}/{num_episodes}\")\n",
    "               print(f\"  Train Reward: {train_metrics['total_reward']:.4f}\")\n",
    "               print(f\"  Val Reward: {val_metrics['total_reward']:.4f}\")\n",
    "               print(f\"  Loss: {train_metrics['average_loss']:.6f}\")\n",
    "               print(f\"  Epsilon: {train_metrics['epsilon']:.4f}\")\n",
    "               print(f\"  Val Performance: {val_metrics['performance'].get('total_return', 0):.2%}\")\n",
    "               print(\"-\" * 50)\n",
    "               \n",
    "               if val_metrics['total_reward'] > best_val_reward + MIN_IMPROVEMENT:\n",
    "                   best_val_reward = val_metrics['total_reward']\n",
    "                   patience_counter = 0\n",
    "               else:\n",
    "                   patience_counter += 1\n",
    "               \n",
    "               if patience_counter >= PATIENCE:\n",
    "                   print(f\"Early stopping at episode {episode}\")\n",
    "                   break\n",
    "       \n",
    "       print(\"Training done\")\n",
    "       return training_history\n",
    "   \n",
    "   def _print_trade_list(self, test_env):\n",
    "    print(f\"\\n=== TRADE LIST ===\")\n",
    "    if not hasattr(test_env, 'trade_history') or not test_env.trade_history:\n",
    "        print(\"No trades exec\")\n",
    "        return\n",
    "    \n",
    "    for i, trade in enumerate(test_env.trade_history, 1):\n",
    "        pnl = trade['pnl']\n",
    "        # Get contracts from position_size (use absolute value)\n",
    "        contracts = abs(trade.get('position_size', 0))\n",
    "        print(f\"Trade {i}: {contracts} contracts | P&L {pnl:+.2f}\")\n",
    "    \n",
    "    total_pnl = sum(trade['pnl'] for trade in test_env.trade_history)\n",
    "    print(f\"Total trades: {len(test_env.trade_history)}\")\n",
    "    print(f\"Total P&L: {total_pnl:+.2f}\")\n",
    "   \n",
    "   def backtest(self, test_env) -> Dict:\n",
    "       print(\"Starting backtesting...\")\n",
    "       \n",
    "       self.ddqn.online_net.eval()\n",
    "       \n",
    "       results = self.evaluate_episode(test_env)\n",
    "       \n",
    "       performance = results['performance']\n",
    "       actions = results['actions']\n",
    "       \n",
    "       action_counts = {}\n",
    "       for action in actions:\n",
    "           action_counts[action] = action_counts.get(action, 0) + 1\n",
    "       \n",
    "       backtest_results = {\n",
    "           'performance_metrics': performance,\n",
    "           'total_reward': results['total_reward'],\n",
    "           'total_steps': results['steps'],\n",
    "           'action_distribution': action_counts,\n",
    "           'action_sequence': actions\n",
    "       }\n",
    "       \n",
    "       print(\"\\n\" + \"=\"*60)\n",
    "       print(\"BACKTESTING RESULTS\")\n",
    "       print(\"=\"*60)\n",
    "       \n",
    "       print(f\"Total Return: {performance.get('total_return', 0):.2%}\")\n",
    "       print(f\"Sharpe Ratio: {performance.get('sharpe_ratio', 0):.2f}\")\n",
    "       print(f\"Max Drawdown: {performance.get('max_drawdown', 0):.2%}\")\n",
    "       print(f\"Win Rate: {performance.get('win_rate', 0):.1%}\")\n",
    "       print(f\"Profit Factor: {performance.get('profit_factor', 0):.2f}\")\n",
    "       print(f\"Total Trades: {performance.get('total_trades', 0)}\")\n",
    "       print(f\"Final Equity: ${performance.get('final_equity', 0):,.2f}\")\n",
    "       \n",
    "       self._print_trade_list(test_env)\n",
    "\n",
    "       test_env.debug_pnl_calculation()\n",
    "\n",
    "\n",
    "\n",
    "       print(f\"\\nAction Distribution:\")\n",
    "       for action, count in action_counts.items():\n",
    "           action_pct = POSITION_ACTIONS[action]\n",
    "           print(f\"  Action {action} ({action_pct:+.0%}): {count} times ({count/len(actions):.1%})\")\n",
    "       \n",
    "       return backtest_results\n",
    "   \n",
    "   def plot_training_curves(self, training_history: Dict, save_path: str = None):\n",
    "       fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "       \n",
    "       axes[0, 0].plot(training_history['train_rewards'], label='Train', alpha=0.7)\n",
    "       axes[0, 0].plot(training_history['val_rewards'], label='Validation', alpha=0.7)\n",
    "       axes[0, 0].set_title('Episode Rewards')\n",
    "       axes[0, 0].set_xlabel('Episode')\n",
    "       axes[0, 0].set_ylabel('Total Reward')\n",
    "       axes[0, 0].legend()\n",
    "       axes[0, 0].grid(True)\n",
    "       \n",
    "       axes[0, 1].plot(training_history['losses'], color='red', alpha=0.7)\n",
    "       axes[0, 1].set_title('Training Loss')\n",
    "       axes[0, 1].set_xlabel('Episode')\n",
    "       axes[0, 1].set_ylabel('Average Loss')\n",
    "       axes[0, 1].grid(True)\n",
    "       \n",
    "       axes[1, 0].plot(training_history['epsilons'], color='green', alpha=0.7)\n",
    "       axes[1, 0].set_title('Epsilon Decay')\n",
    "       axes[1, 0].set_xlabel('Episode')\n",
    "       axes[1, 0].set_ylabel('Epsilon')\n",
    "       axes[1, 0].grid(True)\n",
    "       \n",
    "       if training_history['val_performance']:\n",
    "           returns = [p.get('total_return', 0) for p in training_history['val_performance']]\n",
    "           sharpes = [p.get('sharpe_ratio', 0) for p in training_history['val_performance']]\n",
    "           \n",
    "           ax2 = axes[1, 1]\n",
    "           ax3 = ax2.twinx()\n",
    "           \n",
    "           line1 = ax2.plot(returns, 'b-', label='Total Return', alpha=0.7)\n",
    "           line2 = ax3.plot(sharpes, 'r-', label='Sharpe Ratio', alpha=0.7)\n",
    "           \n",
    "           ax2.set_xlabel('Episode')\n",
    "           ax2.set_ylabel('Total Return', color='b')\n",
    "           ax3.set_ylabel('Sharpe Ratio', color='r')\n",
    "           ax2.set_title('Validation Performance')\n",
    "           \n",
    "           lines = line1 + line2\n",
    "           labels = [l.get_label() for l in lines]\n",
    "           ax2.legend(lines, labels, loc='upper left')\n",
    "           ax2.grid(True)\n",
    "       \n",
    "       plt.tight_layout()\n",
    "       \n",
    "       if save_path:\n",
    "           plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "       \n",
    "       if PLOT_TRAINING_CURVES:\n",
    "           plt.show()\n",
    "\n",
    "def create_agent_from_env(env) -> DDQNTradingAgent:\n",
    "   state = env.reset()\n",
    "   state_size = len(state)\n",
    "   \n",
    "   market_size = env.data_sequences.shape[1] * env.data_sequences.shape[2]\n",
    "   trading_state_size = state_size - market_size\n",
    "   \n",
    "   agent = DDQNTradingAgent(\n",
    "       market_feature_dim=env.data_sequences.shape[1],\n",
    "       trading_state_dim=trading_state_size,\n",
    "       action_space_size=env.get_action_space_size(),\n",
    "       lookback_window=env.data_sequences.shape[2]\n",
    "   )\n",
    "   \n",
    "   return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca7fd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MAIN EXECUTION FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def create_directories():\n",
    "   directories = [MODEL_SAVE_PATH, LOG_SAVE_PATH, RESULTS_SAVE_PATH]\n",
    "   for directory in directories:\n",
    "       os.makedirs(directory, exist_ok=True)\n",
    "   print(\"directories created\")\n",
    "\n",
    "def load_and_prepare_data(csv_path: str):\n",
    "      \n",
    "   data_handler = TradingDataHandler(csv_path)\n",
    "   \n",
    "   raw_data = data_handler.load_data()\n",
    "   processed_data = data_handler.preprocess_data()\n",
    "   \n",
    "   sequences, targets, dates = data_handler.create_sequences(\n",
    "       processed_data, \n",
    "       lookback=LOOKBACK_WINDOW\n",
    "   )\n",
    "   \n",
    "   data_splits = data_handler.split_data(\n",
    "       sequences, targets, dates, \n",
    "       quick_test=QUICK_TEST_MODE\n",
    "   )\n",
    "   \n",
    "   feature_info = data_handler.get_feature_info()\n",
    "   print(f\"\\nData Information:\")\n",
    "   print(f\"  Features: {feature_info['n_features']}\")\n",
    "   print(f\"  Samples: {feature_info['n_samples']}\")\n",
    "   print(f\"  Date range: {feature_info['date_range'][0]} to {feature_info['date_range'][1]}\")\n",
    "   print(f\"  Lookback window: {LOOKBACK_WINDOW} days\")\n",
    "   \n",
    "   return data_splits, data_handler\n",
    "\n",
    "def create_environments(data_splits):\n",
    "   \n",
    "   env_wrapper = TradingEnvironmentWrapper(data_splits)\n",
    "   \n",
    "   train_env = env_wrapper.create_env('train')\n",
    "   val_env = env_wrapper.create_env('val')\n",
    "   test_env = env_wrapper.create_env('test')\n",
    "   \n",
    "   env_info = env_wrapper.get_env_info()\n",
    "   for split, info in env_info.items():\n",
    "       print(f\"\\n{split.upper()} Environment:\")\n",
    "       print(f\"  Samples: {info['n_samples']}\")\n",
    "       print(f\"  State size: {info['state_size']}\")\n",
    "       print(f\"  Action space: {info['action_space_size']}\")\n",
    "       print(f\"  Date range: {info['date_range'][0]} to {info['date_range'][1]}\")\n",
    "   \n",
    "   return env_wrapper, train_env, val_env, test_env\n",
    "\n",
    "def train_agent(train_env, val_env):\n",
    "   \n",
    "   agent = create_agent_from_env(train_env)\n",
    "   \n",
    "   num_episodes = QUICK_TEST_EPISODES if QUICK_TEST_MODE else EPISODES\n",
    "   \n",
    "   print(f\"training for {num_episodes} episodes...\")\n",
    "   \n",
    "   training_history = agent.train(train_env, val_env, num_episodes)\n",
    "   \n",
    "   if PLOT_TRAINING_CURVES:\n",
    "       save_path = f\"{RESULTS_SAVE_PATH}training_curves.png\" if SAVE_PLOTS else None\n",
    "       agent.plot_training_curves(training_history, save_path)\n",
    "   \n",
    "   return agent, training_history\n",
    "\n",
    "def backtest_agent(agent, test_env):\n",
    "   \n",
    "   backtest_results = agent.backtest(test_env)\n",
    "   \n",
    "   return backtest_results\n",
    "\n",
    "def plot_backtest_results(test_env, backtest_results):\n",
    "   if not PLOT_TRADING_RESULTS:\n",
    "       return\n",
    "   \n",
    "   fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "   \n",
    "   equity_curve = test_env.equity_curve\n",
    "   dates = test_env.data_dates[:len(equity_curve)]\n",
    "   \n",
    "   axes[0, 0].plot(dates, equity_curve, linewidth=2, alpha=0.8)\n",
    "   axes[0, 0].set_title('Equity Curve', fontsize=14, fontweight='bold')\n",
    "   axes[0, 0].set_xlabel('Date')\n",
    "   axes[0, 0].set_ylabel('Equity ($)')\n",
    "   axes[0, 0].grid(True, alpha=0.3)\n",
    "   axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "   \n",
    "   peak = np.maximum.accumulate(equity_curve)\n",
    "   drawdown = (peak - equity_curve) / peak\n",
    "   \n",
    "   axes[0, 1].fill_between(dates, drawdown, 0, alpha=0.7, color='red')\n",
    "   axes[0, 1].set_title('Drawdown', fontsize=14, fontweight='bold')\n",
    "   axes[0, 1].set_xlabel('Date')\n",
    "   axes[0, 1].set_ylabel('Drawdown (%)')\n",
    "   axes[0, 1].yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: '{:.1%}'.format(y)))\n",
    "   axes[0, 1].grid(True, alpha=0.3)\n",
    "   axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "   \n",
    "   action_dist = backtest_results['action_distribution']\n",
    "   actions = list(action_dist.keys())\n",
    "   counts = list(action_dist.values())\n",
    "   action_labels = [f\"Action {a}\\n({POSITION_ACTIONS[a]:+.0%})\" for a in actions]\n",
    "   \n",
    "   axes[1, 0].bar(action_labels, counts, alpha=0.8)\n",
    "   axes[1, 0].set_title('Action Distribution', fontsize=14, fontweight='bold')\n",
    "   axes[1, 0].set_xlabel('Action')\n",
    "   axes[1, 0].set_ylabel('Frequency')\n",
    "   axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "   axes[1, 0].grid(True, alpha=0.3)\n",
    "   \n",
    "   performance = backtest_results['performance_metrics']\n",
    "   metrics_names = ['Total Return', 'Sharpe Ratio', 'Max Drawdown', 'Win Rate', 'Profit Factor']\n",
    "   metrics_values = [\n",
    "       performance.get('total_return', 0),\n",
    "       performance.get('sharpe_ratio', 0),\n",
    "       performance.get('max_drawdown', 0),\n",
    "       performance.get('win_rate', 0),\n",
    "       performance.get('profit_factor', 0)\n",
    "   ]\n",
    "   \n",
    "   formatted_values = [\n",
    "       f\"{metrics_values[0]:.1%}\",\n",
    "       f\"{metrics_values[1]:.2f}\",\n",
    "       f\"{metrics_values[2]:.1%}\",\n",
    "       f\"{metrics_values[3]:.1%}\",\n",
    "       f\"{metrics_values[4]:.2f}\"\n",
    "   ]\n",
    "   \n",
    "   bars = axes[1, 1].bar(metrics_names, metrics_values, alpha=0.8)\n",
    "   axes[1, 1].set_title('Performance Metrics', fontsize=14, fontweight='bold')\n",
    "   axes[1, 1].set_ylabel('Value')\n",
    "   axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "   axes[1, 1].grid(True, alpha=0.3)\n",
    "   \n",
    "   for bar, formatted_val in zip(bars, formatted_values):\n",
    "       height = bar.get_height()\n",
    "       axes[1, 1].text(bar.get_x() + bar.get_width()/2., height + max(metrics_values)*0.01,\n",
    "                      formatted_val, ha='center', va='bottom', fontweight='bold')\n",
    "   \n",
    "   plt.tight_layout()\n",
    "   \n",
    "   if SAVE_PLOTS:\n",
    "       plt.savefig(f\"{RESULTS_SAVE_PATH}backtest_analysis.png\", \n",
    "                  dpi=300, bbox_inches='tight')\n",
    "   \n",
    "   plt.show()\n",
    "\n",
    "def main(csv_path: str):\n",
    "    print(\"=\"*60)\n",
    "    print(\"DDQN TRADING SYSTEM - GOLDMAN ROLL STRATEGY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    create_directories()\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(42)\n",
    "    \n",
    "    try:\n",
    "        data_splits, data_handler = load_and_prepare_data(csv_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        return None, None, None  # Return None values instead of just return\n",
    "    \n",
    "    env_wrapper, train_env, val_env, test_env = create_environments(data_splits)\n",
    "    \n",
    "    try:\n",
    "        agent, training_history = train_agent(train_env, val_env)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during training: {e}\")\n",
    "        return None, None, None  # Return None values instead of just return\n",
    "    \n",
    "    try:\n",
    "        backtest_results = backtest_agent(agent, test_env)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during backtesting: {e}\")\n",
    "        return None, None, None  # Return None values instead of just return\n",
    "    \n",
    "    try:\n",
    "        plot_backtest_results(test_env, backtest_results)\n",
    "    except Exception as e:\n",
    "        print(f\"Error plotting results: {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"COMPLETED\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return agent, backtest_results, training_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cf80860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MULTI-RUN EXECUTION WITH DIFFERENT SEEDS\n",
    "# =============================================================================\n",
    "\n",
    "def run_with_seed(csv_path: str, seed: int, run_number: int):\n",
    "\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"START RUN {run_number}/5 WITH SEED {seed}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Set seeds\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    # Run the main function\n",
    "    result = main(csv_path)\n",
    "    \n",
    "    if result[0] is not None:\n",
    "        agent, backtest_results, training_history = result\n",
    "        performance = backtest_results['performance_metrics']\n",
    "        \n",
    "        # Extract key metrics\n",
    "        run_results = {\n",
    "            'seed': seed,\n",
    "            'run': run_number,\n",
    "            'total_return': performance.get('total_return', 0),\n",
    "            'sharpe_ratio': performance.get('sharpe_ratio', 0),\n",
    "            'max_drawdown': performance.get('max_drawdown', 0),\n",
    "            'win_rate': performance.get('win_rate', 0),\n",
    "            'profit_factor': performance.get('profit_factor', 0),\n",
    "            'total_trades': performance.get('total_trades', 0),\n",
    "            'final_equity': performance.get('final_equity', 0)\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nRUN {run_number} COMPLETED SUCCESSFULLY!\")\n",
    "        print(f\"Total Return: {run_results['total_return']:.2%}\")\n",
    "        print(f\"Sharpe Ratio: {run_results['sharpe_ratio']:.2f}\")\n",
    "        print(f\"Max Drawdown: {run_results['max_drawdown']:.2%}\")\n",
    "        \n",
    "        return run_results\n",
    "    else:\n",
    "        print(f\"RUN {run_number} FAILED!\")\n",
    "        return None\n",
    "\n",
    "def run_multiple_seeds(csv_path: str):\n",
    "    #5 experiments with different seeds and summary\n",
    "    seeds = [42, 123, 456, 789, 999]\n",
    "    all_results = []\n",
    "    successful_runs = 0\n",
    "    \n",
    "    print(\"5 runs start\")\n",
    "    \n",
    "    # Run 5 experiments\n",
    "    for i, seed in enumerate(seeds, 1):\n",
    "        result = run_with_seed(csv_path, seed, i)\n",
    "        if result is not None:\n",
    "            all_results.append(result)\n",
    "            successful_runs += 1\n",
    "    \n",
    "    # Calculate summary statistics\n",
    "    if successful_runs > 0:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"SUMMARY - {successful_runs}/5 SUCCESSFUL RUNS\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Calculate averages and std deviations\n",
    "        metrics = ['total_return', 'sharpe_ratio', 'max_drawdown', 'win_rate', \n",
    "                  'profit_factor', 'total_trades', 'final_equity']\n",
    "        \n",
    "        summary = {}\n",
    "        for metric in metrics:\n",
    "            values = [r[metric] for r in all_results if r[metric] is not None]\n",
    "            if values:\n",
    "                summary[f'{metric}_mean'] = np.mean(values)\n",
    "                summary[f'{metric}_std'] = np.std(values)\n",
    "                summary[f'{metric}_min'] = np.min(values)\n",
    "                summary[f'{metric}_max'] = np.max(values)\n",
    "        \n",
    "        # Print detailed results\n",
    "        print(\"\\nINDIVIDUAL RUN RESULTS:\")\n",
    "        print(\"-\" * 100)\n",
    "        print(f\"{'Run':<5} {'Seed':<6} {'Return':<10} {'Sharpe':<8} {'Drawdown':<12} {'Win Rate':<10} {'Trades':<8}\")\n",
    "        print(\"-\" * 100)\n",
    "        \n",
    "        for result in all_results:\n",
    "            print(f\"{result['run']:<5} {result['seed']:<6} \"\n",
    "                  f\"{result['total_return']:<10.2%} {result['sharpe_ratio']:<8.2f} \"\n",
    "                  f\"{result['max_drawdown']:<12.2%} {result['win_rate']:<10.1%} \"\n",
    "                  f\"{result['total_trades']:<8.0f}\")\n",
    "        \n",
    "        print(\"\\nSUMMARY STATISTICS:\")\n",
    "        print(\"-\" * 60)\n",
    "        print(f\"Average Return: {summary['total_return_mean']:.2%} Â± {summary['total_return_std']:.2%}\")\n",
    "        print(f\"Average Sharpe: {summary['sharpe_ratio_mean']:.2f} Â± {summary['sharpe_ratio_std']:.2f}\")\n",
    "        print(f\"Average Drawdown: {summary['max_drawdown_mean']:.2%} Â± {summary['max_drawdown_std']:.2%}\")\n",
    "        print(f\"Average Win Rate: {summary['win_rate_mean']:.1%} Â± {summary['win_rate_std']:.1%}\")\n",
    "        print(f\"Average Trades: {summary['total_trades_mean']:.0f} Â± {summary['total_trades_std']:.0f}\")\n",
    "        \n",
    "        print(f\"\\nRANGES:\")\n",
    "        print(f\"Return Range: {summary['total_return_min']:.2%} to {summary['total_return_max']:.2%}\")\n",
    "        print(f\"Sharpe Range: {summary['sharpe_ratio_min']:.2f} to {summary['sharpe_ratio_max']:.2f}\")\n",
    "        \n",
    "        return all_results, summary\n",
    "    else:\n",
    "        print(\"ALL RUNS FAILED!\")\n",
    "        return [], {}\n",
    "\n",
    "\n",
    "def run_multiple_csvs_and_seeds(csv_files: list):\n",
    "    #5 experiments with different seeds for multiple CSV files\n",
    "    \n",
    "    all_results_by_file = {}\n",
    "    all_summaries_by_file = {}\n",
    "    \n",
    "    print(f\"STARTING MULTI-CSV STATISTICAL VALIDATION\")\n",
    "    print(f\"Files to process: {len(csv_files)}\")\n",
    "    print(f\"Runs per file: 5\")\n",
    "    print(f\"Total experiments: {len(csv_files) * 5}\")\n",
    "    \n",
    "    for file_idx, csv_file in enumerate(csv_files, 1):\n",
    "        print(f\"\\n{'='*100}\")\n",
    "        print(f\"PROCESSING FILE {file_idx}/{len(csv_files)}: {csv_file}\")\n",
    "        print(f\"{'='*100}\")\n",
    "        \n",
    "        if not os.path.exists(csv_file):\n",
    "            print(f\"ERROR: CSV file not found at: {csv_file}\")\n",
    "            continue\n",
    "        \n",
    "        # Run 5 seeds for this CSV file\n",
    "        results, summary = run_multiple_seeds(csv_file)\n",
    "        \n",
    "        # Store results\n",
    "        file_name = os.path.basename(csv_file).replace('.csv', '')\n",
    "        all_results_by_file[file_name] = results\n",
    "        all_summaries_by_file[file_name] = summary\n",
    "        \n",
    "        # Print file summary\n",
    "        if results:\n",
    "            print(f\"\\n{'-'*60}\")\n",
    "            print(f\"SUMMARY FOR {file_name}\")\n",
    "            print(f\"{'-'*60}\")\n",
    "            avg_return = summary.get('total_return_mean', 0)\n",
    "            avg_sharpe = summary.get('sharpe_ratio_mean', 0)\n",
    "            successful_runs = len(results)\n",
    "            print(f\"Successful runs: {successful_runs}/5\")\n",
    "            print(f\"Average return: {avg_return:.2%}\")\n",
    "            print(f\"Average Sharpe: {avg_sharpe:.2f}\")\n",
    "        else:\n",
    "            print(f\"NO SUCCESSFUL RUNS FOR {file_name}\")\n",
    "    \n",
    "    # Print overall summary\n",
    "    print_overall_summary(all_results_by_file, all_summaries_by_file)\n",
    "    \n",
    "    return all_results_by_file, all_summaries_by_file\n",
    "\n",
    "def print_overall_summary(all_results_by_file: dict, all_summaries_by_file: dict):\n",
    "    \n",
    "    print(f\"\\n{'='*100}\")\n",
    "    print(f\"OVERALL SUMMARY ACROSS ALL FILES\")\n",
    "    print(f\"{'='*100}\")\n",
    "    \n",
    "    # Summary table\n",
    "    print(f\"\\nFILE PERFORMANCE SUMMARY:\")\n",
    "    print(\"-\" * 120)\n",
    "    print(f\"{'File':<20} {'Runs':<6} {'Avg Return':<12} {'Avg Sharpe':<12} {'Avg Drawdown':<14} {'Avg Trades':<12}\")\n",
    "    print(\"-\" * 120)\n",
    "    \n",
    "    all_returns = []\n",
    "    all_sharpes = []\n",
    "    total_successful_runs = 0\n",
    "    \n",
    "    for file_name, summary in all_summaries_by_file.items():\n",
    "        if summary:  # If file had successful runs\n",
    "            runs = len(all_results_by_file[file_name])\n",
    "            avg_return = summary.get('total_return_mean', 0)\n",
    "            avg_sharpe = summary.get('sharpe_ratio_mean', 0)\n",
    "            avg_drawdown = summary.get('max_drawdown_mean', 0)\n",
    "            avg_trades = summary.get('total_trades_mean', 0)\n",
    "            \n",
    "            print(f\"{file_name:<20} {runs:<6} {avg_return:<12.2%} {avg_sharpe:<12.2f} \"\n",
    "                  f\"{avg_drawdown:<14.2%} {avg_trades:<12.0f}\")\n",
    "            \n",
    "            all_returns.append(avg_return)\n",
    "            all_sharpes.append(avg_sharpe)\n",
    "            total_successful_runs += runs\n",
    "        else:\n",
    "            print(f\"{file_name:<20} {'0':<6} {'N/A':<12} {'N/A':<12} {'N/A':<14} {'N/A':<12}\")\n",
    "    \n",
    "    print(\"-\" * 120)\n",
    "    \n",
    "    # Overall statistics\n",
    "    if all_returns:\n",
    "        print(f\"\\nOVERALL STATISTICS:\")\n",
    "        print(f\"Total successful runs: {total_successful_runs}\")\n",
    "        print(f\"Files with successful runs: {len([s for s in all_summaries_by_file.values() if s])}\")\n",
    "        print(f\"Average return across all files: {np.mean(all_returns):.2%}\")\n",
    "        print(f\"Average Sharpe across all files: {np.mean(all_sharpes):.2f}\")\n",
    "        print(f\"Best performing file (return): {max(all_returns):.2%}\")\n",
    "        print(f\"Worst performing file (return): {min(all_returns):.2%}\")\n",
    "    else:\n",
    "        print(f\"\\nNO SUCCESSFUL RUNS ACROSS ANY FILES!\")\n",
    "\n",
    "# =============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    import random\n",
    "    \n",
    "    csv_files = [\n",
    "        \"/notebooks/CL12 - Sheet1.csv\",    # Brent Oil\n",
    "        \"/notebooks/CO12 - Sheet1.csv\",    # Crude Oil\n",
    "        \"/notebooks/GC12 - Sheet1.csv\",    # Gold\n",
    "        \"/notebooks/LC12 - Sheet1.csv\",    # Live Cattle\n",
    "        \"/notebooks/SB12 - Sheet1.csv\",    # Sugar #11\n",
    "        \"/notebooks/SI12 - Sheet1.csv\",    # Silver\n",
    "    ]\n",
    "    \n",
    "    # Check which files exist\n",
    "    existing_files = []\n",
    "    for file_path in csv_files:\n",
    "        if os.path.exists(file_path):\n",
    "            existing_files.append(file_path)\n",
    "            print(f\"âœ“ Found: {file_path}\")\n",
    "        else:\n",
    "            print(f\"âœ— Missing: {file_path}\")\n",
    "    \n",
    "    if existing_files:\n",
    "        print(f\"\\nProcessing {len(existing_files)} files...\")\n",
    "        all_results, all_summaries = run_multiple_csvs_and_seeds(existing_files)\n",
    "    else:\n",
    "        print(\"ERROR: No CSV files found!\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
