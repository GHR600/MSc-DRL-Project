{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fad67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data Handler for DDQN Trading System\n",
    "Handles loading, preprocessing, and feature engineering of trading data\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214fd574",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "from typing import Tuple, Dict, List\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab569dd5",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95421e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TradingDataHandler:\n",
    "    def __init__(self, csv_path: str):\n",
    "        \"\"\"\n",
    "        Initialize the data handler\n",
    "        \n",
    "        Args:\n",
    "            csv_path: Path to the CSV file\n",
    "        \"\"\"\n",
    "        self.csv_path = csv_path\n",
    "        self.raw_data = None\n",
    "        self.processed_data = None\n",
    "        self.scaler = StandardScaler()\n",
    "        self.feature_columns = config.FEATURE_COLUMNS\n",
    "        self.date_column = config.DATE_COLUMN\n",
    "        self.target_column = config.TARGET_COLUMN\n",
    "        \n",
    "    def load_data(self) -> pd.DataFrame:\n",
    "        \"\"\"Load raw data from CSV\"\"\"\n",
    "        print(f\"Loading data from {self.csv_path}...\")\n",
    "        \n",
    "        # Load CSV with proper column names\n",
    "        column_names = [\n",
    "            'PX_OPEN1', 'PX_HIGH1', 'PX_LOW1', 'PX_LAST1', 'PX_VOLUME1', 'OPEN_INT1',\n",
    "            'PX_OPEN2', 'PX_HIGH2', 'PX_LOW2', 'PX_LAST2', 'PX_VOLUME2', 'OPEN_INT2',\n",
    "            'Dates', 'VOL Change1', 'Vol Change %1', 'OI Change1', 'OI Change %1',\n",
    "            'CALENDAR', 'Vol Ratio', 'Vol Ratio Change', 'OI Ratio', 'OI Ratio Change',\n",
    "            'VOL Change2', 'Vol Change %2', 'OI Change2', 'OI Change %2'\n",
    "            ]\n",
    "        \n",
    "        self.raw_data = pd.read_csv(self.csv_path, names=column_names, header=None, skiprows=3)\n",
    "        \n",
    "        # Convert date column \n",
    "        self.raw_data['Dates'] = pd.to_datetime(self.raw_data['Dates'], dayfirst=True)\n",
    "        self.raw_data = self.raw_data.sort_values('Dates').reset_index(drop=True)\n",
    "        self.raw_data['date'] = self.raw_data['Dates']\n",
    "\n",
    "        print(f\"Loaded {len(self.raw_data)} rows of data\")\n",
    "        print(f\"Date range: {self.raw_data['Dates'].min()} to {self.raw_data['Dates'].max()}\")\n",
    "\n",
    "        print(f\"CALENDAR spread stats:\")\n",
    "        print(f\"  Min: {self.raw_data['CALENDAR'].min()}\")\n",
    "        print(f\"  Max: {self.raw_data['CALENDAR'].max()}\")\n",
    "        print(f\"  Mean: {self.raw_data['CALENDAR'].mean()}\")\n",
    "        print(f\"  Std: {self.raw_data['CALENDAR'].std()}\")\n",
    "\n",
    "        \n",
    "        return self.raw_data\n",
    "    \n",
    "    def calculate_business_day_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Calculate Goldman roll related features\"\"\"\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Calculate business day of month\n",
    "        df['business_day_of_month'] = df['Dates'].apply(self._get_business_day_of_month)\n",
    "        \n",
    "        # Calculate days until/since Goldman roll period\n",
    "        df['days_to_roll_start'] = df['business_day_of_month'].apply(\n",
    "            lambda x: max(0, config.GOLDMAN_ROLL_START_DAY - x) if x < config.GOLDMAN_ROLL_START_DAY \n",
    "            else 0\n",
    "        )\n",
    "        \n",
    "        df['days_since_roll_end'] = df['business_day_of_month'].apply(\n",
    "            lambda x: max(0, x - config.GOLDMAN_ROLL_END_DAY) if x > config.GOLDMAN_ROLL_END_DAY \n",
    "            else 0\n",
    "        )\n",
    "        \n",
    "        # Binary feature for Goldman roll period\n",
    "        df['in_goldman_roll'] = (\n",
    "            (df['business_day_of_month'] >= config.GOLDMAN_ROLL_START_DAY) & \n",
    "            (df['business_day_of_month'] <= config.GOLDMAN_ROLL_END_DAY)\n",
    "        ).astype(int)\n",
    "        \n",
    "        # Extended window feature (15 days around roll)\n",
    "        df['in_extended_roll_window'] = (\n",
    "            (df['business_day_of_month'] >= config.GOLDMAN_ROLL_START_DAY - 5) & \n",
    "            (df['business_day_of_month'] <= config.GOLDMAN_ROLL_END_DAY + 5)\n",
    "        ).astype(int)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _get_business_day_of_month(self, date: datetime) -> int:\n",
    "        \"\"\"Calculate which business day of the month this is\"\"\"\n",
    "        # Get first day of month\n",
    "        first_day = date.replace(day=1)\n",
    "        \n",
    "        # Count business days from first day to current date\n",
    "        business_days = 0\n",
    "        current_date = first_day\n",
    "        \n",
    "        while current_date <= date:\n",
    "            if current_date.weekday() < 5:  # Monday = 0, Friday = 4\n",
    "                business_days += 1\n",
    "            current_date += timedelta(days=1)\n",
    "            \n",
    "        return business_days\n",
    "    \n",
    "    def engineer_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Engineer additional features\"\"\"\n",
    "        df = df.copy()\n",
    "    \n",
    "        # Price-based features (update column names)\n",
    "        df['front_hl_ratio'] = df['PX_HIGH1'] / df['PX_LOW1']\n",
    "        df['second_hl_ratio'] = df['PX_HIGH2'] / df['PX_LOW2']\n",
    "        df['front_close_position'] = (df['PX_LAST1'] - df['PX_LOW1']) / (df['PX_HIGH1'] - df['PX_LOW1'])\n",
    "        df['second_close_position'] = (df['PX_LAST2'] - df['PX_LOW2']) / (df['PX_HIGH2'] - df['PX_LOW2'])\n",
    "\n",
    "        # Volume and OI momentum\n",
    "        df['volume_momentum'] = df['Vol Ratio'].rolling(5).mean()\n",
    "        df['oi_momentum'] = df['OI Ratio'].rolling(5).mean()\n",
    "\n",
    "        # Calendar spread momentum\n",
    "        df['spread_momentum_5'] = df['CALENDAR'].rolling(5).mean()\n",
    "        df['spread_momentum_10'] = df['CALENDAR'].rolling(10).mean()\n",
    "        df['spread_volatility'] = df['CALENDAR'].rolling(10).std()\n",
    "\n",
    "        # Relative strength\n",
    "        df['front_vs_second_strength'] = df['PX_LAST1'] / df['PX_LAST2']\n",
    "    \n",
    "        return df\n",
    "    \n",
    "    def preprocess_data(self) -> pd.DataFrame:\n",
    "        \"\"\"Complete preprocessing pipeline\"\"\"\n",
    "        if self.raw_data is None:\n",
    "            self.load_data()\n",
    "            \n",
    "        df = self.raw_data.copy()\n",
    "        \n",
    "        # Handle missing values\n",
    "        df = df.fillna(method='ffill').fillna(method='bfill')\n",
    "        \n",
    "        # Add business day features\n",
    "        df = self.calculate_business_day_features(df)\n",
    "        \n",
    "        # Engineer additional features\n",
    "        df = self.engineer_features(df)\n",
    "        \n",
    "        # Remove any remaining NaN values\n",
    "        df = df.dropna()\n",
    "        \n",
    "        self.processed_data = df\n",
    "        print(f\"Preprocessing complete. Final dataset: {len(df)} rows\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def create_sequences(self, df: pd.DataFrame, lookback: int = None) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Create sequences for CNN input\n",
    "    \n",
    "        Returns:\n",
    "        features: (n_samples, n_features, lookback_window)\n",
    "        targets: (n_samples,) - calendar spread values\n",
    "        dates: (n_samples,) - corresponding dates\n",
    "        \"\"\"\n",
    "        if lookback is None:\n",
    "            lookback = config.LOOKBACK_WINDOW\n",
    "        \n",
    "        # Get feature columns (exclude date columns and any engineered date features)\n",
    "        feature_cols = [col for col in df.columns if col not in ['date', 'Dates']]\n",
    "    \n",
    "        # Also exclude any datetime-related engineered features\n",
    "        feature_cols = [col for col in feature_cols if not col.startswith('business_day')]\n",
    "        feature_cols = [col for col in feature_cols if not col.startswith('days_')]\n",
    "        feature_cols = [col for col in feature_cols if not col.startswith('in_goldman')]\n",
    "        feature_cols = [col for col in feature_cols if not col.startswith('in_extended')]\n",
    "    \n",
    "        print(f\"Using {len(feature_cols)} features for training\")\n",
    "        print(f\"Features: {feature_cols}...\")  # Print first 5 feature names\n",
    "    \n",
    "        # Normalize features\n",
    "        feature_data = self.scaler.fit_transform(df[feature_cols])\n",
    "    \n",
    "        sequences = []\n",
    "        targets = []\n",
    "        dates = []\n",
    "    \n",
    "        for i in range(lookback, len(df)):\n",
    "            # Get sequence of features\n",
    "            sequence = feature_data[i-lookback:i].T  # Transpose to (n_features, lookback)\n",
    "            sequences.append(sequence)\n",
    "        \n",
    "            # Target is the calendar spread at current time\n",
    "            targets.append(df.iloc[i][self.target_column])\n",
    "            dates.append(df.iloc[i]['Dates'])  # Use 'Dates' instead of 'date'\n",
    "    \n",
    "        return np.array(sequences), np.array(targets), np.array(dates)\n",
    "    \n",
    "    def split_data(self, sequences: np.ndarray, targets: np.ndarray, dates: np.ndarray, \n",
    "                   quick_test: bool = False) -> Dict:\n",
    "        \"\"\"Split data into train/validation/test sets\"\"\"\n",
    "        \n",
    "        if quick_test:\n",
    "            # Use only recent data for quick testing\n",
    "            n_years = config.QUICK_TEST_YEARS\n",
    "            cutoff_date = dates[-1] - pd.DateOffset(years=n_years)\n",
    "            mask = pd.to_datetime(dates) >= cutoff_date\n",
    "            \n",
    "            sequences = sequences[mask]\n",
    "            targets = targets[mask]\n",
    "            dates = dates[mask]\n",
    "            \n",
    "        n_samples = len(sequences)\n",
    "        \n",
    "        # Calculate split indices\n",
    "        train_end = int(n_samples * config.TRAIN_RATIO)\n",
    "        val_end = int(n_samples * (config.TRAIN_RATIO + config.VAL_RATIO))\n",
    "        \n",
    "        data_splits = {\n",
    "            'train': {\n",
    "                'sequences': sequences[:train_end],\n",
    "                'targets': targets[:train_end],\n",
    "                'dates': dates[:train_end]\n",
    "            },\n",
    "            'val': {\n",
    "                'sequences': sequences[train_end:val_end],\n",
    "                'targets': targets[train_end:val_end],\n",
    "                'dates': dates[train_end:val_end]\n",
    "            },\n",
    "            'test': {\n",
    "                'sequences': sequences[val_end:],\n",
    "                'targets': targets[val_end:],\n",
    "                'dates': dates[val_end:]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        print(f\"Data splits:\")\n",
    "        print(f\"  Train: {len(data_splits['train']['sequences'])} samples\")\n",
    "        print(f\"  Validation: {len(data_splits['val']['sequences'])} samples\") \n",
    "        print(f\"  Test: {len(data_splits['test']['sequences'])} samples\")\n",
    "        \n",
    "        return data_splits\n",
    "    \n",
    "    def get_feature_info(self) -> Dict:\n",
    "        \"\"\"Get information about features\"\"\"\n",
    "        if self.processed_data is None:\n",
    "            self.preprocess_data()\n",
    "            \n",
    "        feature_cols = [col for col in self.processed_data.columns if col not in ['date']]\n",
    "        \n",
    "        return {\n",
    "            'n_features': len(feature_cols),\n",
    "            'feature_names': feature_cols,\n",
    "            'n_samples': len(self.processed_data),\n",
    "            'date_range': (self.processed_data['date'].min(), self.processed_data['date'].max())\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e700c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage and testing\n",
    "if __name__ == \"__main__\":\n",
    "    # This will be useful for testing the data handler\n",
    "    print(\"Testing TradingDataHandler...\")\n",
    "    \n",
    "    # You would initialize like this:\n",
    "    # handler = TradingDataHandler(\"your_data.csv\")\n",
    "    # data = handler.preprocess_data()\n",
    "    # sequences, targets, dates = handler.create_sequences(data)\n",
    "    # splits = handler.split_data(sequences, targets, dates, quick_test=True)\n",
    "    \n",
    "    print(\"Data handler ready for use!\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
